{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_u2yc1W3DE2",
        "outputId": "0d730b38-9b2c-4882-cf49-5187d68a9f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting panda_gym\n",
            "  Downloading panda_gym-3.0.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting pybullet (from panda_gym)\n",
            "  Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from panda_gym) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panda_gym-3.0.7-py3-none-any.whl (23 kB)\n",
            "Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, panda_gym, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 panda_gym-3.0.7 pybullet-3.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium torch numpy matplotlib tqdm panda_gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fICnPcou3H3s",
        "outputId": "64bdb751-7723-400b-c18d-6606f1eed351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Training on PandaReach-v3...\n",
            "Iteration: 1, timesteps: 2048\n",
            "Mean reward: -45.13, mean episode length: 45.27\n",
            "Losses: {'policy_loss': np.float64(-0.01423507425643038), 'value_loss': np.float64(0.3657156912377104), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.007575771590927616)}\n",
            "Iteration: 2, timesteps: 4096\n",
            "Mean reward: -47.67, mean episode length: 47.71\n",
            "Losses: {'policy_loss': np.float64(-0.02100983641576022), 'value_loss': np.float64(0.29713847059756515), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.009160734160104766)}\n",
            "Iteration: 3, timesteps: 6144\n",
            "Mean reward: -43.35, mean episode length: 43.50\n",
            "Losses: {'policy_loss': np.float64(-0.019710145736462438), 'value_loss': np.float64(0.43364804834127424), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.008433567130123266)}\n",
            "Iteration: 4, timesteps: 8192\n",
            "Mean reward: -45.98, mean episode length: 46.11\n",
            "Losses: {'policy_loss': np.float64(-0.02010320747504011), 'value_loss': np.float64(0.44568423070013524), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.009178254180005752)}\n",
            "Iteration: 5, timesteps: 10240\n",
            "Mean reward: -46.40, mean episode length: 46.51\n",
            "Losses: {'policy_loss': np.float64(-0.02222925626556389), 'value_loss': np.float64(0.4715606348123401), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.009336528126732446)}\n",
            "Iteration: 6, timesteps: 12288\n",
            "Mean reward: -42.77, mean episode length: 42.96\n",
            "Losses: {'policy_loss': np.float64(-0.02303630369133316), 'value_loss': np.float64(0.5187560095917434), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.00910617681511212)}\n",
            "Iteration: 7, timesteps: 14336\n",
            "Mean reward: -44.28, mean episode length: 44.46\n",
            "Losses: {'policy_loss': np.float64(-0.02531248875311576), 'value_loss': np.float64(0.4907371859997511), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.010847332206321881)}\n",
            "Iteration: 8, timesteps: 16384\n",
            "Mean reward: -40.16, mean episode length: 40.42\n",
            "Losses: {'policy_loss': np.float64(-0.023637738448451275), 'value_loss': np.float64(0.5130229691043496), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.010800632476457395)}\n",
            "Iteration: 9, timesteps: 18432\n",
            "Mean reward: -41.96, mean episode length: 42.17\n",
            "Losses: {'policy_loss': np.float64(-0.029020567482803017), 'value_loss': np.float64(0.474664205359295), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.012677657962194644)}\n",
            "Iteration: 10, timesteps: 20480\n",
            "Mean reward: -40.96, mean episode length: 41.16\n",
            "Losses: {'policy_loss': np.float64(-0.027324190907529557), 'value_loss': np.float64(0.4213272732216865), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.012653365041478537)}\n",
            "Iteration: 11, timesteps: 22528\n",
            "Mean reward: -39.88, mean episode length: 40.12\n",
            "Losses: {'policy_loss': np.float64(-0.024397479518665933), 'value_loss': np.float64(0.4092202828731388), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01184123823477421)}\n",
            "Iteration: 12, timesteps: 24576\n",
            "Mean reward: -35.48, mean episode length: 35.84\n",
            "Losses: {'policy_loss': np.float64(-0.027225184335839002), 'value_loss': np.float64(0.5012251591309905), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.012727573054144159)}\n",
            "Iteration: 13, timesteps: 26624\n",
            "Mean reward: -38.09, mean episode length: 38.40\n",
            "Losses: {'policy_loss': np.float64(-0.023871598750702105), 'value_loss': np.float64(0.3760184154380113), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.011805109222768806)}\n",
            "Iteration: 14, timesteps: 28672\n",
            "Mean reward: -39.10, mean episode length: 39.35\n",
            "Losses: {'policy_loss': np.float64(-0.02517375324096065), 'value_loss': np.float64(0.37644261424429715), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.011008624435635283)}\n",
            "Iteration: 15, timesteps: 30720\n",
            "Mean reward: -37.62, mean episode length: 37.89\n",
            "Losses: {'policy_loss': np.float64(-0.02366576207568869), 'value_loss': np.float64(0.4489298318978399), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01204344782163389)}\n",
            "Iteration: 16, timesteps: 32768\n",
            "Mean reward: -39.25, mean episode length: 39.49\n",
            "Losses: {'policy_loss': np.float64(-0.024019438860705122), 'value_loss': np.float64(0.3607478729216382), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014259246617439204)}\n",
            "Early stopping at epoch 10/10 due to reaching KL target\n",
            "Iteration: 17, timesteps: 34816\n",
            "Mean reward: -42.40, mean episode length: 42.57\n",
            "Losses: {'policy_loss': np.float64(-0.028801940311677755), 'value_loss': np.float64(0.36781850196421145), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01532131512358319)}\n",
            "Early stopping at epoch 9/10 due to reaching KL target\n",
            "Iteration: 18, timesteps: 36864\n",
            "Mean reward: -40.04, mean episode length: 40.26\n",
            "Losses: {'policy_loss': np.float64(-0.025548645929019485), 'value_loss': np.float64(0.3595223394739959), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015141322714043781)}\n",
            "Iteration: 19, timesteps: 38912\n",
            "Mean reward: -38.77, mean episode length: 39.02\n",
            "Losses: {'policy_loss': np.float64(-0.024171161605045198), 'value_loss': np.float64(0.38071188328322025), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.013956595561467111)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 20, timesteps: 40960\n",
            "Mean reward: -35.32, mean episode length: 35.63\n",
            "Losses: {'policy_loss': np.float64(-0.012431130307959393), 'value_loss': np.float64(0.3489944523316808), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016066671327280346)}\n",
            "Iteration: 21, timesteps: 43008\n",
            "Mean reward: -36.00, mean episode length: 36.30\n",
            "Losses: {'policy_loss': np.float64(-0.022242241376079618), 'value_loss': np.float64(0.3771304880734533), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.011762934236321599)}\n",
            "Early stopping at epoch 8/10 due to reaching KL target\n",
            "Iteration: 22, timesteps: 45056\n",
            "Mean reward: -37.17, mean episode length: 37.44\n",
            "Losses: {'policy_loss': np.float64(-0.024643628275953233), 'value_loss': np.float64(0.3692269735620357), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015268827490217518)}\n",
            "Early stopping at epoch 10/10 due to reaching KL target\n",
            "Iteration: 23, timesteps: 47104\n",
            "Mean reward: -33.57, mean episode length: 33.92\n",
            "Losses: {'policy_loss': np.float64(-0.0244776096456917), 'value_loss': np.float64(0.3590728725772351), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01512587706092745)}\n",
            "Iteration: 24, timesteps: 49152\n",
            "Mean reward: -36.23, mean episode length: 36.54\n",
            "Losses: {'policy_loss': np.float64(-0.020636381235090085), 'value_loss': np.float64(0.3703472821507603), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014570542742148973)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_49152.pth\n",
            "Early stopping at epoch 6/10 due to reaching KL target\n",
            "Iteration: 25, timesteps: 51200\n",
            "Mean reward: -37.52, mean episode length: 37.80\n",
            "Losses: {'policy_loss': np.float64(-0.015039616460853722), 'value_loss': np.float64(0.33688461400258046), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015164778800681233)}\n",
            "Evaluation: Mean reward: -36.20, mean episode length: 36.50\n",
            "Early stopping at epoch 8/10 due to reaching KL target\n",
            "Iteration: 26, timesteps: 53248\n",
            "Mean reward: -29.19, mean episode length: 29.65\n",
            "Losses: {'policy_loss': np.float64(-0.022642569056188222), 'value_loss': np.float64(0.43164264579536393), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015357946904259734)}\n",
            "Iteration: 27, timesteps: 55296\n",
            "Mean reward: -30.10, mean episode length: 30.54\n",
            "Losses: {'policy_loss': np.float64(-0.023682909339549953), 'value_loss': np.float64(0.3912910779006779), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014103618406807072)}\n",
            "Iteration: 28, timesteps: 57344\n",
            "Mean reward: -32.13, mean episode length: 32.51\n",
            "Losses: {'policy_loss': np.float64(-0.02329300984274596), 'value_loss': np.float64(0.31307273455895485), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014965580700663849)}\n",
            "Early stopping at epoch 9/10 due to reaching KL target\n",
            "Iteration: 29, timesteps: 59392\n",
            "Mean reward: -30.92, mean episode length: 31.33\n",
            "Losses: {'policy_loss': np.float64(-0.018405184059196875), 'value_loss': np.float64(0.3511108226246304), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01516587425269083)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 30, timesteps: 61440\n",
            "Mean reward: -34.08, mean episode length: 34.42\n",
            "Losses: {'policy_loss': np.float64(-0.01699435914051719), 'value_loss': np.float64(0.3361074393149465), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015163844521157443)}\n",
            "Iteration: 31, timesteps: 63488\n",
            "Mean reward: -30.92, mean episode length: 31.33\n",
            "Losses: {'policy_loss': np.float64(-0.01790608930459712), 'value_loss': np.float64(0.3865076558897272), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.012531399689032696)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 32, timesteps: 65536\n",
            "Mean reward: -32.44, mean episode length: 32.82\n",
            "Losses: {'policy_loss': np.float64(-0.014815846749115735), 'value_loss': np.float64(0.359561818279326), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015134883631253615)}\n",
            "Iteration: 33, timesteps: 67584\n",
            "Mean reward: -29.26, mean episode length: 29.71\n",
            "Losses: {'policy_loss': np.float64(-0.01780087479564827), 'value_loss': np.float64(0.35430076404009014), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014077652868581935)}\n",
            "Early stopping at epoch 9/10 due to reaching KL target\n",
            "Iteration: 34, timesteps: 69632\n",
            "Mean reward: -31.61, mean episode length: 32.00\n",
            "Losses: {'policy_loss': np.float64(-0.018542505904204316), 'value_loss': np.float64(0.36303505865443086), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015301956109599106)}\n",
            "Iteration: 35, timesteps: 71680\n",
            "Mean reward: -31.11, mean episode length: 31.52\n",
            "Losses: {'policy_loss': np.float64(-0.020054290306870826), 'value_loss': np.float64(0.3103643413167447), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.012973244459135457)}\n",
            "Iteration: 36, timesteps: 73728\n",
            "Mean reward: -31.00, mean episode length: 31.41\n",
            "Losses: {'policy_loss': np.float64(-0.022152518258371855), 'value_loss': np.float64(0.30275680096819996), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014912482391810044)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 37, timesteps: 75776\n",
            "Mean reward: -36.11, mean episode length: 36.41\n",
            "Losses: {'policy_loss': np.float64(-0.011210340569959953), 'value_loss': np.float64(0.3254871578537859), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01547621931968024)}\n",
            "Early stopping at epoch 7/10 due to reaching KL target\n",
            "Iteration: 38, timesteps: 77824\n",
            "Mean reward: -31.00, mean episode length: 31.41\n",
            "Losses: {'policy_loss': np.float64(-0.015350540904494534), 'value_loss': np.float64(0.36002637080049943), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015360065349211385)}\n",
            "Early stopping at epoch 10/10 due to reaching KL target\n",
            "Iteration: 39, timesteps: 79872\n",
            "Mean reward: -27.49, mean episode length: 27.97\n",
            "Losses: {'policy_loss': np.float64(-0.02300209898094181), 'value_loss': np.float64(0.42494011716917157), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015437209035735578)}\n",
            "Early stopping at epoch 8/10 due to reaching KL target\n",
            "Iteration: 40, timesteps: 81920\n",
            "Mean reward: -27.81, mean episode length: 28.28\n",
            "Losses: {'policy_loss': np.float64(-0.01961641382877133), 'value_loss': np.float64(0.4533665995695628), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015059479508636286)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 41, timesteps: 83968\n",
            "Mean reward: -31.53, mean episode length: 31.92\n",
            "Losses: {'policy_loss': np.float64(-0.011582108214497566), 'value_loss': np.float64(0.3190177488140762), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015138100052718073)}\n",
            "Iteration: 42, timesteps: 86016\n",
            "Mean reward: -29.93, mean episode length: 30.36\n",
            "Losses: {'policy_loss': np.float64(-0.02070324846135918), 'value_loss': np.float64(0.41346592819318173), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014857310478691942)}\n",
            "Iteration: 43, timesteps: 88064\n",
            "Mean reward: -24.89, mean episode length: 25.42\n",
            "Losses: {'policy_loss': np.float64(-0.018486449788906612), 'value_loss': np.float64(0.36679666182026266), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.014643695356789976)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 44, timesteps: 90112\n",
            "Mean reward: -25.96, mean episode length: 26.48\n",
            "Losses: {'policy_loss': np.float64(-0.01060341737077882), 'value_loss': np.float64(0.4660937264561653), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01605828200505736)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 45, timesteps: 92160\n",
            "Mean reward: -22.89, mean episode length: 23.47\n",
            "Losses: {'policy_loss': np.float64(-0.003027735724269102), 'value_loss': np.float64(0.41947199155886966), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015093806112417951)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 46, timesteps: 94208\n",
            "Mean reward: -26.25, mean episode length: 26.76\n",
            "Losses: {'policy_loss': np.float64(-0.015008731675334275), 'value_loss': np.float64(0.3693515559658408), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015638716926332563)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 47, timesteps: 96256\n",
            "Mean reward: -28.83, mean episode length: 29.29\n",
            "Losses: {'policy_loss': np.float64(-0.01352089739521034), 'value_loss': np.float64(0.3877957619726658), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015019717096583918)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 48, timesteps: 98304\n",
            "Mean reward: -28.88, mean episode length: 29.33\n",
            "Losses: {'policy_loss': np.float64(-0.006890373508213088), 'value_loss': np.float64(0.3990918130148202), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01513482091104379)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_98304.pth\n",
            "Iteration: 49, timesteps: 100352\n",
            "Mean reward: -23.24, mean episode length: 23.81\n",
            "Losses: {'policy_loss': np.float64(-0.021986078887130132), 'value_loss': np.float64(0.4938504952006042), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01491647798975464)}\n",
            "Early stopping at epoch 6/10 due to reaching KL target\n",
            "Iteration: 50, timesteps: 102400\n",
            "Mean reward: -19.22, mean episode length: 19.88\n",
            "Losses: {'policy_loss': np.float64(-0.012749318280839361), 'value_loss': np.float64(0.5767135020966331), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015393520060266988)}\n",
            "Evaluation: Mean reward: -2.70, mean episode length: 3.70\n",
            "Early stopping at epoch 9/10 due to reaching KL target\n",
            "Iteration: 51, timesteps: 104448\n",
            "Mean reward: -19.70, mean episode length: 20.34\n",
            "Losses: {'policy_loss': np.float64(-0.01952248167557021), 'value_loss': np.float64(0.4102431664553781), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015255495168579122)}\n",
            "Early stopping at epoch 6/10 due to reaching KL target\n",
            "Iteration: 52, timesteps: 106496\n",
            "Mean reward: -24.72, mean episode length: 25.26\n",
            "Losses: {'policy_loss': np.float64(-0.017510498636208165), 'value_loss': np.float64(0.4139957998801644), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015354997488126779)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 53, timesteps: 108544\n",
            "Mean reward: -15.83, mean episode length: 16.55\n",
            "Losses: {'policy_loss': np.float64(-0.0033516666347471378), 'value_loss': np.float64(0.5535953824097911), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01827779535475808)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 54, timesteps: 110592\n",
            "Mean reward: -23.49, mean episode length: 24.06\n",
            "Losses: {'policy_loss': np.float64(-0.0029785416748685143), 'value_loss': np.float64(0.38480656951044995), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017936936948293198)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 55, timesteps: 112640\n",
            "Mean reward: -21.50, mean episode length: 22.11\n",
            "Losses: {'policy_loss': np.float64(0.0020206846626630672), 'value_loss': np.float64(0.42948981871207553), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016705318266758695)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 56, timesteps: 114688\n",
            "Mean reward: -19.08, mean episode length: 19.74\n",
            "Losses: {'policy_loss': np.float64(-0.013436874462058768), 'value_loss': np.float64(0.3977750289253891), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015495989669580013)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 57, timesteps: 116736\n",
            "Mean reward: -22.13, mean episode length: 22.73\n",
            "Losses: {'policy_loss': np.float64(-0.010144420491997153), 'value_loss': np.float64(0.4096736472565681), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01592547669861233)}\n",
            "Early stopping at epoch 7/10 due to reaching KL target\n",
            "Iteration: 58, timesteps: 118784\n",
            "Mean reward: -18.83, mean episode length: 19.50\n",
            "Losses: {'policy_loss': np.float64(-0.013151127042614721), 'value_loss': np.float64(0.36995283820267233), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015267128819167348)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 59, timesteps: 120832\n",
            "Mean reward: -16.95, mean episode length: 17.65\n",
            "Losses: {'policy_loss': np.float64(0.010362353699747473), 'value_loss': np.float64(0.5459816595539451), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016672187543008476)}\n",
            "Early stopping at epoch 8/10 due to reaching KL target\n",
            "Iteration: 60, timesteps: 122880\n",
            "Mean reward: -17.88, mean episode length: 18.57\n",
            "Losses: {'policy_loss': np.float64(-0.020545297516946448), 'value_loss': np.float64(0.5001991834724322), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01540444032434607)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 61, timesteps: 124928\n",
            "Mean reward: -21.90, mean episode length: 22.51\n",
            "Losses: {'policy_loss': np.float64(0.009544676868245006), 'value_loss': np.float64(0.5482721785083413), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017234359052963555)}\n",
            "Early stopping at epoch 7/10 due to reaching KL target\n",
            "Iteration: 62, timesteps: 126976\n",
            "Mean reward: -21.40, mean episode length: 22.01\n",
            "Losses: {'policy_loss': np.float64(-0.02111937340565159), 'value_loss': np.float64(0.49790720654917614), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015396354744942593)}\n",
            "Early stopping at epoch 6/10 due to reaching KL target\n",
            "Iteration: 63, timesteps: 129024\n",
            "Mean reward: -18.32, mean episode length: 18.99\n",
            "Losses: {'policy_loss': np.float64(-0.014762952011854699), 'value_loss': np.float64(0.5166053179030617), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015667913146899082)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 64, timesteps: 131072\n",
            "Mean reward: -16.89, mean episode length: 17.60\n",
            "Losses: {'policy_loss': np.float64(-0.013870273111388087), 'value_loss': np.float64(0.5139909512363374), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015315458877012134)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 65, timesteps: 133120\n",
            "Mean reward: -25.20, mean episode length: 25.73\n",
            "Losses: {'policy_loss': np.float64(0.0028797049017157406), 'value_loss': np.float64(0.40157043452685076), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015816505251374718)}\n",
            "Early stopping at epoch 6/10 due to reaching KL target\n",
            "Iteration: 66, timesteps: 135168\n",
            "Mean reward: -18.97, mean episode length: 19.64\n",
            "Losses: {'policy_loss': np.float64(-0.015113070117270885), 'value_loss': np.float64(0.483345160416017), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015259018080541864)}\n",
            "Early stopping at epoch 7/10 due to reaching KL target\n",
            "Iteration: 67, timesteps: 137216\n",
            "Mean reward: -14.29, mean episode length: 15.06\n",
            "Losses: {'policy_loss': np.float64(-0.012841503476790552), 'value_loss': np.float64(0.4857419758502926), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015169235777908139)}\n",
            "Early stopping at epoch 7/10 due to reaching KL target\n",
            "Iteration: 68, timesteps: 139264\n",
            "Mean reward: -15.49, mean episode length: 16.23\n",
            "Losses: {'policy_loss': np.float64(-0.013311551867185958), 'value_loss': np.float64(0.43066654946388944), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015021231108611184)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 69, timesteps: 141312\n",
            "Mean reward: -14.51, mean episode length: 15.27\n",
            "Losses: {'policy_loss': np.float64(-0.003972102596890181), 'value_loss': np.float64(0.5366620169952512), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016247114748694003)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 70, timesteps: 143360\n",
            "Mean reward: -12.86, mean episode length: 13.66\n",
            "Losses: {'policy_loss': np.float64(-0.007108136429451406), 'value_loss': np.float64(0.5745391414966434), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015516206352913287)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 71, timesteps: 145408\n",
            "Mean reward: -12.52, mean episode length: 13.32\n",
            "Losses: {'policy_loss': np.float64(-0.011208607582375407), 'value_loss': np.float64(0.5326529643498361), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016276854141324293)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 72, timesteps: 147456\n",
            "Mean reward: -11.84, mean episode length: 12.66\n",
            "Losses: {'policy_loss': np.float64(0.0035316383582539856), 'value_loss': np.float64(0.4641897997353226), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01582965069974307)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_147456.pth\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 73, timesteps: 149504\n",
            "Mean reward: -13.64, mean episode length: 14.41\n",
            "Losses: {'policy_loss': np.float64(0.008763044912484474), 'value_loss': np.float64(0.5393092683516443), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016369167555239983)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 74, timesteps: 151552\n",
            "Mean reward: -13.96, mean episode length: 14.75\n",
            "Losses: {'policy_loss': np.float64(0.0011485170398373157), 'value_loss': np.float64(0.5375950797460973), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015639720819308423)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 75, timesteps: 153600\n",
            "Mean reward: -11.69, mean episode length: 12.52\n",
            "Losses: {'policy_loss': np.float64(-0.003687489515868947), 'value_loss': np.float64(0.5036297105252743), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015095047987415455)}\n",
            "Evaluation: Mean reward: -2.60, mean episode length: 3.60\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 76, timesteps: 155648\n",
            "Mean reward: -11.68, mean episode length: 12.50\n",
            "Losses: {'policy_loss': np.float64(-0.010590631305240095), 'value_loss': np.float64(0.46759952707216146), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01609797211131081)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 77, timesteps: 157696\n",
            "Mean reward: -10.95, mean episode length: 11.78\n",
            "Losses: {'policy_loss': np.float64(0.0053461925708688796), 'value_loss': np.float64(0.47040171176195145), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02193173523119185)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 78, timesteps: 159744\n",
            "Mean reward: -14.38, mean episode length: 15.14\n",
            "Losses: {'policy_loss': np.float64(-0.001883828789383794), 'value_loss': np.float64(0.44640411157161), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015200898352001483)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 79, timesteps: 161792\n",
            "Mean reward: -15.00, mean episode length: 15.75\n",
            "Losses: {'policy_loss': np.float64(0.009433103783521801), 'value_loss': np.float64(0.5376765038818121), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017591518830158748)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 80, timesteps: 163840\n",
            "Mean reward: -13.02, mean episode length: 13.81\n",
            "Losses: {'policy_loss': np.float64(-0.00021579687017947435), 'value_loss': np.float64(0.44661364424973726), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015821073917322792)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 81, timesteps: 165888\n",
            "Mean reward: -12.04, mean episode length: 12.85\n",
            "Losses: {'policy_loss': np.float64(-0.006397094010026194), 'value_loss': np.float64(0.45929833606351167), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015115820686332881)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 82, timesteps: 167936\n",
            "Mean reward: -11.27, mean episode length: 12.10\n",
            "Losses: {'policy_loss': np.float64(-0.011409732804168016), 'value_loss': np.float64(0.5278447261080146), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01589647090440849)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 83, timesteps: 169984\n",
            "Mean reward: -11.43, mean episode length: 12.26\n",
            "Losses: {'policy_loss': np.float64(0.006338744860840961), 'value_loss': np.float64(0.487115188036114), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021060625091195107)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 84, timesteps: 172032\n",
            "Mean reward: -13.52, mean episode length: 14.30\n",
            "Losses: {'policy_loss': np.float64(-0.01234048359328881), 'value_loss': np.float64(0.4067672424018383), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01659480914240703)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 85, timesteps: 174080\n",
            "Mean reward: -11.26, mean episode length: 12.09\n",
            "Losses: {'policy_loss': np.float64(0.004441456840140745), 'value_loss': np.float64(0.48988634161651134), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018843250669306144)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 86, timesteps: 176128\n",
            "Mean reward: -12.37, mean episode length: 13.18\n",
            "Losses: {'policy_loss': np.float64(-0.0013328664499567822), 'value_loss': np.float64(0.4508323702029884), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016602211355348118)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 87, timesteps: 178176\n",
            "Mean reward: -11.21, mean episode length: 12.03\n",
            "Losses: {'policy_loss': np.float64(-0.00659392325906083), 'value_loss': np.float64(0.4645893913693726), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01556269169668667)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 88, timesteps: 180224\n",
            "Mean reward: -10.33, mean episode length: 11.19\n",
            "Losses: {'policy_loss': np.float64(0.015080942306667566), 'value_loss': np.float64(0.5951371593400836), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0236327271850314)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 89, timesteps: 182272\n",
            "Mean reward: -16.39, mean episode length: 17.12\n",
            "Losses: {'policy_loss': np.float64(0.0057827726704999804), 'value_loss': np.float64(0.43822945887222886), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015103121288120747)}\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 90, timesteps: 184320\n",
            "Mean reward: -10.91, mean episode length: 11.74\n",
            "Losses: {'policy_loss': np.float64(-0.011597477633040398), 'value_loss': np.float64(0.4926194974221289), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015224443387705833)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 91, timesteps: 186368\n",
            "Mean reward: -10.58, mean episode length: 11.42\n",
            "Losses: {'policy_loss': np.float64(-0.00448731553236333), 'value_loss': np.float64(0.4415822407851617), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016549696437626455)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 92, timesteps: 188416\n",
            "Mean reward: -10.58, mean episode length: 11.42\n",
            "Losses: {'policy_loss': np.float64(0.009812407719437033), 'value_loss': np.float64(0.49103852454572916), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01720023268717341)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 93, timesteps: 190464\n",
            "Mean reward: -13.82, mean episode length: 14.59\n",
            "Losses: {'policy_loss': np.float64(-0.003955318902929624), 'value_loss': np.float64(0.442620403598994), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01606351468944922)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 94, timesteps: 192512\n",
            "Mean reward: -12.22, mean episode length: 13.04\n",
            "Losses: {'policy_loss': np.float64(6.567826494574547e-05), 'value_loss': np.float64(0.48119315225631), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0214273711462738)}\n",
            "Early stopping at epoch 4/10 due to reaching KL target\n",
            "Iteration: 95, timesteps: 194560\n",
            "Mean reward: -12.15, mean episode length: 12.96\n",
            "Losses: {'policy_loss': np.float64(-0.008573655708460137), 'value_loss': np.float64(0.4332535385619849), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015707852646301035)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 96, timesteps: 196608\n",
            "Mean reward: -10.07, mean episode length: 10.93\n",
            "Losses: {'policy_loss': np.float64(0.004264247138053179), 'value_loss': np.float64(0.4817489879205823), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017284314177231863)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_196608.pth\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 97, timesteps: 198656\n",
            "Mean reward: -10.25, mean episode length: 11.11\n",
            "Losses: {'policy_loss': np.float64(-0.007334773108595982), 'value_loss': np.float64(0.4801452125733097), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01658923212865678)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 98, timesteps: 200704\n",
            "Mean reward: -11.21, mean episode length: 12.05\n",
            "Losses: {'policy_loss': np.float64(0.0028981899231439456), 'value_loss': np.float64(0.4740692023187876), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017141167452791706)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 99, timesteps: 202752\n",
            "Mean reward: -12.78, mean episode length: 13.58\n",
            "Losses: {'policy_loss': np.float64(0.00016574263766718408), 'value_loss': np.float64(0.4249498639255762), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016086497188856203)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 100, timesteps: 204800\n",
            "Mean reward: -10.18, mean episode length: 11.04\n",
            "Losses: {'policy_loss': np.float64(0.000914408898097463), 'value_loss': np.float64(0.4478906402364373), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015511062330915593)}\n",
            "Evaluation: Mean reward: -7.30, mean episode length: 8.20\n",
            "Early stopping at epoch 5/10 due to reaching KL target\n",
            "Iteration: 101, timesteps: 206848\n",
            "Mean reward: -11.96, mean episode length: 12.78\n",
            "Losses: {'policy_loss': np.float64(-0.012261822656728327), 'value_loss': np.float64(0.4621857926249504), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01581109907128848)}\n",
            "Early stopping at epoch 8/10 due to reaching KL target\n",
            "Iteration: 102, timesteps: 208896\n",
            "Mean reward: -10.64, mean episode length: 11.49\n",
            "Losses: {'policy_loss': np.float64(-0.016917051529162563), 'value_loss': np.float64(0.3858767217025161), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015535875423665857)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 103, timesteps: 210944\n",
            "Mean reward: -8.35, mean episode length: 9.24\n",
            "Losses: {'policy_loss': np.float64(0.013650197419337928), 'value_loss': np.float64(0.43668686877936125), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01675365655682981)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 104, timesteps: 212992\n",
            "Mean reward: -8.08, mean episode length: 8.98\n",
            "Losses: {'policy_loss': np.float64(0.004049603681778535), 'value_loss': np.float64(0.4797594645060599), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017230875862878747)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 105, timesteps: 215040\n",
            "Mean reward: -8.47, mean episode length: 9.37\n",
            "Losses: {'policy_loss': np.float64(0.007519653532654047), 'value_loss': np.float64(0.4687528293579817), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017629748006584123)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 106, timesteps: 217088\n",
            "Mean reward: -7.36, mean episode length: 8.28\n",
            "Losses: {'policy_loss': np.float64(0.006696963318972848), 'value_loss': np.float64(0.40341698471456766), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01639094896381721)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 107, timesteps: 219136\n",
            "Mean reward: -9.19, mean episode length: 10.06\n",
            "Losses: {'policy_loss': np.float64(0.00974410743219778), 'value_loss': np.float64(0.41358456294983625), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01794874426559545)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 108, timesteps: 221184\n",
            "Mean reward: -8.79, mean episode length: 9.68\n",
            "Losses: {'policy_loss': np.float64(0.0083470877725631), 'value_loss': np.float64(0.47400517109781504), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02193468829500489)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 109, timesteps: 223232\n",
            "Mean reward: -10.18, mean episode length: 11.04\n",
            "Losses: {'policy_loss': np.float64(0.007028856663964689), 'value_loss': np.float64(0.4655247931368649), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018403982190648094)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 110, timesteps: 225280\n",
            "Mean reward: -10.52, mean episode length: 11.37\n",
            "Losses: {'policy_loss': np.float64(0.010255211149342358), 'value_loss': np.float64(0.48194538056850433), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021912188560236245)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 111, timesteps: 227328\n",
            "Mean reward: -8.05, mean episode length: 8.95\n",
            "Losses: {'policy_loss': np.float64(0.006543338662595488), 'value_loss': np.float64(0.43562849471345544), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017618410027353093)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 112, timesteps: 229376\n",
            "Mean reward: -8.15, mean episode length: 9.05\n",
            "Losses: {'policy_loss': np.float64(0.00644698430551216), 'value_loss': np.float64(0.4596534287557006), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01821886096149683)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 113, timesteps: 231424\n",
            "Mean reward: -8.98, mean episode length: 9.87\n",
            "Losses: {'policy_loss': np.float64(0.005009382090065628), 'value_loss': np.float64(0.4615700086578727), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019180948875145987)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 114, timesteps: 233472\n",
            "Mean reward: -6.86, mean episode length: 7.79\n",
            "Losses: {'policy_loss': np.float64(0.006073008931707591), 'value_loss': np.float64(0.366601089714095), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01695134687179234)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 115, timesteps: 235520\n",
            "Mean reward: -9.22, mean episode length: 10.10\n",
            "Losses: {'policy_loss': np.float64(0.01380723767215386), 'value_loss': np.float64(0.419431135058403), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018634502572240308)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 116, timesteps: 237568\n",
            "Mean reward: -7.43, mean episode length: 8.35\n",
            "Losses: {'policy_loss': np.float64(-0.006134836206911132), 'value_loss': np.float64(0.38783178897574544), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01649929187260568)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 117, timesteps: 239616\n",
            "Mean reward: -7.37, mean episode length: 8.28\n",
            "Losses: {'policy_loss': np.float64(-0.0022245834552450106), 'value_loss': np.float64(0.4131308838259429), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018065017226035707)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 118, timesteps: 241664\n",
            "Mean reward: -7.58, mean episode length: 8.49\n",
            "Losses: {'policy_loss': np.float64(0.006890075484989211), 'value_loss': np.float64(0.3446815807837993), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016566953854635358)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 119, timesteps: 243712\n",
            "Mean reward: -8.53, mean episode length: 9.42\n",
            "Losses: {'policy_loss': np.float64(0.0048781142686493695), 'value_loss': np.float64(0.42181317787617445), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024240350554464385)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 120, timesteps: 245760\n",
            "Mean reward: -8.73, mean episode length: 9.62\n",
            "Losses: {'policy_loss': np.float64(0.00923715572571382), 'value_loss': np.float64(0.42960712825879455), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020722791377920657)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_245760.pth\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 121, timesteps: 247808\n",
            "Mean reward: -7.59, mean episode length: 8.49\n",
            "Losses: {'policy_loss': np.float64(0.0031903552589938045), 'value_loss': np.float64(0.43865694873966277), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01562716072658077)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 122, timesteps: 249856\n",
            "Mean reward: -8.90, mean episode length: 9.78\n",
            "Losses: {'policy_loss': np.float64(0.005520565522601828), 'value_loss': np.float64(0.4804887566715479), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018436914106132463)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 123, timesteps: 251904\n",
            "Mean reward: -6.61, mean episode length: 7.55\n",
            "Losses: {'policy_loss': np.float64(-0.002256792227854021), 'value_loss': np.float64(0.39354456262663007), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018337222514674067)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 124, timesteps: 253952\n",
            "Mean reward: -8.14, mean episode length: 9.05\n",
            "Losses: {'policy_loss': np.float64(0.001820167584810406), 'value_loss': np.float64(0.3788175950758159), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020953096420271322)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 125, timesteps: 256000\n",
            "Mean reward: -7.04, mean episode length: 7.97\n",
            "Losses: {'policy_loss': np.float64(0.0076054351520724595), 'value_loss': np.float64(0.3767229113727808), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01802484638756141)}\n",
            "Evaluation: Mean reward: -7.30, mean episode length: 8.20\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 126, timesteps: 258048\n",
            "Mean reward: -8.06, mean episode length: 8.96\n",
            "Losses: {'policy_loss': np.float64(0.0019234299543313682), 'value_loss': np.float64(0.38285981118679047), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025014372484292835)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 127, timesteps: 260096\n",
            "Mean reward: -7.53, mean episode length: 8.46\n",
            "Losses: {'policy_loss': np.float64(0.008956019737524912), 'value_loss': np.float64(0.37219483964145184), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018206731998361647)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 128, timesteps: 262144\n",
            "Mean reward: -8.49, mean episode length: 9.38\n",
            "Losses: {'policy_loss': np.float64(-0.0018764650449156761), 'value_loss': np.float64(0.37310218694619834), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016580530456849374)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 129, timesteps: 264192\n",
            "Mean reward: -6.81, mean episode length: 7.75\n",
            "Losses: {'policy_loss': np.float64(0.010821269184816629), 'value_loss': np.float64(0.39643074991181493), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018570699670817703)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 130, timesteps: 266240\n",
            "Mean reward: -8.20, mean episode length: 9.10\n",
            "Losses: {'policy_loss': np.float64(0.0034973851579707116), 'value_loss': np.float64(0.40969906002283096), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01938941769185476)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 131, timesteps: 268288\n",
            "Mean reward: -6.60, mean episode length: 7.54\n",
            "Losses: {'policy_loss': np.float64(0.012187507003545761), 'value_loss': np.float64(0.40810240153223276), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015732832456706092)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 132, timesteps: 270336\n",
            "Mean reward: -7.57, mean episode length: 8.48\n",
            "Losses: {'policy_loss': np.float64(-0.004423093962638329), 'value_loss': np.float64(0.33523360701898736), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01737812477707242)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 133, timesteps: 272384\n",
            "Mean reward: -7.37, mean episode length: 8.29\n",
            "Losses: {'policy_loss': np.float64(0.007326007296796888), 'value_loss': np.float64(0.39008602127432823), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020681158493971452)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 134, timesteps: 274432\n",
            "Mean reward: -6.68, mean episode length: 7.61\n",
            "Losses: {'policy_loss': np.float64(0.005912701191846281), 'value_loss': np.float64(0.3232621527276933), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017100426892284304)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 135, timesteps: 276480\n",
            "Mean reward: -9.20, mean episode length: 10.07\n",
            "Losses: {'policy_loss': np.float64(0.006042218185029924), 'value_loss': np.float64(0.4199703596532345), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0187909941887483)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 136, timesteps: 278528\n",
            "Mean reward: -7.58, mean episode length: 8.50\n",
            "Losses: {'policy_loss': np.float64(0.013437498593702912), 'value_loss': np.float64(0.3962095705792308), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023795550339855254)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 137, timesteps: 280576\n",
            "Mean reward: -8.03, mean episode length: 8.94\n",
            "Losses: {'policy_loss': np.float64(-0.004334851051680744), 'value_loss': np.float64(0.40222229063510895), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017134374196757562)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 138, timesteps: 282624\n",
            "Mean reward: -6.84, mean episode length: 7.78\n",
            "Losses: {'policy_loss': np.float64(0.0013875521253794432), 'value_loss': np.float64(0.3938598125241697), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01642356932279654)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 139, timesteps: 284672\n",
            "Mean reward: -7.34, mean episode length: 8.25\n",
            "Losses: {'policy_loss': np.float64(0.0014676665014121681), 'value_loss': np.float64(0.3488682103343308), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015293797696358524)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 140, timesteps: 286720\n",
            "Mean reward: -5.61, mean episode length: 6.56\n",
            "Losses: {'policy_loss': np.float64(0.007578708464279771), 'value_loss': np.float64(0.32613250194117427), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015123419201700017)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 141, timesteps: 288768\n",
            "Mean reward: -5.71, mean episode length: 6.66\n",
            "Losses: {'policy_loss': np.float64(0.009971791470889002), 'value_loss': np.float64(0.30989129678346217), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023771311447490007)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 142, timesteps: 290816\n",
            "Mean reward: -6.49, mean episode length: 7.42\n",
            "Losses: {'policy_loss': np.float64(0.004773705790285021), 'value_loss': np.float64(0.3487495267763734), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018343634612392634)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 143, timesteps: 292864\n",
            "Mean reward: -7.85, mean episode length: 8.76\n",
            "Losses: {'policy_loss': np.float64(0.007131763966754079), 'value_loss': np.float64(0.39422896318137646), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017888244474306703)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 144, timesteps: 294912\n",
            "Mean reward: -6.36, mean episode length: 7.31\n",
            "Losses: {'policy_loss': np.float64(-0.0009244517811263601), 'value_loss': np.float64(0.3469083442663153), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015255123066405455)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_294912.pth\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 145, timesteps: 296960\n",
            "Mean reward: -5.99, mean episode length: 6.94\n",
            "Losses: {'policy_loss': np.float64(0.001802052662242204), 'value_loss': np.float64(0.3252666471526027), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017438196184230037)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 146, timesteps: 299008\n",
            "Mean reward: -7.17, mean episode length: 8.10\n",
            "Losses: {'policy_loss': np.float64(0.014720421051606536), 'value_loss': np.float64(0.35449888790026307), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018485859414795414)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 147, timesteps: 301056\n",
            "Mean reward: -6.34, mean episode length: 7.28\n",
            "Losses: {'policy_loss': np.float64(0.0075160873821005225), 'value_loss': np.float64(0.3219622219912708), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0224512722925283)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 148, timesteps: 303104\n",
            "Mean reward: -5.79, mean episode length: 6.74\n",
            "Losses: {'policy_loss': np.float64(0.0027112422103527933), 'value_loss': np.float64(0.29237301694229245), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017751672872691415)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 149, timesteps: 305152\n",
            "Mean reward: -7.21, mean episode length: 8.12\n",
            "Losses: {'policy_loss': np.float64(0.010447537759318948), 'value_loss': np.float64(0.37306548887863755), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016711750271497294)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 150, timesteps: 307200\n",
            "Mean reward: -5.88, mean episode length: 6.83\n",
            "Losses: {'policy_loss': np.float64(0.009793708217330277), 'value_loss': np.float64(0.31086655356921256), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020253612950909883)}\n",
            "Evaluation: Mean reward: -3.10, mean episode length: 4.10\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 151, timesteps: 309248\n",
            "Mean reward: -5.78, mean episode length: 6.74\n",
            "Losses: {'policy_loss': np.float64(0.01585332228569314), 'value_loss': np.float64(0.24225781066343188), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019101322075584903)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 152, timesteps: 311296\n",
            "Mean reward: -5.97, mean episode length: 6.92\n",
            "Losses: {'policy_loss': np.float64(0.003421532397624105), 'value_loss': np.float64(0.33083590446040034), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016597072943113744)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 153, timesteps: 313344\n",
            "Mean reward: -5.80, mean episode length: 6.75\n",
            "Losses: {'policy_loss': np.float64(0.006543521478306502), 'value_loss': np.float64(0.3275885006878525), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019280102831544355)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 154, timesteps: 315392\n",
            "Mean reward: -5.80, mean episode length: 6.75\n",
            "Losses: {'policy_loss': np.float64(0.007537039229646325), 'value_loss': np.float64(0.30984514486044645), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03580614808015525)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 155, timesteps: 317440\n",
            "Mean reward: -5.14, mean episode length: 6.11\n",
            "Losses: {'policy_loss': np.float64(0.0036482762661762536), 'value_loss': np.float64(0.26257063378579915), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025249128317227587)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 156, timesteps: 319488\n",
            "Mean reward: -5.42, mean episode length: 6.38\n",
            "Losses: {'policy_loss': np.float64(0.0029621814901474863), 'value_loss': np.float64(0.25567409733776003), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020903741067741066)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 157, timesteps: 321536\n",
            "Mean reward: -5.28, mean episode length: 6.24\n",
            "Losses: {'policy_loss': np.float64(-0.001974421422346495), 'value_loss': np.float64(0.2384481918415986), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018628300589625724)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 158, timesteps: 323584\n",
            "Mean reward: -4.93, mean episode length: 5.90\n",
            "Losses: {'policy_loss': np.float64(0.0016922560171224177), 'value_loss': np.float64(0.22279040166176856), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017199322348460555)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 159, timesteps: 325632\n",
            "Mean reward: -5.07, mean episode length: 6.04\n",
            "Losses: {'policy_loss': np.float64(0.004624966764822602), 'value_loss': np.float64(0.22704129037447274), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021323803783161566)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 160, timesteps: 327680\n",
            "Mean reward: -4.37, mean episode length: 5.36\n",
            "Losses: {'policy_loss': np.float64(0.005404782365076244), 'value_loss': np.float64(0.16139857121743262), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026088104670634493)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 161, timesteps: 329728\n",
            "Mean reward: -4.56, mean episode length: 5.54\n",
            "Losses: {'policy_loss': np.float64(0.016118016676045954), 'value_loss': np.float64(0.1682282981928438), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023880088119767606)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 162, timesteps: 331776\n",
            "Mean reward: -5.26, mean episode length: 6.23\n",
            "Losses: {'policy_loss': np.float64(0.004643297055736184), 'value_loss': np.float64(0.21574243670329452), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01655385224148631)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 163, timesteps: 333824\n",
            "Mean reward: -4.50, mean episode length: 5.49\n",
            "Losses: {'policy_loss': np.float64(0.011887943604961038), 'value_loss': np.float64(0.15394968376494944), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02299262341693975)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 164, timesteps: 335872\n",
            "Mean reward: -5.29, mean episode length: 6.25\n",
            "Losses: {'policy_loss': np.float64(0.01809832942672074), 'value_loss': np.float64(0.251098258420825), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025421854574233294)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 165, timesteps: 337920\n",
            "Mean reward: -4.59, mean episode length: 5.58\n",
            "Losses: {'policy_loss': np.float64(0.0110825318552088), 'value_loss': np.float64(0.1673284552525729), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022684287250740454)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 166, timesteps: 339968\n",
            "Mean reward: -4.48, mean episode length: 5.47\n",
            "Losses: {'policy_loss': np.float64(0.008080545463599265), 'value_loss': np.float64(0.13064578641206026), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02491061674663797)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 167, timesteps: 342016\n",
            "Mean reward: -4.82, mean episode length: 5.80\n",
            "Losses: {'policy_loss': np.float64(0.028012894210405648), 'value_loss': np.float64(0.15271629113703966), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.032970643922453746)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 168, timesteps: 344064\n",
            "Mean reward: -5.59, mean episode length: 6.56\n",
            "Losses: {'policy_loss': np.float64(0.014731140807271004), 'value_loss': np.float64(0.20817100489512086), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025251710147131234)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_344064.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 169, timesteps: 346112\n",
            "Mean reward: -4.14, mean episode length: 5.13\n",
            "Losses: {'policy_loss': np.float64(0.005971847887849435), 'value_loss': np.float64(0.10365443397313356), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01858488522702828)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 170, timesteps: 348160\n",
            "Mean reward: -4.34, mean episode length: 5.33\n",
            "Losses: {'policy_loss': np.float64(0.00806940020993352), 'value_loss': np.float64(0.1323412642814219), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015079192176926881)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 171, timesteps: 350208\n",
            "Mean reward: -4.29, mean episode length: 5.29\n",
            "Losses: {'policy_loss': np.float64(0.01165148738073185), 'value_loss': np.float64(0.09452324884478003), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022412543883547187)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 172, timesteps: 352256\n",
            "Mean reward: -4.67, mean episode length: 5.65\n",
            "Losses: {'policy_loss': np.float64(0.002963467122754082), 'value_loss': np.float64(0.14986733352998272), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01898462465032935)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 173, timesteps: 354304\n",
            "Mean reward: -4.30, mean episode length: 5.29\n",
            "Losses: {'policy_loss': np.float64(0.004029793664813042), 'value_loss': np.float64(0.09431772271636873), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015209277480607852)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 174, timesteps: 356352\n",
            "Mean reward: -4.88, mean episode length: 5.87\n",
            "Losses: {'policy_loss': np.float64(-0.0003841075231321156), 'value_loss': np.float64(0.17556539387442172), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027994789328658953)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 175, timesteps: 358400\n",
            "Mean reward: -5.05, mean episode length: 6.04\n",
            "Losses: {'policy_loss': np.float64(0.01031033875187859), 'value_loss': np.float64(0.17096443055197597), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0169116104952991)}\n",
            "Evaluation: Mean reward: -3.40, mean episode length: 4.40\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 176, timesteps: 360448\n",
            "Mean reward: -4.38, mean episode length: 5.37\n",
            "Losses: {'policy_loss': np.float64(0.008687764639034867), 'value_loss': np.float64(0.12783344450872391), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02142077829921618)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 177, timesteps: 362496\n",
            "Mean reward: -4.56, mean episode length: 5.55\n",
            "Losses: {'policy_loss': np.float64(0.013763140712399036), 'value_loss': np.float64(0.09579732874408364), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01928315300028771)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 178, timesteps: 364544\n",
            "Mean reward: -4.47, mean episode length: 5.45\n",
            "Losses: {'policy_loss': np.float64(0.007620426767971367), 'value_loss': np.float64(0.11707011342514306), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.030886543536325917)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 179, timesteps: 366592\n",
            "Mean reward: -4.56, mean episode length: 5.54\n",
            "Losses: {'policy_loss': np.float64(0.010712583112763241), 'value_loss': np.float64(0.13896696048323065), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021687109518097714)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 180, timesteps: 368640\n",
            "Mean reward: -4.74, mean episode length: 5.72\n",
            "Losses: {'policy_loss': np.float64(0.012438120087608695), 'value_loss': np.float64(0.14507023920305073), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022930334002012387)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 181, timesteps: 370688\n",
            "Mean reward: -4.27, mean episode length: 5.26\n",
            "Losses: {'policy_loss': np.float64(-0.0018825159932021052), 'value_loss': np.float64(0.08085834007943049), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015724657059763558)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 182, timesteps: 372736\n",
            "Mean reward: -4.36, mean episode length: 5.35\n",
            "Losses: {'policy_loss': np.float64(0.003192081057932228), 'value_loss': np.float64(0.10816716810222715), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020272185531212017)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 183, timesteps: 374784\n",
            "Mean reward: -3.75, mean episode length: 4.74\n",
            "Losses: {'policy_loss': np.float64(0.010310935962479562), 'value_loss': np.float64(0.06656625884352252), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02093702505226247)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 184, timesteps: 376832\n",
            "Mean reward: -3.98, mean episode length: 4.97\n",
            "Losses: {'policy_loss': np.float64(0.011291690170764923), 'value_loss': np.float64(0.07598016789415851), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02496667264495045)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 185, timesteps: 378880\n",
            "Mean reward: -4.04, mean episode length: 5.03\n",
            "Losses: {'policy_loss': np.float64(0.01009958644863218), 'value_loss': np.float64(0.09692695684498176), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017810544784879312)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 186, timesteps: 380928\n",
            "Mean reward: -4.10, mean episode length: 5.09\n",
            "Losses: {'policy_loss': np.float64(0.011183553549926728), 'value_loss': np.float64(0.0944154947064817), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01743367171729915)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 187, timesteps: 382976\n",
            "Mean reward: -3.92, mean episode length: 4.91\n",
            "Losses: {'policy_loss': np.float64(0.006389017129549757), 'value_loss': np.float64(0.07379561924608424), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01760683985776268)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 188, timesteps: 385024\n",
            "Mean reward: -3.69, mean episode length: 4.69\n",
            "Losses: {'policy_loss': np.float64(0.005358776717912406), 'value_loss': np.float64(0.024335412570508197), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01997135448618792)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 189, timesteps: 387072\n",
            "Mean reward: -3.87, mean episode length: 4.86\n",
            "Losses: {'policy_loss': np.float64(0.0006522622425109148), 'value_loss': np.float64(0.048459313315106556), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01611254883755464)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 190, timesteps: 389120\n",
            "Mean reward: -3.50, mean episode length: 4.50\n",
            "Losses: {'policy_loss': np.float64(0.015690641477704048), 'value_loss': np.float64(0.03777868827455677), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018878256290918216)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 191, timesteps: 391168\n",
            "Mean reward: -3.88, mean episode length: 4.88\n",
            "Losses: {'policy_loss': np.float64(0.007073041633702815), 'value_loss': np.float64(0.07015988224884495), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016893857216928154)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 192, timesteps: 393216\n",
            "Mean reward: -3.68, mean episode length: 4.68\n",
            "Losses: {'policy_loss': np.float64(0.0105514787719585), 'value_loss': np.float64(0.044991294358624145), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015267588576534763)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_393216.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 193, timesteps: 395264\n",
            "Mean reward: -4.29, mean episode length: 5.28\n",
            "Losses: {'policy_loss': np.float64(0.006055146222934127), 'value_loss': np.float64(0.12224833178333938), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02226554785738699)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 194, timesteps: 397312\n",
            "Mean reward: -3.65, mean episode length: 4.64\n",
            "Losses: {'policy_loss': np.float64(0.011131332401419058), 'value_loss': np.float64(0.038435811205999926), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02126606644014828)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 195, timesteps: 399360\n",
            "Mean reward: -3.64, mean episode length: 4.64\n",
            "Losses: {'policy_loss': np.float64(0.030402470263652503), 'value_loss': np.float64(0.06063160294434056), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025342045759316534)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 196, timesteps: 401408\n",
            "Mean reward: -3.57, mean episode length: 4.57\n",
            "Losses: {'policy_loss': np.float64(0.007760723528917879), 'value_loss': np.float64(0.07721824524924159), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023156116454629228)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 197, timesteps: 403456\n",
            "Mean reward: -3.61, mean episode length: 4.60\n",
            "Losses: {'policy_loss': np.float64(0.010399332211818546), 'value_loss': np.float64(0.06847282452508807), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023229400772834197)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 198, timesteps: 405504\n",
            "Mean reward: -3.64, mean episode length: 4.63\n",
            "Losses: {'policy_loss': np.float64(0.0061962176114320755), 'value_loss': np.float64(0.05725136393448338), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02060506265843287)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 199, timesteps: 407552\n",
            "Mean reward: -3.77, mean episode length: 4.76\n",
            "Losses: {'policy_loss': np.float64(0.02968488825717941), 'value_loss': np.float64(0.0699930049595423), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.033922379079740494)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 200, timesteps: 409600\n",
            "Mean reward: -3.54, mean episode length: 4.54\n",
            "Losses: {'policy_loss': np.float64(-0.003629387792898342), 'value_loss': np.float64(0.06568419514223933), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01924812569632195)}\n",
            "Evaluation: Mean reward: -2.60, mean episode length: 3.60\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 201, timesteps: 411648\n",
            "Mean reward: -3.61, mean episode length: 4.60\n",
            "Losses: {'policy_loss': np.float64(0.004201204515993595), 'value_loss': np.float64(0.044632786128204316), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01807346017449163)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 202, timesteps: 413696\n",
            "Mean reward: -3.52, mean episode length: 4.52\n",
            "Losses: {'policy_loss': np.float64(0.006710605579428375), 'value_loss': np.float64(0.022090696162194945), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01916984345007222)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 203, timesteps: 415744\n",
            "Mean reward: -3.34, mean episode length: 4.34\n",
            "Losses: {'policy_loss': np.float64(0.014105682639637962), 'value_loss': np.float64(0.01697279841755517), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024851723719621077)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 204, timesteps: 417792\n",
            "Mean reward: -3.29, mean episode length: 4.29\n",
            "Losses: {'policy_loss': np.float64(0.011655660113319755), 'value_loss': np.float64(0.018079546804074198), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020931642648065463)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 205, timesteps: 419840\n",
            "Mean reward: -3.22, mean episode length: 4.22\n",
            "Losses: {'policy_loss': np.float64(0.007481135777197778), 'value_loss': np.float64(0.0193307084555272), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016847719612997025)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 206, timesteps: 421888\n",
            "Mean reward: -3.39, mean episode length: 4.39\n",
            "Losses: {'policy_loss': np.float64(-0.0010993515024892986), 'value_loss': np.float64(0.020277571355109103), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01726790158136282)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 207, timesteps: 423936\n",
            "Mean reward: -3.63, mean episode length: 4.63\n",
            "Losses: {'policy_loss': np.float64(0.004864307818934321), 'value_loss': np.float64(0.03175031559658237), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02103209192864597)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 208, timesteps: 425984\n",
            "Mean reward: -3.34, mean episode length: 4.34\n",
            "Losses: {'policy_loss': np.float64(0.003860049109789543), 'value_loss': np.float64(0.01812251438968815), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018092780432198197)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 209, timesteps: 428032\n",
            "Mean reward: -3.34, mean episode length: 4.34\n",
            "Losses: {'policy_loss': np.float64(-0.004378875019028783), 'value_loss': np.float64(0.0185062692714079), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016968916723271832)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 210, timesteps: 430080\n",
            "Mean reward: -3.32, mean episode length: 4.32\n",
            "Losses: {'policy_loss': np.float64(0.0141261403914541), 'value_loss': np.float64(0.015761782851768658), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019238846231019124)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 211, timesteps: 432128\n",
            "Mean reward: -3.32, mean episode length: 4.32\n",
            "Losses: {'policy_loss': np.float64(-0.0009626299724914134), 'value_loss': np.float64(0.021378288336563855), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017617623001569882)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 212, timesteps: 434176\n",
            "Mean reward: -3.39, mean episode length: 4.39\n",
            "Losses: {'policy_loss': np.float64(0.013367765583097935), 'value_loss': np.float64(0.012533174711279571), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022483457054477185)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 213, timesteps: 436224\n",
            "Mean reward: -3.31, mean episode length: 4.31\n",
            "Losses: {'policy_loss': np.float64(0.014774734212551266), 'value_loss': np.float64(0.014458649384323508), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01638685812940821)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 214, timesteps: 438272\n",
            "Mean reward: -3.53, mean episode length: 4.53\n",
            "Losses: {'policy_loss': np.float64(0.005693133745808154), 'value_loss': np.float64(0.01910392232821323), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015182343253400177)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 215, timesteps: 440320\n",
            "Mean reward: -3.39, mean episode length: 4.39\n",
            "Losses: {'policy_loss': np.float64(0.017868142283987254), 'value_loss': np.float64(0.026350618951255456), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02411951936664991)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 216, timesteps: 442368\n",
            "Mean reward: -3.42, mean episode length: 4.42\n",
            "Losses: {'policy_loss': np.float64(0.012102494831196964), 'value_loss': np.float64(0.021157919894903898), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01636696609784849)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_442368.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 217, timesteps: 444416\n",
            "Mean reward: -3.20, mean episode length: 4.20\n",
            "Losses: {'policy_loss': np.float64(0.01199867221293971), 'value_loss': np.float64(0.015247100789565593), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01812363535282202)}\n",
            "Early stopping at epoch 3/10 due to reaching KL target\n",
            "Iteration: 218, timesteps: 446464\n",
            "Mean reward: -3.36, mean episode length: 4.36\n",
            "Losses: {'policy_loss': np.float64(0.0016467596675890188), 'value_loss': np.float64(0.015666296657097217), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01690099959766182)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 219, timesteps: 448512\n",
            "Mean reward: -3.52, mean episode length: 4.52\n",
            "Losses: {'policy_loss': np.float64(0.011859319405630231), 'value_loss': np.float64(0.018420284264720976), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023439123790012673)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 220, timesteps: 450560\n",
            "Mean reward: -3.42, mean episode length: 4.42\n",
            "Losses: {'policy_loss': np.float64(0.0072479016380384564), 'value_loss': np.float64(0.01545065114623867), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025703916588099673)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 221, timesteps: 452608\n",
            "Mean reward: -3.34, mean episode length: 4.34\n",
            "Losses: {'policy_loss': np.float64(0.003431009128689766), 'value_loss': np.float64(0.015752503037219867), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02042555101797916)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 222, timesteps: 454656\n",
            "Mean reward: -3.14, mean episode length: 4.14\n",
            "Losses: {'policy_loss': np.float64(0.0074819629662670195), 'value_loss': np.float64(0.01323822492850013), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017920562997460365)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 223, timesteps: 456704\n",
            "Mean reward: -3.28, mean episode length: 4.28\n",
            "Losses: {'policy_loss': np.float64(0.0027609641547314823), 'value_loss': np.float64(0.012649561394937336), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017773922634660266)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 224, timesteps: 458752\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.011640680953860283), 'value_loss': np.float64(0.015497392072575167), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017090293957153335)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 225, timesteps: 460800\n",
            "Mean reward: -3.31, mean episode length: 4.31\n",
            "Losses: {'policy_loss': np.float64(0.010663218272384256), 'value_loss': np.float64(0.014389717252925038), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01724772815941833)}\n",
            "Evaluation: Mean reward: -2.20, mean episode length: 3.20\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 226, timesteps: 462848\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.005544970466871746), 'value_loss': np.float64(0.014877549561788328), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01695115696929861)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 227, timesteps: 464896\n",
            "Mean reward: -3.27, mean episode length: 4.27\n",
            "Losses: {'policy_loss': np.float64(0.010717925382778049), 'value_loss': np.float64(0.01524435932515189), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015717900736490265)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 228, timesteps: 466944\n",
            "Mean reward: -3.05, mean episode length: 4.05\n",
            "Losses: {'policy_loss': np.float64(0.012094436562620103), 'value_loss': np.float64(0.014692907716380432), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017329330526990816)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 229, timesteps: 468992\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.012859809678047895), 'value_loss': np.float64(0.015186556993285194), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.029493517649825662)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 230, timesteps: 471040\n",
            "Mean reward: -3.19, mean episode length: 4.19\n",
            "Losses: {'policy_loss': np.float64(0.011240902415011078), 'value_loss': np.float64(0.014641568152001128), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017654012801358476)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 231, timesteps: 473088\n",
            "Mean reward: -3.10, mean episode length: 4.10\n",
            "Losses: {'policy_loss': np.float64(0.011717278452124447), 'value_loss': np.float64(0.015933079121168703), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019457118294667453)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 232, timesteps: 475136\n",
            "Mean reward: -3.18, mean episode length: 4.18\n",
            "Losses: {'policy_loss': np.float64(0.008431301423115656), 'value_loss': np.float64(0.014918615372152999), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02004712328198366)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 233, timesteps: 477184\n",
            "Mean reward: -3.33, mean episode length: 4.33\n",
            "Losses: {'policy_loss': np.float64(0.005540136247873306), 'value_loss': np.float64(0.01846229101647623), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01796450454276055)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 234, timesteps: 479232\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.013804841903038323), 'value_loss': np.float64(0.014975498139392585), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01935475380741991)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 235, timesteps: 481280\n",
            "Mean reward: -3.39, mean episode length: 4.39\n",
            "Losses: {'policy_loss': np.float64(0.012699778017122298), 'value_loss': np.float64(0.01962129943422042), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017192253813846037)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 236, timesteps: 483328\n",
            "Mean reward: -3.13, mean episode length: 4.13\n",
            "Losses: {'policy_loss': np.float64(0.004676724478485994), 'value_loss': np.float64(0.015447966346982867), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018034215550869703)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 237, timesteps: 485376\n",
            "Mean reward: -3.18, mean episode length: 4.18\n",
            "Losses: {'policy_loss': np.float64(0.012400889419950545), 'value_loss': np.float64(0.018591591506265104), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02256500258226879)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 238, timesteps: 487424\n",
            "Mean reward: -3.11, mean episode length: 4.11\n",
            "Losses: {'policy_loss': np.float64(0.019315920013468713), 'value_loss': np.float64(0.014435836143093184), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023175002803327516)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 239, timesteps: 489472\n",
            "Mean reward: -3.25, mean episode length: 4.25\n",
            "Losses: {'policy_loss': np.float64(0.016036312270443887), 'value_loss': np.float64(0.014868696744088084), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022216545592527837)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 240, timesteps: 491520\n",
            "Mean reward: -3.27, mean episode length: 4.27\n",
            "Losses: {'policy_loss': np.float64(0.022652257641311735), 'value_loss': np.float64(0.014604619384044781), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03610160751850344)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_491520.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 241, timesteps: 493568\n",
            "Mean reward: -3.70, mean episode length: 4.70\n",
            "Losses: {'policy_loss': np.float64(0.007746051240246743), 'value_loss': np.float64(0.030765703064389527), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022877209325088188)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 242, timesteps: 495616\n",
            "Mean reward: -3.51, mean episode length: 4.51\n",
            "Losses: {'policy_loss': np.float64(0.009279966470785439), 'value_loss': np.float64(0.027355239872122183), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03008876541571226)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 243, timesteps: 497664\n",
            "Mean reward: -3.43, mean episode length: 4.43\n",
            "Losses: {'policy_loss': np.float64(0.004932282434310764), 'value_loss': np.float64(0.02529825980309397), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020957443019142374)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 244, timesteps: 499712\n",
            "Mean reward: -3.28, mean episode length: 4.28\n",
            "Losses: {'policy_loss': np.float64(0.005615097179543227), 'value_loss': np.float64(0.01977296991390176), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021269560151267797)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 245, timesteps: 501760\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.01102972828084603), 'value_loss': np.float64(0.01491192207322456), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020191598814562894)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 246, timesteps: 503808\n",
            "Mean reward: -3.12, mean episode length: 4.12\n",
            "Losses: {'policy_loss': np.float64(0.007482193876057863), 'value_loss': np.float64(0.01999491994502023), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017061538965208456)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 247, timesteps: 505856\n",
            "Mean reward: -3.22, mean episode length: 4.22\n",
            "Losses: {'policy_loss': np.float64(0.003433881443925202), 'value_loss': np.float64(0.015602326864609495), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01924926324863918)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 248, timesteps: 507904\n",
            "Mean reward: -3.27, mean episode length: 4.27\n",
            "Losses: {'policy_loss': np.float64(0.004915043944492936), 'value_loss': np.float64(0.02289973958977498), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02357669909542892)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 249, timesteps: 509952\n",
            "Mean reward: -3.24, mean episode length: 4.24\n",
            "Losses: {'policy_loss': np.float64(0.012386430491460487), 'value_loss': np.float64(0.020856851362623274), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020246571963070892)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 250, timesteps: 512000\n",
            "Mean reward: -3.13, mean episode length: 4.13\n",
            "Losses: {'policy_loss': np.float64(0.01002349442569539), 'value_loss': np.float64(0.01728830617503263), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01892542182758916)}\n",
            "Evaluation: Mean reward: -2.30, mean episode length: 3.30\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 251, timesteps: 514048\n",
            "Mean reward: -3.12, mean episode length: 4.12\n",
            "Losses: {'policy_loss': np.float64(0.019012850447325036), 'value_loss': np.float64(0.015926524647511542), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.034714210050879046)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 252, timesteps: 516096\n",
            "Mean reward: -3.44, mean episode length: 4.44\n",
            "Losses: {'policy_loss': np.float64(0.0011818933708127588), 'value_loss': np.float64(0.019058298668824136), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016876082285307348)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 253, timesteps: 518144\n",
            "Mean reward: -3.38, mean episode length: 4.38\n",
            "Losses: {'policy_loss': np.float64(0.004568944044876844), 'value_loss': np.float64(0.027124300657305866), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022463793953647837)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 254, timesteps: 520192\n",
            "Mean reward: -3.55, mean episode length: 4.55\n",
            "Losses: {'policy_loss': np.float64(0.004945675027556717), 'value_loss': np.float64(0.024609471642179415), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01662514738563914)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 255, timesteps: 522240\n",
            "Mean reward: -3.35, mean episode length: 4.35\n",
            "Losses: {'policy_loss': np.float64(0.01796478749020025), 'value_loss': np.float64(0.023176735470769927), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023936354598845355)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 256, timesteps: 524288\n",
            "Mean reward: -3.31, mean episode length: 4.31\n",
            "Losses: {'policy_loss': np.float64(0.01569551002467051), 'value_loss': np.float64(0.01755432487698272), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019575284386519343)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 257, timesteps: 526336\n",
            "Mean reward: -3.11, mean episode length: 4.11\n",
            "Losses: {'policy_loss': np.float64(0.006391136004822329), 'value_loss': np.float64(0.013212552919867449), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018363005030550994)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 258, timesteps: 528384\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.011383869918063283), 'value_loss': np.float64(0.016473489115014672), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019501251546898857)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 259, timesteps: 530432\n",
            "Mean reward: -3.15, mean episode length: 4.15\n",
            "Losses: {'policy_loss': np.float64(0.0072495408821851015), 'value_loss': np.float64(0.017802691436372697), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02151405804033857)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 260, timesteps: 532480\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.013392520952038467), 'value_loss': np.float64(0.013502624438842759), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02196399777312763)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 261, timesteps: 534528\n",
            "Mean reward: -3.04, mean episode length: 4.04\n",
            "Losses: {'policy_loss': np.float64(0.00986640976043418), 'value_loss': np.float64(0.01581909117521718), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021865025089937262)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 262, timesteps: 536576\n",
            "Mean reward: -3.21, mean episode length: 4.21\n",
            "Losses: {'policy_loss': np.float64(0.011735596635844558), 'value_loss': np.float64(0.020239666511770338), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022574342146981508)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 263, timesteps: 538624\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.013455897627864033), 'value_loss': np.float64(0.015514356462517753), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015223923401208594)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 264, timesteps: 540672\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.007233995362184942), 'value_loss': np.float64(0.010706387183745392), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021485100616700947)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_540672.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 265, timesteps: 542720\n",
            "Mean reward: -3.05, mean episode length: 4.05\n",
            "Losses: {'policy_loss': np.float64(0.013171298429369926), 'value_loss': np.float64(0.013227087678387761), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026673432934330776)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 266, timesteps: 544768\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.013132475898601115), 'value_loss': np.float64(0.01637504101381637), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024312539942911826)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 267, timesteps: 546816\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.01244050549576059), 'value_loss': np.float64(0.015801990288309753), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023689705121796578)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 268, timesteps: 548864\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.011251700227148831), 'value_loss': np.float64(0.017565551039297134), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021796168715809472)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 269, timesteps: 550912\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.013498378684744239), 'value_loss': np.float64(0.013688061328139156), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02157202796661295)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 270, timesteps: 552960\n",
            "Mean reward: -2.81, mean episode length: 3.81\n",
            "Losses: {'policy_loss': np.float64(0.005911689717322588), 'value_loss': np.float64(0.012156606229837053), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01630217576166615)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 271, timesteps: 555008\n",
            "Mean reward: -2.95, mean episode length: 3.95\n",
            "Losses: {'policy_loss': np.float64(0.013928801054134965), 'value_loss': np.float64(0.012126639630878344), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01574717376206536)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 272, timesteps: 557056\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.008461388060823083), 'value_loss': np.float64(0.021644957829266787), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017084347578929737)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 273, timesteps: 559104\n",
            "Mean reward: -2.97, mean episode length: 3.97\n",
            "Losses: {'policy_loss': np.float64(0.013731067185290158), 'value_loss': np.float64(0.01599106183857657), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015838000021176413)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 274, timesteps: 561152\n",
            "Mean reward: -2.89, mean episode length: 3.89\n",
            "Losses: {'policy_loss': np.float64(0.01570026809349656), 'value_loss': np.float64(0.012305372423725203), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017829139076638967)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 275, timesteps: 563200\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.005858768883626908), 'value_loss': np.float64(0.015569000446703285), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015354795221355744)}\n",
            "Evaluation: Mean reward: -2.10, mean episode length: 3.10\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 276, timesteps: 565248\n",
            "Mean reward: -3.09, mean episode length: 4.09\n",
            "Losses: {'policy_loss': np.float64(0.015442934061866254), 'value_loss': np.float64(0.01397535199066624), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023964584004716016)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 277, timesteps: 567296\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.008170906046871096), 'value_loss': np.float64(0.016444985929410905), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017084134466131218)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 278, timesteps: 569344\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.006684533611405641), 'value_loss': np.float64(0.012467543157981709), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021979051889502443)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 279, timesteps: 571392\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.029809602885507047), 'value_loss': np.float64(0.010846930468687788), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03382448898628354)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 280, timesteps: 573440\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.011677390430122614), 'value_loss': np.float64(0.014363111084094271), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022877175229950808)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 281, timesteps: 575488\n",
            "Mean reward: -2.80, mean episode length: 3.80\n",
            "Losses: {'policy_loss': np.float64(0.020736903068609536), 'value_loss': np.float64(0.013058542856015265), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03080239810515195)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 282, timesteps: 577536\n",
            "Mean reward: -2.77, mean episode length: 3.77\n",
            "Losses: {'policy_loss': np.float64(0.012791707995347679), 'value_loss': np.float64(0.012382574786897749), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02559096328332089)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 283, timesteps: 579584\n",
            "Mean reward: -2.93, mean episode length: 3.93\n",
            "Losses: {'policy_loss': np.float64(0.006228079611901194), 'value_loss': np.float64(0.014366475865244865), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01876567193539813)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 284, timesteps: 581632\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.016143324552103877), 'value_loss': np.float64(0.015108071907889098), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0283496402844321)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 285, timesteps: 583680\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.007306576881092042), 'value_loss': np.float64(0.012657782674068585), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02552551920962287)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 286, timesteps: 585728\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.014500244404189289), 'value_loss': np.float64(0.017738258000463247), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027537365866010077)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 287, timesteps: 587776\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.011593401781283319), 'value_loss': np.float64(0.014739388949237764), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.029008086858084425)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 288, timesteps: 589824\n",
            "Mean reward: -2.94, mean episode length: 3.94\n",
            "Losses: {'policy_loss': np.float64(0.015816576778888702), 'value_loss': np.float64(0.014155928511172533), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020520267557003535)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_589824.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 289, timesteps: 591872\n",
            "Mean reward: -2.91, mean episode length: 3.91\n",
            "Losses: {'policy_loss': np.float64(0.010559259972069412), 'value_loss': np.float64(0.012546196725452319), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027722431332222186)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 290, timesteps: 593920\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.005480145278852433), 'value_loss': np.float64(0.020475358600378968), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02078199287643656)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 291, timesteps: 595968\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.012103005457902327), 'value_loss': np.float64(0.013926603423897177), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01836974963953253)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 292, timesteps: 598016\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.014827804698143154), 'value_loss': np.float64(0.014550380525179207), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02720428805332631)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 293, timesteps: 600064\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.007443214737577364), 'value_loss': np.float64(0.016063332222984172), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018609934340929613)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 294, timesteps: 602112\n",
            "Mean reward: -3.03, mean episode length: 4.03\n",
            "Losses: {'policy_loss': np.float64(0.007244114531204104), 'value_loss': np.float64(0.01574228616664186), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01856671653513331)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 295, timesteps: 604160\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.015295506047550589), 'value_loss': np.float64(0.012840479525038972), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03310218482511118)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 296, timesteps: 606208\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.006170367531012744), 'value_loss': np.float64(0.012466346270230133), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019860256514220964)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 297, timesteps: 608256\n",
            "Mean reward: -2.66, mean episode length: 3.66\n",
            "Losses: {'policy_loss': np.float64(0.012775521259754896), 'value_loss': np.float64(0.010674853285308927), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01721794216427952)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 298, timesteps: 610304\n",
            "Mean reward: -2.77, mean episode length: 3.77\n",
            "Losses: {'policy_loss': np.float64(0.009218666149536148), 'value_loss': np.float64(0.016384796414058656), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02635975780140143)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 299, timesteps: 612352\n",
            "Mean reward: -2.80, mean episode length: 3.80\n",
            "Losses: {'policy_loss': np.float64(0.009793493372853845), 'value_loss': np.float64(0.012483669765060768), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02208149596117437)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 300, timesteps: 614400\n",
            "Mean reward: -2.81, mean episode length: 3.81\n",
            "Losses: {'policy_loss': np.float64(0.01739747414831072), 'value_loss': np.float64(0.01301326634711586), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018363898692769)}\n",
            "Evaluation: Mean reward: -1.70, mean episode length: 2.70\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 301, timesteps: 616448\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.010775378264952451), 'value_loss': np.float64(0.012649741256609559), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01932229423255194)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 302, timesteps: 618496\n",
            "Mean reward: -2.93, mean episode length: 3.93\n",
            "Losses: {'policy_loss': np.float64(0.0067631982965394855), 'value_loss': np.float64(0.013595105614513159), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020331063569756225)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 303, timesteps: 620544\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.019783474854193628), 'value_loss': np.float64(0.01615815205150284), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022556647003511898)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 304, timesteps: 622592\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.020856227376498282), 'value_loss': np.float64(0.014375550177646801), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021393011018517427)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 305, timesteps: 624640\n",
            "Mean reward: -2.78, mean episode length: 3.78\n",
            "Losses: {'policy_loss': np.float64(0.01583108922932297), 'value_loss': np.float64(0.0124644297757186), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024088446618407033)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 306, timesteps: 626688\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.020175687153823674), 'value_loss': np.float64(0.016031487990403548), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021769813785795122)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 307, timesteps: 628736\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.013729948928812519), 'value_loss': np.float64(0.014801410085055977), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0161162357107969)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 308, timesteps: 630784\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.010739669058239087), 'value_loss': np.float64(0.01869804662419483), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02070964673475828)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 309, timesteps: 632832\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.021189535269513726), 'value_loss': np.float64(0.016049821686465293), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0358224643568974)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 310, timesteps: 634880\n",
            "Mean reward: -2.70, mean episode length: 3.70\n",
            "Losses: {'policy_loss': np.float64(0.015150041203014553), 'value_loss': np.float64(0.014091697725234553), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02277893727296032)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 311, timesteps: 636928\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.011439100722782314), 'value_loss': np.float64(0.013115665497025475), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01784150075400248)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 312, timesteps: 638976\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.012943375768372789), 'value_loss': np.float64(0.01723297381249722), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024384834512602538)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_638976.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 313, timesteps: 641024\n",
            "Mean reward: -3.15, mean episode length: 4.15\n",
            "Losses: {'policy_loss': np.float64(0.015072789916303009), 'value_loss': np.float64(0.01970667947898619), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.029875290085328743)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 314, timesteps: 643072\n",
            "Mean reward: -2.83, mean episode length: 3.83\n",
            "Losses: {'policy_loss': np.float64(0.013021528429817408), 'value_loss': np.float64(0.017552635836182162), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019952317205024883)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 315, timesteps: 645120\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.016609719314146787), 'value_loss': np.float64(0.01512533298227936), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.031840703304624185)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 316, timesteps: 647168\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.012107525923056528), 'value_loss': np.float64(0.019458254799246788), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024337855327758007)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 317, timesteps: 649216\n",
            "Mean reward: -3.28, mean episode length: 4.28\n",
            "Losses: {'policy_loss': np.float64(0.004980536177754402), 'value_loss': np.float64(0.024073554668575525), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019608781542046927)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 318, timesteps: 651264\n",
            "Mean reward: -3.24, mean episode length: 4.24\n",
            "Losses: {'policy_loss': np.float64(0.01435045077232644), 'value_loss': np.float64(0.021436460898257792), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02982366668584291)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 319, timesteps: 653312\n",
            "Mean reward: -3.09, mean episode length: 4.09\n",
            "Losses: {'policy_loss': np.float64(0.009599440963938832), 'value_loss': np.float64(0.016396146558690816), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018118653606506996)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 320, timesteps: 655360\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.011144591961055994), 'value_loss': np.float64(0.01586502679856494), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023091056500561535)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 321, timesteps: 657408\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.013972169952467084), 'value_loss': np.float64(0.013542644708650187), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020154419995378703)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 322, timesteps: 659456\n",
            "Mean reward: -3.10, mean episode length: 4.10\n",
            "Losses: {'policy_loss': np.float64(0.018403163237962872), 'value_loss': np.float64(0.019669872388476506), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.028267390094697475)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 323, timesteps: 661504\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.012525079306215048), 'value_loss': np.float64(0.016644604154862463), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02416034971247427)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 324, timesteps: 663552\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.013843947555869818), 'value_loss': np.float64(0.01659687122446485), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02417251281440258)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 325, timesteps: 665600\n",
            "Mean reward: -2.93, mean episode length: 3.93\n",
            "Losses: {'policy_loss': np.float64(0.01391886657802388), 'value_loss': np.float64(0.013341404846869409), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0235859663807787)}\n",
            "Evaluation: Mean reward: -2.10, mean episode length: 3.10\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 326, timesteps: 667648\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.007883215846959502), 'value_loss': np.float64(0.012240514901350252), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023823081166483462)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 327, timesteps: 669696\n",
            "Mean reward: -3.04, mean episode length: 4.04\n",
            "Losses: {'policy_loss': np.float64(0.021175058383960277), 'value_loss': np.float64(0.015908113942714408), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02774223271990195)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 328, timesteps: 671744\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.004040569823700935), 'value_loss': np.float64(0.015011774172307923), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0218624139088206)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 329, timesteps: 673792\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.016019870294258), 'value_loss': np.float64(0.014697406149934977), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.028172212332719937)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 330, timesteps: 675840\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.019791791099123657), 'value_loss': np.float64(0.013809143798425794), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024821383354719728)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 331, timesteps: 677888\n",
            "Mean reward: -3.05, mean episode length: 4.05\n",
            "Losses: {'policy_loss': np.float64(0.008062725828494877), 'value_loss': np.float64(0.014156228280626237), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019327439920743927)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 332, timesteps: 679936\n",
            "Mean reward: -3.47, mean episode length: 4.47\n",
            "Losses: {'policy_loss': np.float64(0.015019235026556998), 'value_loss': np.float64(0.029180990939494222), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026796267367899418)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 333, timesteps: 681984\n",
            "Mean reward: -3.36, mean episode length: 4.36\n",
            "Losses: {'policy_loss': np.float64(0.012576609675306827), 'value_loss': np.float64(0.028158649714896455), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03215765376808122)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 334, timesteps: 684032\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.010460964811500162), 'value_loss': np.float64(0.025306802854174748), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024722452246351168)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 335, timesteps: 686080\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.011464351991889998), 'value_loss': np.float64(0.018862016702769324), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017660228681052104)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 336, timesteps: 688128\n",
            "Mean reward: -3.21, mean episode length: 4.21\n",
            "Losses: {'policy_loss': np.float64(0.013609430636279285), 'value_loss': np.float64(0.020123957277974114), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015749831189168617)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_688128.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 337, timesteps: 690176\n",
            "Mean reward: -3.32, mean episode length: 4.32\n",
            "Losses: {'policy_loss': np.float64(0.019428349507506937), 'value_loss': np.float64(0.02080022983136587), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027484937105327845)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 338, timesteps: 692224\n",
            "Mean reward: -3.29, mean episode length: 4.29\n",
            "Losses: {'policy_loss': np.float64(0.012448782857973129), 'value_loss': np.float64(0.02225892839487642), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023759499861625955)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 339, timesteps: 694272\n",
            "Mean reward: -3.40, mean episode length: 4.40\n",
            "Losses: {'policy_loss': np.float64(0.012346207862719893), 'value_loss': np.float64(0.022161171276820824), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023151296103606)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 340, timesteps: 696320\n",
            "Mean reward: -3.30, mean episode length: 4.30\n",
            "Losses: {'policy_loss': np.float64(0.011156788386870176), 'value_loss': np.float64(0.01945625527878292), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019932528433855623)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 341, timesteps: 698368\n",
            "Mean reward: -3.29, mean episode length: 4.29\n",
            "Losses: {'policy_loss': np.float64(0.011019420577213168), 'value_loss': np.float64(0.02054893010063097), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019033092161407694)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 342, timesteps: 700416\n",
            "Mean reward: -3.37, mean episode length: 4.37\n",
            "Losses: {'policy_loss': np.float64(0.010347339732106775), 'value_loss': np.float64(0.022638430120423436), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02085100597469136)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 343, timesteps: 702464\n",
            "Mean reward: -3.21, mean episode length: 4.21\n",
            "Losses: {'policy_loss': np.float64(0.010008616431150585), 'value_loss': np.float64(0.02148220405797474), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01917341721127741)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 344, timesteps: 704512\n",
            "Mean reward: -3.31, mean episode length: 4.31\n",
            "Losses: {'policy_loss': np.float64(0.009605413593817502), 'value_loss': np.float64(0.02352617579163052), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023126427666284144)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 345, timesteps: 706560\n",
            "Mean reward: -3.48, mean episode length: 4.48\n",
            "Losses: {'policy_loss': np.float64(0.007316837116377428), 'value_loss': np.float64(0.027761672856286168), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.015193648432614282)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 346, timesteps: 708608\n",
            "Mean reward: -3.58, mean episode length: 4.58\n",
            "Losses: {'policy_loss': np.float64(0.01971904409583658), 'value_loss': np.float64(0.027762042183894664), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02036559590487741)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 347, timesteps: 710656\n",
            "Mean reward: -3.26, mean episode length: 4.26\n",
            "Losses: {'policy_loss': np.float64(0.01565878576366231), 'value_loss': np.float64(0.02533969935029745), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02164604444988072)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 348, timesteps: 712704\n",
            "Mean reward: -3.29, mean episode length: 4.29\n",
            "Losses: {'policy_loss': np.float64(0.013427968340693042), 'value_loss': np.float64(0.029347167059313506), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03239071398274973)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 349, timesteps: 714752\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.012179227778688073), 'value_loss': np.float64(0.021955439617158845), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01566172536695376)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 350, timesteps: 716800\n",
            "Mean reward: -3.04, mean episode length: 4.04\n",
            "Losses: {'policy_loss': np.float64(0.01372473780065775), 'value_loss': np.float64(0.01604825974209234), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026210811600321904)}\n",
            "Evaluation: Mean reward: -2.90, mean episode length: 3.90\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 351, timesteps: 718848\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.017291249590925872), 'value_loss': np.float64(0.018468900641892105), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021827802644111216)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 352, timesteps: 720896\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.01133955956902355), 'value_loss': np.float64(0.014669673080788925), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02428043627878651)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 353, timesteps: 722944\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.01150018721818924), 'value_loss': np.float64(0.013208369564381428), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025258195062633604)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 354, timesteps: 724992\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.012617783155292273), 'value_loss': np.float64(0.015087881532963365), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0227316569944378)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 355, timesteps: 727040\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.01586806820705533), 'value_loss': np.float64(0.01857940194895491), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020149176591075957)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 356, timesteps: 729088\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.01403424353338778), 'value_loss': np.float64(0.01544198309420608), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025365545880049467)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 357, timesteps: 731136\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.014396316750207916), 'value_loss': np.float64(0.018721378000918776), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021068132104119286)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 358, timesteps: 733184\n",
            "Mean reward: -3.12, mean episode length: 4.12\n",
            "Losses: {'policy_loss': np.float64(0.013618817902170122), 'value_loss': np.float64(0.020400447683641687), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016040603164583445)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 359, timesteps: 735232\n",
            "Mean reward: -3.19, mean episode length: 4.19\n",
            "Losses: {'policy_loss': np.float64(0.00960246124304831), 'value_loss': np.float64(0.020660341018810868), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02767242575646378)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 360, timesteps: 737280\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.011286652996204793), 'value_loss': np.float64(0.018543219775892794), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017459028138546273)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_737280.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 361, timesteps: 739328\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.016711158881662413), 'value_loss': np.float64(0.015871775802224874), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01932134330854751)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 362, timesteps: 741376\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.011804392852354795), 'value_loss': np.float64(0.013674526126123965), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020643586467485875)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 363, timesteps: 743424\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.013647272193338722), 'value_loss': np.float64(0.0146890880423598), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027396610472351313)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 364, timesteps: 745472\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.012835186207666993), 'value_loss': np.float64(0.021954715804895386), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025664285727543756)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 365, timesteps: 747520\n",
            "Mean reward: -3.18, mean episode length: 4.18\n",
            "Losses: {'policy_loss': np.float64(0.014237663766834885), 'value_loss': np.float64(0.02381164362304844), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020588737883372232)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 366, timesteps: 749568\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.016169547860044986), 'value_loss': np.float64(0.015045994266984053), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02392071095528081)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 367, timesteps: 751616\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.01566563913365826), 'value_loss': np.float64(0.014963080990128219), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025100659258896485)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 368, timesteps: 753664\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.01669312547892332), 'value_loss': np.float64(0.01654543465701863), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02692126843612641)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 369, timesteps: 755712\n",
            "Mean reward: -3.17, mean episode length: 4.17\n",
            "Losses: {'policy_loss': np.float64(0.017729979124851525), 'value_loss': np.float64(0.022697327309288085), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024915671441704035)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 370, timesteps: 757760\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.018126080802176148), 'value_loss': np.float64(0.016155768651515245), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01883609322248958)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 371, timesteps: 759808\n",
            "Mean reward: -3.29, mean episode length: 4.29\n",
            "Losses: {'policy_loss': np.float64(0.011798171559348702), 'value_loss': np.float64(0.02319722404354252), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019650853413622826)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 372, timesteps: 761856\n",
            "Mean reward: -3.20, mean episode length: 4.20\n",
            "Losses: {'policy_loss': np.float64(0.014036796055734158), 'value_loss': np.float64(0.029782520548906177), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026783844485180452)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 373, timesteps: 763904\n",
            "Mean reward: -3.35, mean episode length: 4.35\n",
            "Losses: {'policy_loss': np.float64(0.008252289961092174), 'value_loss': np.float64(0.030824162939097732), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019064401829382405)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 374, timesteps: 765952\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.010449856286868453), 'value_loss': np.float64(0.025536307628499344), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019682430574903265)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 375, timesteps: 768000\n",
            "Mean reward: -3.09, mean episode length: 4.09\n",
            "Losses: {'policy_loss': np.float64(0.016937310225330293), 'value_loss': np.float64(0.026629028550814837), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02908331758226268)}\n",
            "Evaluation: Mean reward: -1.70, mean episode length: 2.70\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 376, timesteps: 770048\n",
            "Mean reward: -3.10, mean episode length: 4.10\n",
            "Losses: {'policy_loss': np.float64(0.016924601251957938), 'value_loss': np.float64(0.0237434022128582), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02137279705493711)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 377, timesteps: 772096\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.01381955755641684), 'value_loss': np.float64(0.01634359746822156), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025946850975742564)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 378, timesteps: 774144\n",
            "Mean reward: -2.91, mean episode length: 3.91\n",
            "Losses: {'policy_loss': np.float64(0.01494482101406902), 'value_loss': np.float64(0.015465180826140568), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.029419513943139464)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 379, timesteps: 776192\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.0134047920582816), 'value_loss': np.float64(0.020215820521116257), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022135309409350157)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 380, timesteps: 778240\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.009238347935024649), 'value_loss': np.float64(0.020972877915482968), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02040992261026986)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 381, timesteps: 780288\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.011744506482500583), 'value_loss': np.float64(0.021078484161989763), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027952852891758084)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 382, timesteps: 782336\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.010251036030240357), 'value_loss': np.float64(0.017431424086680636), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02436574941384606)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 383, timesteps: 784384\n",
            "Mean reward: -2.83, mean episode length: 3.83\n",
            "Losses: {'policy_loss': np.float64(0.014827358070760965), 'value_loss': np.float64(0.016538766998564824), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022795273107476532)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 384, timesteps: 786432\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.016325539094395936), 'value_loss': np.float64(0.014770512352697551), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017978228657739237)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_786432.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 385, timesteps: 788480\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.019758484908379614), 'value_loss': np.float64(0.021448931394843385), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018069838610244915)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 386, timesteps: 790528\n",
            "Mean reward: -2.79, mean episode length: 3.79\n",
            "Losses: {'policy_loss': np.float64(0.005245083128102124), 'value_loss': np.float64(0.012740697915432975), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01682975093717687)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 387, timesteps: 792576\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.01622061204398051), 'value_loss': np.float64(0.016944600036367774), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024128156364895403)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 388, timesteps: 794624\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.019119359436444938), 'value_loss': np.float64(0.01814573208685033), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02977085320162587)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 389, timesteps: 796672\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.019504881754983217), 'value_loss': np.float64(0.024995519837830216), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023835183237679303)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 390, timesteps: 798720\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.01955079921754077), 'value_loss': np.float64(0.020565147628076375), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0281487061874941)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 391, timesteps: 800768\n",
            "Mean reward: -2.95, mean episode length: 3.95\n",
            "Losses: {'policy_loss': np.float64(0.02548228861996904), 'value_loss': np.float64(0.019381296879146248), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027302913629682735)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 392, timesteps: 802816\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.013133189058862627), 'value_loss': np.float64(0.01977163925766945), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02038613919285126)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 393, timesteps: 804864\n",
            "Mean reward: -3.03, mean episode length: 4.03\n",
            "Losses: {'policy_loss': np.float64(0.01617040717974305), 'value_loss': np.float64(0.017821032146457583), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027338915911968797)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 394, timesteps: 806912\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.012406319845467806), 'value_loss': np.float64(0.016195139207411557), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02120240993099287)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 395, timesteps: 808960\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.013269648654386401), 'value_loss': np.float64(0.015690933651058003), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019580023363232613)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 396, timesteps: 811008\n",
            "Mean reward: -2.76, mean episode length: 3.76\n",
            "Losses: {'policy_loss': np.float64(0.007572683854959905), 'value_loss': np.float64(0.014926405128790066), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020789409056305885)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 397, timesteps: 813056\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.018043148244032636), 'value_loss': np.float64(0.015218812099192291), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025750825589057058)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 398, timesteps: 815104\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.009223267901688814), 'value_loss': np.float64(0.013070650573354214), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024981307418784127)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 399, timesteps: 817152\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.012524725636467338), 'value_loss': np.float64(0.017648007516982034), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021082739025587216)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 400, timesteps: 819200\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.01155828416813165), 'value_loss': np.float64(0.0157435204309877), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02897607465274632)}\n",
            "Evaluation: Mean reward: -2.50, mean episode length: 3.50\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 401, timesteps: 821248\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.016821735771372914), 'value_loss': np.float64(0.015518686384893954), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022290833469014615)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 402, timesteps: 823296\n",
            "Mean reward: -3.11, mean episode length: 4.11\n",
            "Losses: {'policy_loss': np.float64(0.010485803475603461), 'value_loss': np.float64(0.018117759929737076), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020497550343861803)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 403, timesteps: 825344\n",
            "Mean reward: -2.93, mean episode length: 3.93\n",
            "Losses: {'policy_loss': np.float64(0.015083569858688861), 'value_loss': np.float64(0.01814781568828039), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02042945261928253)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 404, timesteps: 827392\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.015776099055074155), 'value_loss': np.float64(0.01823393817176111), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.035447297705104575)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 405, timesteps: 829440\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.016314104985212907), 'value_loss': np.float64(0.01919848140096292), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027006633463315666)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 406, timesteps: 831488\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.0109270797111094), 'value_loss': np.float64(0.026676495268475264), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02438114324468188)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 407, timesteps: 833536\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.009932173416018486), 'value_loss': np.float64(0.020108574302867055), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0167070122261066)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 408, timesteps: 835584\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.018127126386389136), 'value_loss': np.float64(0.016533195972442627), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02492169282049872)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_835584.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 409, timesteps: 837632\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.007963135198224336), 'value_loss': np.float64(0.017152749554952607), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027008742763428017)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 410, timesteps: 839680\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.011159046553075314), 'value_loss': np.float64(0.016489391709910706), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0200232322094962)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 411, timesteps: 841728\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.017443190386984497), 'value_loss': np.float64(0.016218476317590103), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025223499425919726)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 412, timesteps: 843776\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.009362797485664487), 'value_loss': np.float64(0.018135352031094953), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01977444818476215)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 413, timesteps: 845824\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.016545682563446462), 'value_loss': np.float64(0.017284756700973958), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02166925251367502)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 414, timesteps: 847872\n",
            "Mean reward: -2.94, mean episode length: 3.94\n",
            "Losses: {'policy_loss': np.float64(0.02666428912198171), 'value_loss': np.float64(0.01480173593154177), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.030369366839295253)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 415, timesteps: 849920\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.016267584898741916), 'value_loss': np.float64(0.016590075567364693), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022453359444625676)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 416, timesteps: 851968\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.014206169988028705), 'value_loss': np.float64(0.021836517786141485), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018858216208172962)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 417, timesteps: 854016\n",
            "Mean reward: -2.96, mean episode length: 3.96\n",
            "Losses: {'policy_loss': np.float64(0.018767572531942278), 'value_loss': np.float64(0.01753328982158564), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017119849420851097)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 418, timesteps: 856064\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.011771664081607014), 'value_loss': np.float64(0.02537619965733029), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019157446455210447)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 419, timesteps: 858112\n",
            "Mean reward: -2.80, mean episode length: 3.80\n",
            "Losses: {'policy_loss': np.float64(0.01838747999863699), 'value_loss': np.float64(0.014288372447481379), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02823112125042826)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 420, timesteps: 860160\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.02696728694718331), 'value_loss': np.float64(0.017728205304592848), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03911480543320067)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 421, timesteps: 862208\n",
            "Mean reward: -2.81, mean episode length: 3.81\n",
            "Losses: {'policy_loss': np.float64(0.012421507912222296), 'value_loss': np.float64(0.01796120824292302), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02207776447176002)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 422, timesteps: 864256\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.020227409957442433), 'value_loss': np.float64(0.020159949111985043), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03374430461553857)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 423, timesteps: 866304\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.01893543917685747), 'value_loss': np.float64(0.01765922803315334), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023680205835262313)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 424, timesteps: 868352\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.010691215982660651), 'value_loss': np.float64(0.018427204951876774), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027824431570479646)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 425, timesteps: 870400\n",
            "Mean reward: -3.14, mean episode length: 4.14\n",
            "Losses: {'policy_loss': np.float64(0.009560365055222064), 'value_loss': np.float64(0.02921869879355654), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025618257408495992)}\n",
            "Evaluation: Mean reward: -2.30, mean episode length: 3.30\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 426, timesteps: 872448\n",
            "Mean reward: -3.19, mean episode length: 4.19\n",
            "Losses: {'policy_loss': np.float64(0.019081743143033236), 'value_loss': np.float64(0.021012465527746826), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022223350184503943)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 427, timesteps: 874496\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.009856920049060136), 'value_loss': np.float64(0.024296807358041406), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019783970405114815)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 428, timesteps: 876544\n",
            "Mean reward: -3.03, mean episode length: 4.03\n",
            "Losses: {'policy_loss': np.float64(0.018878557544667274), 'value_loss': np.float64(0.021139097603736445), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02305113017791882)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 429, timesteps: 878592\n",
            "Mean reward: -3.03, mean episode length: 4.03\n",
            "Losses: {'policy_loss': np.float64(0.022634199034655467), 'value_loss': np.float64(0.01890493227983825), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.033296780806267634)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 430, timesteps: 880640\n",
            "Mean reward: -2.95, mean episode length: 3.95\n",
            "Losses: {'policy_loss': np.float64(0.01939594664145261), 'value_loss': np.float64(0.017524683818919584), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023780438816174865)}\n",
            "Early stopping at epoch 2/10 due to reaching KL target\n",
            "Iteration: 431, timesteps: 882688\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.007774636062094942), 'value_loss': np.float64(0.019915123062673956), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017726669568219222)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 432, timesteps: 884736\n",
            "Mean reward: -3.09, mean episode length: 4.09\n",
            "Losses: {'policy_loss': np.float64(0.01367775141261518), 'value_loss': np.float64(0.024055104819126427), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0220606513612438)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_884736.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 433, timesteps: 886784\n",
            "Mean reward: -3.11, mean episode length: 4.11\n",
            "Losses: {'policy_loss': np.float64(0.011167793825734407), 'value_loss': np.float64(0.02053041401086375), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.016357706277631223)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 434, timesteps: 888832\n",
            "Mean reward: -3.28, mean episode length: 4.28\n",
            "Losses: {'policy_loss': np.float64(0.022129866236355156), 'value_loss': np.float64(0.029648215975612402), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03135482725338079)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 435, timesteps: 890880\n",
            "Mean reward: -3.21, mean episode length: 4.21\n",
            "Losses: {'policy_loss': np.float64(0.02089429213083349), 'value_loss': np.float64(0.022118149150628597), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.033179867576109245)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 436, timesteps: 892928\n",
            "Mean reward: -3.23, mean episode length: 4.23\n",
            "Losses: {'policy_loss': np.float64(0.012227757717482746), 'value_loss': np.float64(0.023971349059138447), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022863333142595366)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 437, timesteps: 894976\n",
            "Mean reward: -3.11, mean episode length: 4.11\n",
            "Losses: {'policy_loss': np.float64(0.020854041416896507), 'value_loss': np.float64(0.036223462026100606), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.038154515146743506)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 438, timesteps: 897024\n",
            "Mean reward: -2.98, mean episode length: 3.98\n",
            "Losses: {'policy_loss': np.float64(0.0208279691869393), 'value_loss': np.float64(0.026071376079926267), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03503704152535647)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 439, timesteps: 899072\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.017966979241464287), 'value_loss': np.float64(0.024172630975954235), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021576603467110544)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 440, timesteps: 901120\n",
            "Mean reward: -2.91, mean episode length: 3.91\n",
            "Losses: {'policy_loss': np.float64(0.013645930215716362), 'value_loss': np.float64(0.02142928433022462), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0231700717122294)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 441, timesteps: 903168\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.013845641922671348), 'value_loss': np.float64(0.018418726744130254), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.030154133623000234)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 442, timesteps: 905216\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.013953538436908275), 'value_loss': np.float64(0.019377496239030734), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.028486643015639856)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 443, timesteps: 907264\n",
            "Mean reward: -3.18, mean episode length: 4.18\n",
            "Losses: {'policy_loss': np.float64(0.012610981822945178), 'value_loss': np.float64(0.021890511648962274), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023832157399738207)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 444, timesteps: 909312\n",
            "Mean reward: -3.02, mean episode length: 4.02\n",
            "Losses: {'policy_loss': np.float64(0.014005162753164768), 'value_loss': np.float64(0.021089767571538687), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027157624455867335)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 445, timesteps: 911360\n",
            "Mean reward: -3.04, mean episode length: 4.04\n",
            "Losses: {'policy_loss': np.float64(0.020194046199321747), 'value_loss': np.float64(0.022757899772841483), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02820810509729199)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 446, timesteps: 913408\n",
            "Mean reward: -3.22, mean episode length: 4.22\n",
            "Losses: {'policy_loss': np.float64(0.015004249929916114), 'value_loss': np.float64(0.02398271200945601), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03209153324132785)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 447, timesteps: 915456\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.023469827603548765), 'value_loss': np.float64(0.020732203352963552), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023580112669151276)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 448, timesteps: 917504\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.013118786038830876), 'value_loss': np.float64(0.016102728492114693), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03747826974722557)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 449, timesteps: 919552\n",
            "Mean reward: -2.90, mean episode length: 3.90\n",
            "Losses: {'policy_loss': np.float64(0.012392153730615973), 'value_loss': np.float64(0.020866537786787376), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018642384267877787)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 450, timesteps: 921600\n",
            "Mean reward: -2.82, mean episode length: 3.82\n",
            "Losses: {'policy_loss': np.float64(0.014868985279463232), 'value_loss': np.float64(0.01790377704310231), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027390555187594146)}\n",
            "Evaluation: Mean reward: -2.50, mean episode length: 3.50\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 451, timesteps: 923648\n",
            "Mean reward: -2.94, mean episode length: 3.94\n",
            "Losses: {'policy_loss': np.float64(0.013743499817792326), 'value_loss': np.float64(0.018448822462232783), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024183425499359146)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 452, timesteps: 925696\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.01220789272338152), 'value_loss': np.float64(0.02076057973317802), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02152320885215886)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 453, timesteps: 927744\n",
            "Mean reward: -3.07, mean episode length: 4.07\n",
            "Losses: {'policy_loss': np.float64(0.014436460449360311), 'value_loss': np.float64(0.02221805159933865), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017442882613977417)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 454, timesteps: 929792\n",
            "Mean reward: -3.04, mean episode length: 4.04\n",
            "Losses: {'policy_loss': np.float64(0.019000384025275707), 'value_loss': np.float64(0.018189207941759378), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02271236592787318)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 455, timesteps: 931840\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.02456703077768907), 'value_loss': np.float64(0.0229332642047666), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.025847957440419123)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 456, timesteps: 933888\n",
            "Mean reward: -3.09, mean episode length: 4.09\n",
            "Losses: {'policy_loss': np.float64(0.01087743358220905), 'value_loss': np.float64(0.02171809202991426), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020969420787878335)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_933888.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 457, timesteps: 935936\n",
            "Mean reward: -3.01, mean episode length: 4.01\n",
            "Losses: {'policy_loss': np.float64(0.0099274524836801), 'value_loss': np.float64(0.01956400804920122), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026194247067905962)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 458, timesteps: 937984\n",
            "Mean reward: -3.05, mean episode length: 4.05\n",
            "Losses: {'policy_loss': np.float64(0.010777821647934616), 'value_loss': np.float64(0.02602469321573153), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026129075296921656)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 459, timesteps: 940032\n",
            "Mean reward: -3.12, mean episode length: 4.12\n",
            "Losses: {'policy_loss': np.float64(0.022576734772883356), 'value_loss': np.float64(0.024113506689900532), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.08481671067420393)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 460, timesteps: 942080\n",
            "Mean reward: -3.32, mean episode length: 4.32\n",
            "Losses: {'policy_loss': np.float64(0.025373529351782054), 'value_loss': np.float64(0.02679531299509108), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02243463508784771)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 461, timesteps: 944128\n",
            "Mean reward: -3.18, mean episode length: 4.18\n",
            "Losses: {'policy_loss': np.float64(0.01156898564659059), 'value_loss': np.float64(0.02200233010808006), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01835835096426308)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 462, timesteps: 946176\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.014074607752263546), 'value_loss': np.float64(0.01940988760907203), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026135505380807444)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 463, timesteps: 948224\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.015601270250044763), 'value_loss': np.float64(0.017982288991333917), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03534208206110634)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 464, timesteps: 950272\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.020053807529620826), 'value_loss': np.float64(0.02233420329866931), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02974748588167131)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 465, timesteps: 952320\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.013442854164168239), 'value_loss': np.float64(0.019573257653973997), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.020142439665505663)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 466, timesteps: 954368\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.018851655535399914), 'value_loss': np.float64(0.01651185515220277), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.03003510579583235)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 467, timesteps: 956416\n",
            "Mean reward: -2.87, mean episode length: 3.87\n",
            "Losses: {'policy_loss': np.float64(0.010492866014828905), 'value_loss': np.float64(0.016345270996680483), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018265499966219068)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 468, timesteps: 958464\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.015072458190843463), 'value_loss': np.float64(0.015964383608661592), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023790780949639156)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 469, timesteps: 960512\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.020752169541083276), 'value_loss': np.float64(0.02280453877756372), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.026910554472124204)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 470, timesteps: 962560\n",
            "Mean reward: -3.00, mean episode length: 4.00\n",
            "Losses: {'policy_loss': np.float64(0.016033255669754), 'value_loss': np.float64(0.025101241568336263), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.027520053583430126)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 471, timesteps: 964608\n",
            "Mean reward: -2.84, mean episode length: 3.84\n",
            "Losses: {'policy_loss': np.float64(0.017495713487733155), 'value_loss': np.float64(0.016795599745819345), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02743529123836197)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 472, timesteps: 966656\n",
            "Mean reward: -2.78, mean episode length: 3.78\n",
            "Losses: {'policy_loss': np.float64(0.02351340401219204), 'value_loss': np.float64(0.0193859041610267), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.04126200269092806)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 473, timesteps: 968704\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.018897583184298128), 'value_loss': np.float64(0.01966792784514837), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.019751385698327795)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 474, timesteps: 970752\n",
            "Mean reward: -2.88, mean episode length: 3.88\n",
            "Losses: {'policy_loss': np.float64(0.011022714956197888), 'value_loss': np.float64(0.015788997086929157), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.021437381190480664)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 475, timesteps: 972800\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.011249086004681885), 'value_loss': np.float64(0.01851970815914683), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018322199408430606)}\n",
            "Evaluation: Mean reward: -2.20, mean episode length: 3.20\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 476, timesteps: 974848\n",
            "Mean reward: -2.85, mean episode length: 3.85\n",
            "Losses: {'policy_loss': np.float64(0.012449218949768692), 'value_loss': np.float64(0.020683888316852972), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.023473039706004784)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 477, timesteps: 976896\n",
            "Mean reward: -2.93, mean episode length: 3.93\n",
            "Losses: {'policy_loss': np.float64(0.02290771179832518), 'value_loss': np.float64(0.018203639454441145), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0421639792912174)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 478, timesteps: 978944\n",
            "Mean reward: -2.95, mean episode length: 3.95\n",
            "Losses: {'policy_loss': np.float64(0.01724508119514212), 'value_loss': np.float64(0.021239314548438415), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.017921490973094478)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 479, timesteps: 980992\n",
            "Mean reward: -2.86, mean episode length: 3.86\n",
            "Losses: {'policy_loss': np.float64(0.013305507600307465), 'value_loss': np.float64(0.016854840592714027), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.01742003034451045)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 480, timesteps: 983040\n",
            "Mean reward: -2.97, mean episode length: 3.97\n",
            "Losses: {'policy_loss': np.float64(0.012713879812508821), 'value_loss': np.float64(0.02196770632872358), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.032332267466699705)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_983040.pth\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 481, timesteps: 985088\n",
            "Mean reward: -2.79, mean episode length: 3.79\n",
            "Losses: {'policy_loss': np.float64(0.02410618175053969), 'value_loss': np.float64(0.017932224087417126), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.031239921314409003)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 482, timesteps: 987136\n",
            "Mean reward: -3.08, mean episode length: 4.08\n",
            "Losses: {'policy_loss': np.float64(0.012882212526164949), 'value_loss': np.float64(0.02301158444606699), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02821107892668806)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 483, timesteps: 989184\n",
            "Mean reward: -3.06, mean episode length: 4.06\n",
            "Losses: {'policy_loss': np.float64(0.007675872533582151), 'value_loss': np.float64(0.031153553281910717), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02749814058188349)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 484, timesteps: 991232\n",
            "Mean reward: -3.16, mean episode length: 4.16\n",
            "Losses: {'policy_loss': np.float64(0.020524000225123018), 'value_loss': np.float64(0.026076543843373656), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.024950021819677204)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 485, timesteps: 993280\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.01236794691067189), 'value_loss': np.float64(0.020583414123393595), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.018197367899119854)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 486, timesteps: 995328\n",
            "Mean reward: -2.92, mean episode length: 3.92\n",
            "Losses: {'policy_loss': np.float64(0.0162473822128959), 'value_loss': np.float64(0.020140163163887337), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02578612364595756)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 487, timesteps: 997376\n",
            "Mean reward: -2.99, mean episode length: 3.99\n",
            "Losses: {'policy_loss': np.float64(0.016493386472575366), 'value_loss': np.float64(0.016745106637245044), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.0232837071234826)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 488, timesteps: 999424\n",
            "Mean reward: -2.94, mean episode length: 3.94\n",
            "Losses: {'policy_loss': np.float64(0.012966004724148661), 'value_loss': np.float64(0.01997846449376084), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.02184068044880405)}\n",
            "Early stopping at epoch 1/10 due to reaching KL target\n",
            "Iteration: 489, timesteps: 1001472\n",
            "Mean reward: -2.91, mean episode length: 3.91\n",
            "Losses: {'policy_loss': np.float64(0.013960570795461535), 'value_loss': np.float64(0.017519151559099555), 'entropy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.022601249656872824)}\n",
            "Model saved to sb3_style_models/PandaReach-v3_final.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import panda_gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Type, Union, Optional, Any, Callable\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, dtype=np.float64)\n",
        "        self.var = np.ones(shape, dtype=np.float64)\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * self.count\n",
        "        m_b = batch_var * batch_count\n",
        "        m_2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
        "        new_var = m_2 / tot_count\n",
        "\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = tot_count\n",
        "\n",
        "class VecNormalize:\n",
        "    def __init__(self, obs_rms=None, ret_rms=None, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "        self.obs_rms = obs_rms or RunningMeanStd(shape=(1,))\n",
        "        self.ret_rms = ret_rms or RunningMeanStd(shape=())\n",
        "        self.clipob = clipob\n",
        "        self.cliprew = cliprew\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.returns = 0.0\n",
        "\n",
        "    def normalize_obs(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            normalized_obs = {}\n",
        "            for key, value in obs.items():\n",
        "                if key in [\"observation\", \"desired_goal\"]:\n",
        "                    normalized_obs[key] = self._normalize_obs(value)\n",
        "                else:\n",
        "                    normalized_obs[key] = value\n",
        "            return normalized_obs\n",
        "        else:\n",
        "            return self._normalize_obs(obs)\n",
        "\n",
        "    def _normalize_obs(self, obs):\n",
        "        obs_array = np.array(obs)\n",
        "        self.obs_rms.update(obs_array)\n",
        "        return np.clip((obs_array - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon),\n",
        "                       -self.clipob, self.clipob)\n",
        "\n",
        "    def normalize_reward(self, reward):\n",
        "        self.returns = self.returns * self.gamma + reward\n",
        "        self.ret_rms.update(np.array([self.returns]))\n",
        "        return np.clip(reward / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "\n",
        "    def reset_returns(self):\n",
        "        self.returns = 0.0\n",
        "\n",
        "# NN for Actor\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, goal_dim, action_dim, net_arch=[256, 256], activation_fn=nn.ReLU):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.input_dim = obs_dim + goal_dim\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        last_dim = self.input_dim\n",
        "\n",
        "        for dim in net_arch:\n",
        "            self.layers.append(nn.Linear(last_dim, dim))\n",
        "            self.layers.append(activation_fn())\n",
        "            last_dim = dim\n",
        "\n",
        "        # Mean of the Gaussian policy\n",
        "        self.mean_layer = nn.Linear(last_dim, action_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "        # weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=1)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        mean = self.mean_layer(x)\n",
        "        mean = torch.tanh(mean)\n",
        "\n",
        "        log_std = self.log_std.expand_as(mean)\n",
        "        log_std = torch.clamp(log_std, -20, 2)\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "# NN for Critic\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, goal_dim, net_arch=[256, 256], activation_fn=nn.ReLU):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.input_dim = obs_dim + goal_dim\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        last_dim = self.input_dim\n",
        "\n",
        "        for dim in net_arch:\n",
        "            self.layers.append(nn.Linear(last_dim, dim))\n",
        "            self.layers.append(activation_fn())\n",
        "            last_dim = dim\n",
        "\n",
        "        self.value_layer = nn.Linear(last_dim, 1)\n",
        "\n",
        "        # weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=1)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        value = self.value_layer(x)\n",
        "        return value\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self, buffer_size, obs_dim, goal_dim, action_dim, gamma=0.99, gae_lambda=0.95):\n",
        "        self.observations = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
        "        self.goals = np.zeros((buffer_size, goal_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.advantages = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.returns = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.values = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.log_probs = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.dones = np.zeros(buffer_size, dtype=np.float32)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.buffer_size = buffer_size\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "    def add(self, obs, goal, action, reward, done, value, log_prob):\n",
        "        self.observations[self.pos] = obs\n",
        "        self.goals[self.pos] = goal\n",
        "        self.actions[self.pos] = action\n",
        "        self.rewards[self.pos] = reward\n",
        "        self.dones[self.pos] = done\n",
        "        self.values[self.pos] = value\n",
        "        self.log_probs[self.pos] = log_prob\n",
        "\n",
        "        self.pos += 1\n",
        "        if self.pos == self.buffer_size:\n",
        "            self.full = True\n",
        "            self.pos = 0\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value=0.0):\n",
        "        last_gae_lam = 0\n",
        "        for step in reversed(range(self.buffer_size)):\n",
        "            if step == self.buffer_size - 1:\n",
        "                next_non_terminal = 1.0 - self.dones[step]\n",
        "                next_values = last_value\n",
        "            else:\n",
        "                next_non_terminal = 1.0 - self.dones[step + 1]\n",
        "                next_values = self.values[step + 1]\n",
        "\n",
        "            delta = self.rewards[step] + self.gamma * next_values * next_non_terminal - self.values[step]\n",
        "            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
        "            self.advantages[step] = last_gae_lam\n",
        "\n",
        "        self.returns = self.advantages + self.values\n",
        "\n",
        "    def get(self):\n",
        "        indices = np.arange(self.buffer_size)\n",
        "        return (\n",
        "            self.observations,\n",
        "            self.goals,\n",
        "            self.actions,\n",
        "            self.rewards,\n",
        "            self.returns,\n",
        "            self.log_probs,\n",
        "            self.advantages\n",
        "        )\n",
        "\n",
        "    def clear(self):\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "# PPO Agent class\n",
        "class PPO:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        clip_range_vf=None,\n",
        "        ent_coef=0.0,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        use_sde=False,\n",
        "        sde_sample_freq=4,\n",
        "        target_kl=None,\n",
        "        tensorboard_log=None,\n",
        "        create_eval_env=False,\n",
        "        policy_kwargs=None,\n",
        "        verbose=0,\n",
        "        seed=None,\n",
        "        device=device,\n",
        "        _init_setup_model=True\n",
        "    ):\n",
        "        self.observation_space = env.observation_space\n",
        "        self.action_space = env.action_space\n",
        "        self.env = env\n",
        "        self.obs_dim = env.observation_space['observation'].shape[0]\n",
        "        self.goal_dim = env.observation_space['desired_goal'].shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "\n",
        "        # Actions\n",
        "        self.action_scale = (env.action_space.high - env.action_space.low) / 2\n",
        "        self.action_bias = (env.action_space.high + env.action_space.low) / 2\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.ent_coef = ent_coef\n",
        "        self.vf_coef = vf_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.clip_range = clip_range\n",
        "        self.clip_range_vf = clip_range_vf\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "        policy_kwargs = policy_kwargs or {}\n",
        "        self.actor = Actor(self.obs_dim, self.goal_dim, self.action_dim,\n",
        "                           net_arch=policy_kwargs.get(\"net_arch\", [256, 256]))\n",
        "        self.critic = Critic(self.obs_dim, self.goal_dim,\n",
        "                            net_arch=policy_kwargs.get(\"net_arch\", [256, 256]))\n",
        "\n",
        "        self.actor.to(device)\n",
        "        self.critic.to(device)\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Experience bufffer\n",
        "        self.rollout_buffer = RolloutBuffer(n_steps, self.obs_dim, self.goal_dim, self.action_dim, gamma, gae_lambda)\n",
        "\n",
        "        # Normalization\n",
        "        self.normalizer = VecNormalize(gamma=gamma)\n",
        "\n",
        "        self.timesteps = 0\n",
        "        \n",
        "        # Track losses\n",
        "        self.logger = {\n",
        "            \"timesteps\": [],\n",
        "            \"mean_reward\": [],\n",
        "            \"episode_length\": [],\n",
        "            \"policy_loss\": [],\n",
        "            \"value_loss\": [],\n",
        "            \"entropy_loss\": [],\n",
        "            \"kl_divergence\": []\n",
        "        }\n",
        "\n",
        "        self.callbacks = []\n",
        "\n",
        "    def _process_observation(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            observation = np.array(obs[\"observation\"], dtype=np.float32)\n",
        "            goal = np.array(obs[\"desired_goal\"], dtype=np.float32)\n",
        "        else:\n",
        "            observation = obs[:self.obs_dim]\n",
        "            goal = obs[self.obs_dim:]\n",
        "\n",
        "        return observation, goal\n",
        "\n",
        "    def _combined_input(self, obs, goal):\n",
        "        if isinstance(obs, np.ndarray) and isinstance(goal, np.ndarray):\n",
        "            return np.concatenate([obs, goal], axis=-1)\n",
        "        else:\n",
        "            return torch.cat([obs, goal], dim=-1)\n",
        "\n",
        "    def _normalize_observation(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            return self.normalizer.normalize_obs(obs)\n",
        "        return obs\n",
        "\n",
        "    def _normalize_reward(self, reward):\n",
        "        return self.normalizer.normalize_reward(reward)\n",
        "\n",
        "    def predict(self, observation, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            observation = self._normalize_observation(observation)\n",
        "            obs_array, goal_array = self._process_observation(observation)\n",
        "\n",
        "            obs_tensor = torch.FloatTensor(obs_array).to(device)\n",
        "            goal_tensor = torch.FloatTensor(goal_array).to(device)\n",
        "            x = self._combined_input(obs_tensor, goal_tensor)\n",
        "            action_mean, action_std = self.actor(x)\n",
        "\n",
        "            if deterministic:\n",
        "                action = action_mean\n",
        "            else:\n",
        "                dist = Normal(action_mean, action_std)\n",
        "                action = dist.sample()\n",
        "\n",
        "            action_np = action.cpu().numpy()\n",
        "            scaled_action = action_np * self.action_scale + self.action_bias\n",
        "\n",
        "            return scaled_action, None \n",
        "\n",
        "    def collect_rollouts(self, env, n_steps=None):\n",
        "        n_steps = n_steps or self.n_steps\n",
        "        self.rollout_buffer.clear()\n",
        "\n",
        "        self.normalizer.reset_returns()\n",
        "\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        current_episode_reward = 0\n",
        "        current_episode_length = 0\n",
        "\n",
        "        for step in range(n_steps):\n",
        "            current_episode_length += 1\n",
        "\n",
        "            norm_obs = self._normalize_observation(obs)\n",
        "            obs_array, goal_array = self._process_observation(norm_obs)\n",
        "\n",
        "            obs_tensor = torch.FloatTensor(obs_array).to(device)\n",
        "            goal_tensor = torch.FloatTensor(goal_array).to(device)\n",
        "\n",
        "            x = self._combined_input(obs_tensor, goal_tensor)\n",
        "\n",
        "            # Value estimate\n",
        "            with torch.no_grad():\n",
        "                value = self.critic(x).cpu().numpy().flatten()\n",
        "                action_mean, action_std = self.actor(x)\n",
        "\n",
        "                dist = Normal(action_mean, action_std)\n",
        "                action = dist.sample()\n",
        "                log_prob = dist.log_prob(action).sum(dim=-1).cpu().numpy()\n",
        "\n",
        "            action_np = action.cpu().numpy()\n",
        "            scaled_action = action_np * self.action_scale + self.action_bias\n",
        "\n",
        "            next_obs, reward, terminated, truncated, info = env.step(scaled_action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            current_episode_reward += reward\n",
        "            norm_reward = self._normalize_reward(reward)\n",
        "            self.rollout_buffer.add(obs_array, goal_array, action_np, norm_reward, done, value, log_prob)\n",
        "            obs = next_obs\n",
        "\n",
        "            if done:\n",
        "                episode_rewards.append(current_episode_reward)\n",
        "                episode_lengths.append(current_episode_length)\n",
        "                current_episode_reward = 0\n",
        "                current_episode_length = 0\n",
        "\n",
        "                obs, _ = env.reset()\n",
        "                self.normalizer.reset_returns()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            obs_array, goal_array = self._process_observation(self._normalize_observation(obs))\n",
        "            obs_tensor = torch.FloatTensor(obs_array).to(device)\n",
        "            goal_tensor = torch.FloatTensor(goal_array).to(device)\n",
        "            x = self._combined_input(obs_tensor, goal_tensor)\n",
        "            last_value = self.critic(x).cpu().numpy().flatten()\n",
        "\n",
        "        self.rollout_buffer.compute_returns_and_advantages(last_value)\n",
        "\n",
        "        mean_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
        "        mean_length = np.mean(episode_lengths) if episode_lengths else 0\n",
        "\n",
        "        return mean_reward, mean_length\n",
        "\n",
        "    def train(self):\n",
        "        observations, goals, actions, _, returns, old_log_probs, advantages = self.rollout_buffer.get()\n",
        "        batch_size = self.batch_size or self.n_steps\n",
        "        n_batches = self.n_steps // batch_size\n",
        "\n",
        "        # Normalize\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "        entropy_losses = []\n",
        "        kl_divs = []\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            perm = np.random.permutation(self.n_steps)\n",
        "\n",
        "            for batch_idx in range(n_batches):\n",
        "                start_idx = batch_idx * batch_size\n",
        "                end_idx = (batch_idx + 1) * batch_size\n",
        "                batch_indices = perm[start_idx:end_idx]\n",
        "\n",
        "                obs_batch = torch.FloatTensor(observations[batch_indices]).to(device)\n",
        "                goal_batch = torch.FloatTensor(goals[batch_indices]).to(device)\n",
        "                action_batch = torch.FloatTensor(actions[batch_indices]).to(device)\n",
        "                old_log_prob_batch = torch.FloatTensor(old_log_probs[batch_indices]).to(device)\n",
        "                advantage_batch = torch.FloatTensor(advantages[batch_indices]).to(device)\n",
        "                return_batch = torch.FloatTensor(returns[batch_indices]).to(device)\n",
        "                state_batch = self._combined_input(obs_batch, goal_batch)\n",
        "\n",
        "                # Update critic\n",
        "                values = self.critic(state_batch).flatten()\n",
        "\n",
        "                # Value loss\n",
        "                if self.clip_range_vf is None:\n",
        "                    value_loss = F.mse_loss(values, return_batch)\n",
        "                else:\n",
        "                    values_clipped = torch.clamp(\n",
        "                        values,\n",
        "                        return_batch - self.clip_range_vf,\n",
        "                        return_batch + self.clip_range_vf\n",
        "                    )\n",
        "                    value_loss = torch.max(\n",
        "                        F.mse_loss(values, return_batch),\n",
        "                        F.mse_loss(values_clipped, return_batch)\n",
        "                    )\n",
        "\n",
        "                # Update critic\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                value_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Update actor\n",
        "                action_mean, action_std = self.actor(state_batch)\n",
        "                dist = Normal(action_mean, action_std)\n",
        "\n",
        "                log_prob = dist.log_prob(action_batch).sum(dim=-1)\n",
        "                entropy = dist.entropy().sum(dim=-1).mean()\n",
        "\n",
        "                ratio = torch.exp(log_prob - old_log_prob_batch)\n",
        "                policy_loss_1 = advantage_batch * ratio\n",
        "                policy_loss_2 = advantage_batch * torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range)\n",
        "                policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
        "\n",
        "                entropy_loss = -entropy * self.ent_coef\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    log_ratio = log_prob - old_log_prob_batch\n",
        "                    approx_kl = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).item()\n",
        "                    kl_divs.append(approx_kl)\n",
        "\n",
        "                # Update actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                (policy_loss + entropy_loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                policy_losses.append(policy_loss.item())\n",
        "                value_losses.append(value_loss.item())\n",
        "                entropy_losses.append(entropy_loss.item())\n",
        "\n",
        "            if self.target_kl is not None and np.mean(kl_divs) > 1.5 * self.target_kl:\n",
        "                print(f\"Early stopping at epoch {epoch+1}/{self.n_epochs} due to reaching KL target\")\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": np.mean(policy_losses),\n",
        "            \"value_loss\": np.mean(value_losses),\n",
        "            \"entropy_loss\": np.mean(entropy_losses),\n",
        "            \"kl_divergence\": np.mean(kl_divs)\n",
        "        }\n",
        "\n",
        "    def learn(self, total_timesteps, callback=None, log_interval=1, eval_env=None, eval_freq=None, n_eval_episodes=5, tb_log_name=\"PPO\", reset_num_timesteps=True):\n",
        "        timesteps_elapsed = 0\n",
        "        timesteps_since_eval = 0\n",
        "        iterations = 0\n",
        "\n",
        "        self.timesteps = 0 if reset_num_timesteps else self.timesteps\n",
        "\n",
        "        while self.timesteps < total_timesteps:\n",
        "            mean_reward, mean_len = self.collect_rollouts(self.env, self.n_steps)\n",
        "\n",
        "            losses = self.train()\n",
        "\n",
        "            timesteps_elapsed += self.n_steps\n",
        "            self.timesteps += self.n_steps\n",
        "            timesteps_since_eval += self.n_steps\n",
        "            iterations += 1\n",
        "\n",
        "            if iterations % log_interval == 0:\n",
        "                print(f\"Iteration: {iterations}, timesteps: {self.timesteps}\")\n",
        "                print(f\"Mean reward: {mean_reward:.2f}, mean episode length: {mean_len:.2f}\")\n",
        "                print(f\"Losses: {losses}\")\n",
        "\n",
        "                self.logger[\"timesteps\"].append(self.timesteps)\n",
        "                self.logger[\"mean_reward\"].append(mean_reward)\n",
        "                self.logger[\"episode_length\"].append(mean_len)\n",
        "\n",
        "                # Storing loss values\n",
        "                self.logger[\"policy_loss\"].append(losses[\"policy_loss\"])\n",
        "                self.logger[\"value_loss\"].append(losses[\"value_loss\"])\n",
        "                self.logger[\"entropy_loss\"].append(losses[\"entropy_loss\"])\n",
        "                self.logger[\"kl_divergence\"].append(losses[\"kl_divergence\"])\n",
        "\n",
        "            if eval_env is not None and eval_freq is not None and timesteps_since_eval >= eval_freq:\n",
        "                timesteps_since_eval = 0\n",
        "                self.evaluate_policy(eval_env, n_eval_episodes)\n",
        "\n",
        "            if callback is not None:\n",
        "                callback(locals(), globals())\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate_policy(self, env=None, n_eval_episodes=10, deterministic=True):\n",
        "        eval_env = env or self.env\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(n_eval_episodes):\n",
        "            obs, _ = eval_env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "\n",
        "            while not done:\n",
        "                action, _ = self.predict(obs, deterministic=deterministic)\n",
        "                obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "\n",
        "        mean_reward = np.mean(episode_rewards)\n",
        "        mean_length = np.mean(episode_lengths)\n",
        "\n",
        "        print(f\"Evaluation: Mean reward: {mean_reward:.2f}, mean episode length: {mean_length:.2f}\")\n",
        "\n",
        "        return mean_reward, mean_length\n",
        "\n",
        "    def save(self, path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save({\n",
        "            \"actor_state_dict\": self.actor.state_dict(),\n",
        "            \"critic_state_dict\": self.critic.state_dict(),\n",
        "            \"actor_optimizer_state_dict\": self.actor_optimizer.state_dict(),\n",
        "            \"critic_optimizer_state_dict\": self.critic_optimizer.state_dict(),\n",
        "            \"normalizer_mean\": self.normalizer.obs_rms.mean,\n",
        "            \"normalizer_var\": self.normalizer.obs_rms.var,\n",
        "            \"normalizer_count\": self.normalizer.obs_rms.count,\n",
        "            \"ret_rms_mean\": self.normalizer.ret_rms.mean,\n",
        "            \"ret_rms_var\": self.normalizer.ret_rms.var,\n",
        "            \"ret_rms_count\": self.normalizer.ret_rms.count,\n",
        "        }, path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        self.actor.load_state_dict(checkpoint[\"actor_state_dict\"])\n",
        "        self.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
        "        self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
        "        self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
        "\n",
        "        # Load normalizer\n",
        "        self.normalizer.obs_rms.mean = checkpoint[\"normalizer_mean\"]\n",
        "        self.normalizer.obs_rms.var = checkpoint[\"normalizer_var\"]\n",
        "        self.normalizer.obs_rms.count = checkpoint[\"normalizer_count\"]\n",
        "        self.normalizer.ret_rms.mean = checkpoint[\"ret_rms_mean\"]\n",
        "        self.normalizer.ret_rms.var = checkpoint[\"ret_rms_var\"]\n",
        "        self.normalizer.ret_rms.count = checkpoint[\"ret_rms_count\"]\n",
        "\n",
        "        print(f\"Model loaded from {path}\")\n",
        "        return self\n",
        "\n",
        "# Callback system\n",
        "class BaseCallback:\n",
        "    def __init__(self, verbose=0):\n",
        "        self.verbose = verbose\n",
        "        self._init_callback()\n",
        "\n",
        "    def _init_callback(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, locals_dict, globals_dict):\n",
        "        self.on_step()\n",
        "        return True\n",
        "\n",
        "    def on_step(self):\n",
        "        pass\n",
        "\n",
        "class CheckpointCallback(BaseCallback):\n",
        "    def __init__(self, save_freq, save_path, name_prefix=\"model\", verbose=0):\n",
        "        super(CheckpointCallback, self).__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        self.last_save_step = 0\n",
        "\n",
        "    def _init_callback(self):\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def on_step(self):\n",
        "        model = locals_dict[\"self\"]\n",
        "        timesteps = model.timesteps\n",
        "\n",
        "        if timesteps > 0 and timesteps - self.last_save_step >= self.save_freq:\n",
        "            path = f\"{self.save_path}/{self.name_prefix}_{timesteps}_steps.pth\"\n",
        "            model.save(path)\n",
        "            self.last_save_step = timesteps\n",
        "\n",
        "def train_ppo(env_name=\"PandaReach-v3\", total_timesteps=1000000, log_interval=1, save_interval=10000, eval_interval=50000, n_eval_episodes=10):\n",
        "    print(f\"Training on {env_name}...\")\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    eval_env = gym.make(env_name)  # Separate env for evaluation\n",
        "\n",
        "    os.makedirs('sb3_style_models', exist_ok=True)\n",
        "    os.makedirs('sb3_style_logs', exist_ok=True)\n",
        "\n",
        "    # Initialize PPO agent\n",
        "    ppo = PPO(\n",
        "        env=env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        clip_range_vf=None,\n",
        "        ent_coef=0.0,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        target_kl=0.01,\n",
        "        policy_kwargs={\"net_arch\": [256, 256]}\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = lambda local_vars, global_vars: (\n",
        "        local_vars['self'].save(f\"sb3_style_models/{env_name}_{local_vars['self'].timesteps}.pth\")\n",
        "        if local_vars['iterations'] % (save_interval // local_vars['self'].n_steps) == 0\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # Train agent\n",
        "    ppo.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=checkpoint_callback,\n",
        "        log_interval=log_interval,\n",
        "        eval_env=eval_env,\n",
        "        eval_freq=eval_interval,\n",
        "        n_eval_episodes=n_eval_episodes\n",
        "    )\n",
        "\n",
        "    ppo.save(f\"sb3_style_models/{env_name}_final.pth\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Mean reward\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"mean_reward\"])\n",
        "    plt.title('Mean Reward')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Reward')\n",
        "\n",
        "    # Episode length\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"episode_length\"])\n",
        "    plt.title('Episode Length')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Length')\n",
        "\n",
        "    # Policy and value losses\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"policy_loss\"], label='Policy Loss')\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"value_loss\"], label='Value Loss')\n",
        "    plt.title('Policy and Value Losses')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Entropy loss and KL divergence\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"entropy_loss\"], label='Entropy Loss')\n",
        "    plt.plot(ppo.logger[\"timesteps\"], ppo.logger[\"kl_divergence\"], label='KL Divergence')\n",
        "    plt.title('Entropy Loss and KL Divergence')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sb3_style_logs/training_progress.png')\n",
        "    plt.close()\n",
        "\n",
        "    return ppo\n",
        "\n",
        "def evaluate_ppo(env_name=\"PandaReach-v3\", model_path=None, n_eval_episodes=10, render=True):\n",
        "    print(f\"Evaluating on {env_name}...\")\n",
        "\n",
        "    env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
        "    # Initialize agent\n",
        "    ppo = PPO(env=env)\n",
        "\n",
        "    if model_path:\n",
        "        ppo.load(model_path)\n",
        "        print(f\"Model loaded from {model_path}\")\n",
        "    else:\n",
        "        print(\"No model loaded, using random policy\")\n",
        "\n",
        "    mean_reward, mean_length = ppo.evaluate_policy(n_eval_episodes=n_eval_episodes)\n",
        "    env.close()\n",
        "    return mean_reward, mean_length\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_name = \"PandaReach-v3\"\n",
        "    ppo_agent = train_ppo(\n",
        "        env_name=env_name,\n",
        "        total_timesteps=1000000,\n",
        "        log_interval=1,\n",
        "        save_interval=50000,\n",
        "        eval_interval=50000\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_NH2mL3S3LFk"
      },
      "outputs": [],
      "source": [
        "#check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKjsi-6d3L7B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
