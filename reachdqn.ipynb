{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXOJnEz7sgVY",
        "outputId": "ceb280a1-f4f3-4de0-910b-89af0ab2c511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting panda_gym\n",
            "  Downloading panda_gym-3.0.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting pybullet (from panda_gym)\n",
            "  Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from panda_gym) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panda_gym-3.0.7-py3-none-any.whl (23 kB)\n",
            "Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, panda_gym, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 panda_gym-3.0.7 pybullet-3.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium torch numpy matplotlib tqdm panda_gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49kiwugvsub3",
        "outputId": "0753080d-0e09-4f2b-dec6-7c02093f4c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Timestep: 150802, Episodes: 16920\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 150825, Episodes: 16930\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 150852, Episodes: 16940\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 150885, Episodes: 16950\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 150916, Episodes: 16960\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 150956, Episodes: 16970\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 150985, Episodes: 16980\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 151014, Episodes: 16990\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 151039, Episodes: 17000\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 151068, Episodes: 17010\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 151097, Episodes: 17020\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 151135, Episodes: 17030\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151168, Episodes: 17040\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 151198, Episodes: 17050\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 151227, Episodes: 17060\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151261, Episodes: 17070\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 151293, Episodes: 17080\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 151335, Episodes: 17090\n",
            "Mean reward: -3.20, Mean length: 4.20\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151361, Episodes: 17100\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 151394, Episodes: 17110\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 151416, Episodes: 17120\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151438, Episodes: 17130\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 151470, Episodes: 17140\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 151503, Episodes: 17150\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 151531, Episodes: 17160\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 151564, Episodes: 17170\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 151590, Episodes: 17180\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 151615, Episodes: 17190\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 151647, Episodes: 17200\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 151677, Episodes: 17210\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 151710, Episodes: 17220\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 151732, Episodes: 17230\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 151759, Episodes: 17240\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151786, Episodes: 17250\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151817, Episodes: 17260\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 151849, Episodes: 17270\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 151876, Episodes: 17280\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 151904, Episodes: 17290\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 151933, Episodes: 17300\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 151961, Episodes: 17310\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 151984, Episodes: 17320\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 152013, Episodes: 17330\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 152038, Episodes: 17340\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 152064, Episodes: 17350\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 152097, Episodes: 17360\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 152130, Episodes: 17370\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 152170, Episodes: 17380\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 152202, Episodes: 17390\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 152225, Episodes: 17400\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 152264, Episodes: 17410\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 152296, Episodes: 17420\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 152327, Episodes: 17430\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 152359, Episodes: 17440\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 152394, Episodes: 17450\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00087, Exploration rate: 0.050\n",
            "Timestep: 152420, Episodes: 17460\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 152453, Episodes: 17470\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 152481, Episodes: 17480\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 152516, Episodes: 17490\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 152551, Episodes: 17500\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 152581, Episodes: 17510\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 152613, Episodes: 17520\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 152643, Episodes: 17530\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 152678, Episodes: 17540\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 152710, Episodes: 17550\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 152736, Episodes: 17560\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 152772, Episodes: 17570\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 152800, Episodes: 17580\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 152837, Episodes: 17590\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 152871, Episodes: 17600\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 152897, Episodes: 17610\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 152929, Episodes: 17620\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 152965, Episodes: 17630\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153001, Episodes: 17640\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 153029, Episodes: 17650\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 153048, Episodes: 17660\n",
            "Mean reward: -0.90, Mean length: 1.90\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 153076, Episodes: 17670\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 153110, Episodes: 17680\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 153140, Episodes: 17690\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 153169, Episodes: 17700\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 153198, Episodes: 17710\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 153230, Episodes: 17720\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 153259, Episodes: 17730\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153287, Episodes: 17740\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 153322, Episodes: 17750\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 153353, Episodes: 17760\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 153385, Episodes: 17770\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 153412, Episodes: 17780\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 153440, Episodes: 17790\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 153470, Episodes: 17800\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 153497, Episodes: 17810\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153524, Episodes: 17820\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 153557, Episodes: 17830\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153586, Episodes: 17840\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 153619, Episodes: 17850\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153644, Episodes: 17860\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153670, Episodes: 17870\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 153694, Episodes: 17880\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153717, Episodes: 17890\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 153749, Episodes: 17900\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153778, Episodes: 17910\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 153808, Episodes: 17920\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153837, Episodes: 17930\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153859, Episodes: 17940\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 153881, Episodes: 17950\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 153916, Episodes: 17960\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 153951, Episodes: 17970\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 153982, Episodes: 17980\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 154011, Episodes: 17990\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 154043, Episodes: 18000\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 154068, Episodes: 18010\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 154102, Episodes: 18020\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00085, Exploration rate: 0.050\n",
            "Timestep: 154139, Episodes: 18030\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 154168, Episodes: 18040\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 154196, Episodes: 18050\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 154223, Episodes: 18060\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 154252, Episodes: 18070\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 154275, Episodes: 18080\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 154308, Episodes: 18090\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 154336, Episodes: 18100\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 154378, Episodes: 18110\n",
            "Mean reward: -3.20, Mean length: 4.20\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 154413, Episodes: 18120\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 154440, Episodes: 18130\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 154477, Episodes: 18140\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 154503, Episodes: 18150\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 154536, Episodes: 18160\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 154566, Episodes: 18170\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 154592, Episodes: 18180\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 154623, Episodes: 18190\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 154655, Episodes: 18200\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 154681, Episodes: 18210\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 154709, Episodes: 18220\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 154741, Episodes: 18230\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 154765, Episodes: 18240\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 154797, Episodes: 18250\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 154830, Episodes: 18260\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 154860, Episodes: 18270\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 154892, Episodes: 18280\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 154922, Episodes: 18290\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 154958, Episodes: 18300\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 154994, Episodes: 18310\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 155021, Episodes: 18320\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 155046, Episodes: 18330\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 155074, Episodes: 18340\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 155107, Episodes: 18350\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 155139, Episodes: 18360\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 155172, Episodes: 18370\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 155205, Episodes: 18380\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 155241, Episodes: 18390\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 155269, Episodes: 18400\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 155302, Episodes: 18410\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 155332, Episodes: 18420\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 155362, Episodes: 18430\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 155389, Episodes: 18440\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 155415, Episodes: 18450\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 155445, Episodes: 18460\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 155477, Episodes: 18470\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 155501, Episodes: 18480\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 155534, Episodes: 18490\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 155565, Episodes: 18500\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 155597, Episodes: 18510\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 155632, Episodes: 18520\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 155663, Episodes: 18530\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 155693, Episodes: 18540\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 155724, Episodes: 18550\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 155754, Episodes: 18560\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 155784, Episodes: 18570\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 155818, Episodes: 18580\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 155853, Episodes: 18590\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 155885, Episodes: 18600\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 155919, Episodes: 18610\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 155948, Episodes: 18620\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 155976, Episodes: 18630\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 156003, Episodes: 18640\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 156033, Episodes: 18650\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 156066, Episodes: 18660\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 156096, Episodes: 18670\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 156131, Episodes: 18680\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 156161, Episodes: 18690\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 156188, Episodes: 18700\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 156212, Episodes: 18710\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 156243, Episodes: 18720\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 156269, Episodes: 18730\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 156298, Episodes: 18740\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 156327, Episodes: 18750\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 156359, Episodes: 18760\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 156387, Episodes: 18770\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 156419, Episodes: 18780\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 156442, Episodes: 18790\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 156465, Episodes: 18800\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 156491, Episodes: 18810\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 156520, Episodes: 18820\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 156556, Episodes: 18830\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 156582, Episodes: 18840\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 156608, Episodes: 18850\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 156642, Episodes: 18860\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 156676, Episodes: 18870\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 156707, Episodes: 18880\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 156730, Episodes: 18890\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 156758, Episodes: 18900\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 156783, Episodes: 18910\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 156811, Episodes: 18920\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 156841, Episodes: 18930\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 156868, Episodes: 18940\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 156909, Episodes: 18950\n",
            "Mean reward: -3.10, Mean length: 4.10\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 156941, Episodes: 18960\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 156974, Episodes: 18970\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 156996, Episodes: 18980\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 157023, Episodes: 18990\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 157053, Episodes: 19000\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 157087, Episodes: 19010\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 157116, Episodes: 19020\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 157144, Episodes: 19030\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 157176, Episodes: 19040\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 157212, Episodes: 19050\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 157244, Episodes: 19060\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 157278, Episodes: 19070\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 157305, Episodes: 19080\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 157335, Episodes: 19090\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 157363, Episodes: 19100\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 157390, Episodes: 19110\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 157426, Episodes: 19120\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 157454, Episodes: 19130\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 157488, Episodes: 19140\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 157517, Episodes: 19150\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 157544, Episodes: 19160\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 157575, Episodes: 19170\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 157603, Episodes: 19180\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 157639, Episodes: 19190\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 157671, Episodes: 19200\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 157712, Episodes: 19210\n",
            "Mean reward: -3.10, Mean length: 4.10\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 157739, Episodes: 19220\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 157769, Episodes: 19230\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 157799, Episodes: 19240\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 157831, Episodes: 19250\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 157857, Episodes: 19260\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 157883, Episodes: 19270\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 157919, Episodes: 19280\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 157952, Episodes: 19290\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 157980, Episodes: 19300\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 158011, Episodes: 19310\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 158036, Episodes: 19320\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 158063, Episodes: 19330\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 158090, Episodes: 19340\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 158116, Episodes: 19350\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 158148, Episodes: 19360\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 158173, Episodes: 19370\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 158200, Episodes: 19380\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 158226, Episodes: 19390\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 158259, Episodes: 19400\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 158295, Episodes: 19410\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 158322, Episodes: 19420\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 158348, Episodes: 19430\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 158382, Episodes: 19440\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 158411, Episodes: 19450\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 158443, Episodes: 19460\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 158471, Episodes: 19470\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 158496, Episodes: 19480\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 158525, Episodes: 19490\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 158559, Episodes: 19500\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 158585, Episodes: 19510\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 158614, Episodes: 19520\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 158645, Episodes: 19530\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 158673, Episodes: 19540\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 158699, Episodes: 19550\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 158726, Episodes: 19560\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 158765, Episodes: 19570\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 158797, Episodes: 19580\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 158824, Episodes: 19590\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 158849, Episodes: 19600\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 158875, Episodes: 19610\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 158904, Episodes: 19620\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 158940, Episodes: 19630\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 158973, Episodes: 19640\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 159003, Episodes: 19650\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 159036, Episodes: 19660\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 159063, Episodes: 19670\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 159092, Episodes: 19680\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 159125, Episodes: 19690\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 159153, Episodes: 19700\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159183, Episodes: 19710\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159217, Episodes: 19720\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159246, Episodes: 19730\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 159268, Episodes: 19740\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 159297, Episodes: 19750\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 159331, Episodes: 19760\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 159362, Episodes: 19770\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 159393, Episodes: 19780\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 159423, Episodes: 19790\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 159454, Episodes: 19800\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 159486, Episodes: 19810\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 159520, Episodes: 19820\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 159554, Episodes: 19830\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 159583, Episodes: 19840\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159612, Episodes: 19850\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159638, Episodes: 19860\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 159664, Episodes: 19870\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 159688, Episodes: 19880\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 159714, Episodes: 19890\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 159742, Episodes: 19900\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 159778, Episodes: 19910\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 159804, Episodes: 19920\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 159828, Episodes: 19930\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159864, Episodes: 19940\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 159890, Episodes: 19950\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 159921, Episodes: 19960\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 159946, Episodes: 19970\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 159971, Episodes: 19980\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 159996, Episodes: 19990\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 160025, Episodes: 20000\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 160054, Episodes: 20010\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 160087, Episodes: 20020\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 160114, Episodes: 20030\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 160138, Episodes: 20040\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 160172, Episodes: 20050\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 160197, Episodes: 20060\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 160225, Episodes: 20070\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 160258, Episodes: 20080\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 160288, Episodes: 20090\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 160318, Episodes: 20100\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 160346, Episodes: 20110\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 160388, Episodes: 20120\n",
            "Mean reward: -3.20, Mean length: 4.20\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 160412, Episodes: 20130\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 160441, Episodes: 20140\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 160474, Episodes: 20150\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 160506, Episodes: 20160\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 160537, Episodes: 20170\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 160566, Episodes: 20180\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 160601, Episodes: 20190\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 160630, Episodes: 20200\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 160664, Episodes: 20210\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 160697, Episodes: 20220\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 160727, Episodes: 20230\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 160761, Episodes: 20240\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 160789, Episodes: 20250\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 160819, Episodes: 20260\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 160848, Episodes: 20270\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 160877, Episodes: 20280\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 160917, Episodes: 20290\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 160943, Episodes: 20300\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 160971, Episodes: 20310\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 161007, Episodes: 20320\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 161038, Episodes: 20330\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 161067, Episodes: 20340\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 161097, Episodes: 20350\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 161133, Episodes: 20360\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 161157, Episodes: 20370\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 161194, Episodes: 20380\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 161226, Episodes: 20390\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 161254, Episodes: 20400\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 161290, Episodes: 20410\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 161320, Episodes: 20420\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 161352, Episodes: 20430\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 161385, Episodes: 20440\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 161411, Episodes: 20450\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 161449, Episodes: 20460\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 161482, Episodes: 20470\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 161513, Episodes: 20480\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 161544, Episodes: 20490\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 161571, Episodes: 20500\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 161598, Episodes: 20510\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 161628, Episodes: 20520\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 161662, Episodes: 20530\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 161690, Episodes: 20540\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 161716, Episodes: 20550\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 161746, Episodes: 20560\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 161776, Episodes: 20570\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 161803, Episodes: 20580\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 161839, Episodes: 20590\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 161866, Episodes: 20600\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 161898, Episodes: 20610\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 161925, Episodes: 20620\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 161954, Episodes: 20630\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 161982, Episodes: 20640\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 162010, Episodes: 20650\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 162037, Episodes: 20660\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 162066, Episodes: 20670\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 162093, Episodes: 20680\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 162121, Episodes: 20690\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 162145, Episodes: 20700\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 162171, Episodes: 20710\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 162209, Episodes: 20720\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 162244, Episodes: 20730\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 162274, Episodes: 20740\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 162307, Episodes: 20750\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 162331, Episodes: 20760\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 162362, Episodes: 20770\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 162395, Episodes: 20780\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 162422, Episodes: 20790\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 162453, Episodes: 20800\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 162482, Episodes: 20810\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 162519, Episodes: 20820\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 162547, Episodes: 20830\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 162573, Episodes: 20840\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 162610, Episodes: 20850\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 162638, Episodes: 20860\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 162663, Episodes: 20870\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 162700, Episodes: 20880\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 162732, Episodes: 20890\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 162768, Episodes: 20900\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 162802, Episodes: 20910\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 162832, Episodes: 20920\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 162858, Episodes: 20930\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 162892, Episodes: 20940\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 162924, Episodes: 20950\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 162954, Episodes: 20960\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 162984, Episodes: 20970\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 163011, Episodes: 20980\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 163044, Episodes: 20990\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 163074, Episodes: 21000\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 163108, Episodes: 21010\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 163137, Episodes: 21020\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 163171, Episodes: 21030\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 163199, Episodes: 21040\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 163228, Episodes: 21050\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00049, Exploration rate: 0.050\n",
            "Timestep: 163260, Episodes: 21060\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 163293, Episodes: 21070\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 163318, Episodes: 21080\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 163344, Episodes: 21090\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 163366, Episodes: 21100\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 163394, Episodes: 21110\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 163429, Episodes: 21120\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 163456, Episodes: 21130\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 163484, Episodes: 21140\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 163512, Episodes: 21150\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 163540, Episodes: 21160\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 163572, Episodes: 21170\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 163599, Episodes: 21180\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 163632, Episodes: 21190\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 163661, Episodes: 21200\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 163689, Episodes: 21210\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 163717, Episodes: 21220\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 163742, Episodes: 21230\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 163761, Episodes: 21240\n",
            "Mean reward: -0.90, Mean length: 1.90\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 163794, Episodes: 21250\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 163825, Episodes: 21260\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 163856, Episodes: 21270\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 163886, Episodes: 21280\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 163910, Episodes: 21290\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 163941, Episodes: 21300\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 163970, Episodes: 21310\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 163995, Episodes: 21320\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 164033, Episodes: 21330\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 164065, Episodes: 21340\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 164093, Episodes: 21350\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 164119, Episodes: 21360\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 164150, Episodes: 21370\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 164183, Episodes: 21380\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 164212, Episodes: 21390\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 164238, Episodes: 21400\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 164284, Episodes: 21410\n",
            "Mean reward: -3.60, Mean length: 4.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 164318, Episodes: 21420\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 164342, Episodes: 21430\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 164378, Episodes: 21440\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00086, Exploration rate: 0.050\n",
            "Timestep: 164407, Episodes: 21450\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 164436, Episodes: 21460\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 164472, Episodes: 21470\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 164501, Episodes: 21480\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 164530, Episodes: 21490\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00071, Exploration rate: 0.050\n",
            "Timestep: 164565, Episodes: 21500\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 164591, Episodes: 21510\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 164622, Episodes: 21520\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 164646, Episodes: 21530\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 164673, Episodes: 21540\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 164706, Episodes: 21550\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 164733, Episodes: 21560\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 164762, Episodes: 21570\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 164791, Episodes: 21580\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 164821, Episodes: 21590\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 164850, Episodes: 21600\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 164875, Episodes: 21610\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 164904, Episodes: 21620\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 164938, Episodes: 21630\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 164963, Episodes: 21640\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 164990, Episodes: 21650\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 165020, Episodes: 21660\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 165047, Episodes: 21670\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 165084, Episodes: 21680\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 165111, Episodes: 21690\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 165147, Episodes: 21700\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 165178, Episodes: 21710\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 165208, Episodes: 21720\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 165237, Episodes: 21730\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 165269, Episodes: 21740\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 165295, Episodes: 21750\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 165325, Episodes: 21760\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 165355, Episodes: 21770\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 165386, Episodes: 21780\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 165411, Episodes: 21790\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 165441, Episodes: 21800\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00086, Exploration rate: 0.050\n",
            "Timestep: 165471, Episodes: 21810\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 165502, Episodes: 21820\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00085, Exploration rate: 0.050\n",
            "Timestep: 165527, Episodes: 21830\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00085, Exploration rate: 0.050\n",
            "Timestep: 165558, Episodes: 21840\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 165585, Episodes: 21850\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 165611, Episodes: 21860\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 165640, Episodes: 21870\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 165666, Episodes: 21880\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 165697, Episodes: 21890\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 165725, Episodes: 21900\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 165757, Episodes: 21910\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 165781, Episodes: 21920\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 165812, Episodes: 21930\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00083, Exploration rate: 0.050\n",
            "Timestep: 165844, Episodes: 21940\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00086, Exploration rate: 0.050\n",
            "Timestep: 165873, Episodes: 21950\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00086, Exploration rate: 0.050\n",
            "Timestep: 165902, Episodes: 21960\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 165938, Episodes: 21970\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 165968, Episodes: 21980\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00093, Exploration rate: 0.050\n",
            "Timestep: 166015, Episodes: 21990\n",
            "Mean reward: -3.70, Mean length: 4.70\n",
            "Mean loss: 0.00092, Exploration rate: 0.050\n",
            "Timestep: 166040, Episodes: 22000\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00094, Exploration rate: 0.050\n",
            "Timestep: 166070, Episodes: 22010\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00097, Exploration rate: 0.050\n",
            "Timestep: 166099, Episodes: 22020\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00099, Exploration rate: 0.050\n",
            "Timestep: 166123, Episodes: 22030\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 166153, Episodes: 22040\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 166181, Episodes: 22050\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 166206, Episodes: 22060\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 166230, Episodes: 22070\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 166263, Episodes: 22080\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 166291, Episodes: 22090\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 166317, Episodes: 22100\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 166348, Episodes: 22110\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 166376, Episodes: 22120\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 166401, Episodes: 22130\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 166429, Episodes: 22140\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 166457, Episodes: 22150\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 166487, Episodes: 22160\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 166511, Episodes: 22170\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 166544, Episodes: 22180\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 166569, Episodes: 22190\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 166597, Episodes: 22200\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 166629, Episodes: 22210\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 166659, Episodes: 22220\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 166690, Episodes: 22230\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 166720, Episodes: 22240\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 166756, Episodes: 22250\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 166785, Episodes: 22260\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 166819, Episodes: 22270\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00092, Exploration rate: 0.050\n",
            "Timestep: 166849, Episodes: 22280\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 166879, Episodes: 22290\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 166905, Episodes: 22300\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00095, Exploration rate: 0.050\n",
            "Timestep: 166937, Episodes: 22310\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00094, Exploration rate: 0.050\n",
            "Timestep: 166966, Episodes: 22320\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 166990, Episodes: 22330\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00092, Exploration rate: 0.050\n",
            "Timestep: 167021, Episodes: 22340\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 167051, Episodes: 22350\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 167081, Episodes: 22360\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 167117, Episodes: 22370\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 167152, Episodes: 22380\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 167185, Episodes: 22390\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 167214, Episodes: 22400\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 167244, Episodes: 22410\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00108, Exploration rate: 0.050\n",
            "Timestep: 167274, Episodes: 22420\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 167301, Episodes: 22430\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 167334, Episodes: 22440\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 167360, Episodes: 22450\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 167388, Episodes: 22460\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 167422, Episodes: 22470\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 167448, Episodes: 22480\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 167476, Episodes: 22490\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 167510, Episodes: 22500\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 167537, Episodes: 22510\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 167565, Episodes: 22520\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 167603, Episodes: 22530\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 167635, Episodes: 22540\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 167662, Episodes: 22550\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 167688, Episodes: 22560\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 167719, Episodes: 22570\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 167748, Episodes: 22580\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 167773, Episodes: 22590\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 167797, Episodes: 22600\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 167827, Episodes: 22610\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00088, Exploration rate: 0.050\n",
            "Timestep: 167860, Episodes: 22620\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 167886, Episodes: 22630\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 167923, Episodes: 22640\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 167948, Episodes: 22650\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 167980, Episodes: 22660\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00075, Exploration rate: 0.050\n",
            "Timestep: 168011, Episodes: 22670\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 168044, Episodes: 22680\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00080, Exploration rate: 0.050\n",
            "Timestep: 168073, Episodes: 22690\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00084, Exploration rate: 0.050\n",
            "Timestep: 168100, Episodes: 22700\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00085, Exploration rate: 0.050\n",
            "Timestep: 168130, Episodes: 22710\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 168158, Episodes: 22720\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 168182, Episodes: 22730\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 168212, Episodes: 22740\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 168243, Episodes: 22750\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 168273, Episodes: 22760\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 168301, Episodes: 22770\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 168328, Episodes: 22780\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 168368, Episodes: 22790\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 168399, Episodes: 22800\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 168429, Episodes: 22810\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 168461, Episodes: 22820\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00088, Exploration rate: 0.050\n",
            "Timestep: 168485, Episodes: 22830\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00085, Exploration rate: 0.050\n",
            "Timestep: 168514, Episodes: 22840\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00088, Exploration rate: 0.050\n",
            "Timestep: 168541, Episodes: 22850\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 168568, Episodes: 22860\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 168597, Episodes: 22870\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 168634, Episodes: 22880\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 168667, Episodes: 22890\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 168698, Episodes: 22900\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 168726, Episodes: 22910\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 168757, Episodes: 22920\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 168784, Episodes: 22930\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 168810, Episodes: 22940\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 168840, Episodes: 22950\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 168869, Episodes: 22960\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 168903, Episodes: 22970\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 168931, Episodes: 22980\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 168964, Episodes: 22990\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 168992, Episodes: 23000\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 169018, Episodes: 23010\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 169049, Episodes: 23020\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 169074, Episodes: 23030\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 169107, Episodes: 23040\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 169141, Episodes: 23050\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 169165, Episodes: 23060\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 169188, Episodes: 23070\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00086, Exploration rate: 0.050\n",
            "Timestep: 169216, Episodes: 23080\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00089, Exploration rate: 0.050\n",
            "Timestep: 169243, Episodes: 23090\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00088, Exploration rate: 0.050\n",
            "Timestep: 169274, Episodes: 23100\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 169312, Episodes: 23110\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00106, Exploration rate: 0.050\n",
            "Timestep: 169348, Episodes: 23120\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 169378, Episodes: 23130\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00072, Exploration rate: 0.050\n",
            "Timestep: 169407, Episodes: 23140\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 169435, Episodes: 23150\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 169463, Episodes: 23160\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 169495, Episodes: 23170\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 169526, Episodes: 23180\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 169554, Episodes: 23190\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 169582, Episodes: 23200\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 169621, Episodes: 23210\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 169651, Episodes: 23220\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 169682, Episodes: 23230\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 169711, Episodes: 23240\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 169741, Episodes: 23250\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 169769, Episodes: 23260\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 169801, Episodes: 23270\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 169828, Episodes: 23280\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 169858, Episodes: 23290\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 169885, Episodes: 23300\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 169911, Episodes: 23310\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 169940, Episodes: 23320\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 169978, Episodes: 23330\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 170009, Episodes: 23340\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 170039, Episodes: 23350\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 170116, Episodes: 23360\n",
            "Mean reward: -6.80, Mean length: 7.70\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 170145, Episodes: 23370\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00121, Exploration rate: 0.050\n",
            "Timestep: 170175, Episodes: 23380\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00123, Exploration rate: 0.050\n",
            "Timestep: 170206, Episodes: 23390\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00133, Exploration rate: 0.050\n",
            "Timestep: 170235, Episodes: 23400\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00133, Exploration rate: 0.050\n",
            "Timestep: 170311, Episodes: 23410\n",
            "Mean reward: -6.70, Mean length: 7.60\n",
            "Mean loss: 0.00099, Exploration rate: 0.050\n",
            "Timestep: 170342, Episodes: 23420\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 170375, Episodes: 23430\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00077, Exploration rate: 0.050\n",
            "Timestep: 170407, Episodes: 23440\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 170438, Episodes: 23450\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 170469, Episodes: 23460\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00068, Exploration rate: 0.050\n",
            "Timestep: 170499, Episodes: 23470\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 170528, Episodes: 23480\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 170553, Episodes: 23490\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00107, Exploration rate: 0.050\n",
            "Timestep: 170583, Episodes: 23500\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00143, Exploration rate: 0.050\n",
            "Timestep: 170616, Episodes: 23510\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00150, Exploration rate: 0.050\n",
            "Timestep: 170640, Episodes: 23520\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00156, Exploration rate: 0.050\n",
            "Timestep: 170669, Episodes: 23530\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00177, Exploration rate: 0.050\n",
            "Timestep: 170697, Episodes: 23540\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00145, Exploration rate: 0.050\n",
            "Timestep: 170723, Episodes: 23550\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00142, Exploration rate: 0.050\n",
            "Timestep: 170754, Episodes: 23560\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00110, Exploration rate: 0.050\n",
            "Timestep: 170784, Episodes: 23570\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00097, Exploration rate: 0.050\n",
            "Timestep: 170827, Episodes: 23580\n",
            "Mean reward: -3.30, Mean length: 4.30\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 170854, Episodes: 23590\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 170884, Episodes: 23600\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00066, Exploration rate: 0.050\n",
            "Timestep: 170959, Episodes: 23610\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 170988, Episodes: 23620\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 171060, Episodes: 23630\n",
            "Mean reward: -6.30, Mean length: 7.20\n",
            "Mean loss: 0.00135, Exploration rate: 0.050\n",
            "Timestep: 171092, Episodes: 23640\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00135, Exploration rate: 0.050\n",
            "Timestep: 171121, Episodes: 23650\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00132, Exploration rate: 0.050\n",
            "Timestep: 171146, Episodes: 23660\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00092, Exploration rate: 0.050\n",
            "Timestep: 171173, Episodes: 23670\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00133, Exploration rate: 0.050\n",
            "Timestep: 171200, Episodes: 23680\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00129, Exploration rate: 0.050\n",
            "Timestep: 171225, Episodes: 23690\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00100, Exploration rate: 0.050\n",
            "Timestep: 171249, Episodes: 23700\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00100, Exploration rate: 0.050\n",
            "Timestep: 171276, Episodes: 23710\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 171308, Episodes: 23720\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 171335, Episodes: 23730\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 171363, Episodes: 23740\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 171399, Episodes: 23750\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 171426, Episodes: 23760\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 171460, Episodes: 23770\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00051, Exploration rate: 0.050\n",
            "Timestep: 171490, Episodes: 23780\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00081, Exploration rate: 0.050\n",
            "Timestep: 171518, Episodes: 23790\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00082, Exploration rate: 0.050\n",
            "Timestep: 171550, Episodes: 23800\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00094, Exploration rate: 0.050\n",
            "Timestep: 171588, Episodes: 23810\n",
            "Mean reward: -2.80, Mean length: 3.80\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 171616, Episodes: 23820\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00091, Exploration rate: 0.050\n",
            "Timestep: 171649, Episodes: 23830\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 171675, Episodes: 23840\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 171704, Episodes: 23850\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 171728, Episodes: 23860\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00053, Exploration rate: 0.050\n",
            "Timestep: 171754, Episodes: 23870\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 171778, Episodes: 23880\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00056, Exploration rate: 0.050\n",
            "Timestep: 171803, Episodes: 23890\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 171835, Episodes: 23900\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 171869, Episodes: 23910\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00059, Exploration rate: 0.050\n",
            "Timestep: 171893, Episodes: 23920\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00060, Exploration rate: 0.050\n",
            "Timestep: 171933, Episodes: 23930\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 171972, Episodes: 23940\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00057, Exploration rate: 0.050\n",
            "Timestep: 171999, Episodes: 23950\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 172031, Episodes: 23960\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00064, Exploration rate: 0.050\n",
            "Timestep: 172063, Episodes: 23970\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 172090, Episodes: 23980\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00063, Exploration rate: 0.050\n",
            "Timestep: 172116, Episodes: 23990\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 172147, Episodes: 24000\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 172175, Episodes: 24010\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00115, Exploration rate: 0.050\n",
            "Timestep: 172195, Episodes: 24020\n",
            "Mean reward: -1.00, Mean length: 2.00\n",
            "Mean loss: 0.00124, Exploration rate: 0.050\n",
            "Timestep: 172227, Episodes: 24030\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00157, Exploration rate: 0.050\n",
            "Timestep: 172253, Episodes: 24040\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00159, Exploration rate: 0.050\n",
            "Timestep: 172281, Episodes: 24050\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00161, Exploration rate: 0.050\n",
            "Timestep: 172356, Episodes: 24060\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00132, Exploration rate: 0.050\n",
            "Timestep: 172379, Episodes: 24070\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00102, Exploration rate: 0.050\n",
            "Timestep: 172411, Episodes: 24080\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00108, Exploration rate: 0.050\n",
            "Timestep: 172438, Episodes: 24090\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00107, Exploration rate: 0.050\n",
            "Timestep: 172468, Episodes: 24100\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 172503, Episodes: 24110\n",
            "Mean reward: -2.50, Mean length: 3.50\n",
            "Mean loss: 0.00111, Exploration rate: 0.050\n",
            "Timestep: 172539, Episodes: 24120\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 172573, Episodes: 24130\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 172603, Episodes: 24140\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 172628, Episodes: 24150\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 172659, Episodes: 24160\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 172688, Episodes: 24170\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00065, Exploration rate: 0.050\n",
            "Timestep: 172713, Episodes: 24180\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 172738, Episodes: 24190\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00103, Exploration rate: 0.050\n",
            "Timestep: 172763, Episodes: 24200\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00099, Exploration rate: 0.050\n",
            "Timestep: 172784, Episodes: 24210\n",
            "Mean reward: -1.10, Mean length: 2.10\n",
            "Mean loss: 0.00098, Exploration rate: 0.050\n",
            "Timestep: 172815, Episodes: 24220\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00095, Exploration rate: 0.050\n",
            "Timestep: 172844, Episodes: 24230\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 172880, Episodes: 24240\n",
            "Mean reward: -2.60, Mean length: 3.60\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 172906, Episodes: 24250\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00050, Exploration rate: 0.050\n",
            "Timestep: 172930, Episodes: 24260\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 172957, Episodes: 24270\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00055, Exploration rate: 0.050\n",
            "Timestep: 172987, Episodes: 24280\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00052, Exploration rate: 0.050\n",
            "Timestep: 173021, Episodes: 24290\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00061, Exploration rate: 0.050\n",
            "Timestep: 173049, Episodes: 24300\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00067, Exploration rate: 0.050\n",
            "Timestep: 173079, Episodes: 24310\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00099, Exploration rate: 0.050\n",
            "Timestep: 173110, Episodes: 24320\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00099, Exploration rate: 0.050\n",
            "Timestep: 173132, Episodes: 24330\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00098, Exploration rate: 0.050\n",
            "Timestep: 173159, Episodes: 24340\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00095, Exploration rate: 0.050\n",
            "Timestep: 173192, Episodes: 24350\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00087, Exploration rate: 0.050\n",
            "Timestep: 173225, Episodes: 24360\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 173259, Episodes: 24370\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 173286, Episodes: 24380\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00074, Exploration rate: 0.050\n",
            "Timestep: 173310, Episodes: 24390\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00076, Exploration rate: 0.050\n",
            "Timestep: 173337, Episodes: 24400\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00078, Exploration rate: 0.050\n",
            "Timestep: 173365, Episodes: 24410\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00121, Exploration rate: 0.050\n",
            "Timestep: 173391, Episodes: 24420\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00103, Exploration rate: 0.050\n",
            "Timestep: 173424, Episodes: 24430\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 173451, Episodes: 24440\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00102, Exploration rate: 0.050\n",
            "Timestep: 173472, Episodes: 24450\n",
            "Mean reward: -1.10, Mean length: 2.10\n",
            "Mean loss: 0.00102, Exploration rate: 0.050\n",
            "Timestep: 173494, Episodes: 24460\n",
            "Mean reward: -1.20, Mean length: 2.20\n",
            "Mean loss: 0.00100, Exploration rate: 0.050\n",
            "Timestep: 173533, Episodes: 24470\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00054, Exploration rate: 0.050\n",
            "Timestep: 173604, Episodes: 24480\n",
            "Mean reward: -6.20, Mean length: 7.10\n",
            "Mean loss: 0.00153, Exploration rate: 0.050\n",
            "Timestep: 173629, Episodes: 24490\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00169, Exploration rate: 0.050\n",
            "Timestep: 173703, Episodes: 24500\n",
            "Mean reward: -6.50, Mean length: 7.40\n",
            "Mean loss: 0.00177, Exploration rate: 0.050\n",
            "Timestep: 173733, Episodes: 24510\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00102, Exploration rate: 0.050\n",
            "Timestep: 173764, Episodes: 24520\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00101, Exploration rate: 0.050\n",
            "Timestep: 173792, Episodes: 24530\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00089, Exploration rate: 0.050\n",
            "Timestep: 173822, Episodes: 24540\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00092, Exploration rate: 0.050\n",
            "Timestep: 173859, Episodes: 24550\n",
            "Mean reward: -2.70, Mean length: 3.70\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 173887, Episodes: 24560\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00073, Exploration rate: 0.050\n",
            "Timestep: 173918, Episodes: 24570\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 173949, Episodes: 24580\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 173976, Episodes: 24590\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 174001, Episodes: 24600\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00069, Exploration rate: 0.050\n",
            "Timestep: 174124, Episodes: 24610\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 174154, Episodes: 24620\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00062, Exploration rate: 0.050\n",
            "Timestep: 174179, Episodes: 24630\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00058, Exploration rate: 0.050\n",
            "Timestep: 174206, Episodes: 24640\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00108, Exploration rate: 0.050\n",
            "Timestep: 174245, Episodes: 24650\n",
            "Mean reward: -2.90, Mean length: 3.90\n",
            "Mean loss: 0.00110, Exploration rate: 0.050\n",
            "Timestep: 174323, Episodes: 24660\n",
            "Mean reward: -6.90, Mean length: 7.80\n",
            "Mean loss: 0.00201, Exploration rate: 0.050\n",
            "Timestep: 174399, Episodes: 24670\n",
            "Mean reward: -6.70, Mean length: 7.60\n",
            "Mean loss: 0.00165, Exploration rate: 0.050\n",
            "Timestep: 174432, Episodes: 24680\n",
            "Mean reward: -2.30, Mean length: 3.30\n",
            "Mean loss: 0.00160, Exploration rate: 0.050\n",
            "Timestep: 174460, Episodes: 24690\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00161, Exploration rate: 0.050\n",
            "Timestep: 174535, Episodes: 24700\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00070, Exploration rate: 0.050\n",
            "Timestep: 174613, Episodes: 24710\n",
            "Mean reward: -6.90, Mean length: 7.80\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 174696, Episodes: 24720\n",
            "Mean reward: -7.40, Mean length: 8.30\n",
            "Mean loss: 0.00079, Exploration rate: 0.050\n",
            "Timestep: 174821, Episodes: 24730\n",
            "Mean reward: -11.70, Mean length: 12.50\n",
            "Mean loss: 0.00118, Exploration rate: 0.050\n",
            "Timestep: 174904, Episodes: 24740\n",
            "Mean reward: -7.40, Mean length: 8.30\n",
            "Mean loss: 0.00123, Exploration rate: 0.050\n",
            "Timestep: 174934, Episodes: 24750\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00116, Exploration rate: 0.050\n",
            "Timestep: 174961, Episodes: 24760\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00146, Exploration rate: 0.050\n",
            "Timestep: 174990, Episodes: 24770\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00145, Exploration rate: 0.050\n",
            "Timestep: 175069, Episodes: 24780\n",
            "Mean reward: -7.00, Mean length: 7.90\n",
            "Mean loss: 0.00104, Exploration rate: 0.050\n",
            "Timestep: 175191, Episodes: 24790\n",
            "Mean reward: -11.40, Mean length: 12.20\n",
            "Mean loss: 0.00090, Exploration rate: 0.050\n",
            "Timestep: 175265, Episodes: 24800\n",
            "Mean reward: -6.50, Mean length: 7.40\n",
            "Mean loss: 0.00158, Exploration rate: 0.050\n",
            "Timestep: 175293, Episodes: 24810\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00169, Exploration rate: 0.050\n",
            "Timestep: 175415, Episodes: 24820\n",
            "Mean reward: -11.40, Mean length: 12.20\n",
            "Mean loss: 0.00124, Exploration rate: 0.050\n",
            "Timestep: 175443, Episodes: 24830\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00164, Exploration rate: 0.050\n",
            "Timestep: 175473, Episodes: 24840\n",
            "Mean reward: -2.00, Mean length: 3.00\n",
            "Mean loss: 0.00199, Exploration rate: 0.050\n",
            "Timestep: 175498, Episodes: 24850\n",
            "Mean reward: -1.50, Mean length: 2.50\n",
            "Mean loss: 0.00199, Exploration rate: 0.050\n",
            "Timestep: 175573, Episodes: 24860\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00186, Exploration rate: 0.050\n",
            "Timestep: 175604, Episodes: 24870\n",
            "Mean reward: -2.10, Mean length: 3.10\n",
            "Mean loss: 0.00192, Exploration rate: 0.050\n",
            "Timestep: 175628, Episodes: 24880\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00152, Exploration rate: 0.050\n",
            "Timestep: 175657, Episodes: 24890\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00157, Exploration rate: 0.050\n",
            "Timestep: 175684, Episodes: 24900\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00167, Exploration rate: 0.050\n",
            "Timestep: 175759, Episodes: 24910\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 175785, Episodes: 24920\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00125, Exploration rate: 0.050\n",
            "Timestep: 175808, Episodes: 24930\n",
            "Mean reward: -1.30, Mean length: 2.30\n",
            "Mean loss: 0.00119, Exploration rate: 0.050\n",
            "Timestep: 175840, Episodes: 24940\n",
            "Mean reward: -2.20, Mean length: 3.20\n",
            "Mean loss: 0.00105, Exploration rate: 0.050\n",
            "Timestep: 175874, Episodes: 24950\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00113, Exploration rate: 0.050\n",
            "Timestep: 175902, Episodes: 24960\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00122, Exploration rate: 0.050\n",
            "Timestep: 175973, Episodes: 24970\n",
            "Mean reward: -6.20, Mean length: 7.10\n",
            "Mean loss: 0.00121, Exploration rate: 0.050\n",
            "Timestep: 176002, Episodes: 24980\n",
            "Mean reward: -1.90, Mean length: 2.90\n",
            "Mean loss: 0.00134, Exploration rate: 0.050\n",
            "Timestep: 176077, Episodes: 24990\n",
            "Mean reward: -6.60, Mean length: 7.50\n",
            "Mean loss: 0.00133, Exploration rate: 0.050\n",
            "Timestep: 176101, Episodes: 25000\n",
            "Mean reward: -1.40, Mean length: 2.40\n",
            "Mean loss: 0.00180, Exploration rate: 0.050\n",
            "Timestep: 176127, Episodes: 25010\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00142, Exploration rate: 0.050\n",
            "Timestep: 176247, Episodes: 25020\n",
            "Mean reward: -11.20, Mean length: 12.00\n",
            "Mean loss: 0.00225, Exploration rate: 0.050\n",
            "Timestep: 176324, Episodes: 25030\n",
            "Mean reward: -6.80, Mean length: 7.70\n",
            "Mean loss: 0.00182, Exploration rate: 0.050\n",
            "Timestep: 176499, Episodes: 25040\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.00193, Exploration rate: 0.050\n",
            "Timestep: 176581, Episodes: 25050\n",
            "Mean reward: -7.30, Mean length: 8.20\n",
            "Mean loss: 0.00285, Exploration rate: 0.050\n",
            "Timestep: 176608, Episodes: 25060\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00259, Exploration rate: 0.050\n",
            "Timestep: 176734, Episodes: 25070\n",
            "Mean reward: -11.80, Mean length: 12.60\n",
            "Mean loss: 0.00188, Exploration rate: 0.050\n",
            "Timestep: 176768, Episodes: 25080\n",
            "Mean reward: -2.40, Mean length: 3.40\n",
            "Mean loss: 0.00096, Exploration rate: 0.050\n",
            "Timestep: 176850, Episodes: 25090\n",
            "Mean reward: -7.30, Mean length: 8.20\n",
            "Mean loss: 0.00113, Exploration rate: 0.050\n",
            "Timestep: 176877, Episodes: 25100\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00112, Exploration rate: 0.050\n",
            "Timestep: 176905, Episodes: 25110\n",
            "Mean reward: -1.80, Mean length: 2.80\n",
            "Mean loss: 0.00109, Exploration rate: 0.050\n",
            "Timestep: 176974, Episodes: 25120\n",
            "Mean reward: -6.00, Mean length: 6.90\n",
            "Mean loss: 0.00140, Exploration rate: 0.050\n",
            "Timestep: 177143, Episodes: 25130\n",
            "Mean reward: -16.20, Mean length: 16.90\n",
            "Mean loss: 0.00207, Exploration rate: 0.050\n",
            "Timestep: 177266, Episodes: 25140\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.00163, Exploration rate: 0.050\n",
            "Timestep: 177340, Episodes: 25150\n",
            "Mean reward: -6.50, Mean length: 7.40\n",
            "Mean loss: 0.00138, Exploration rate: 0.050\n",
            "Timestep: 177412, Episodes: 25160\n",
            "Mean reward: -6.30, Mean length: 7.20\n",
            "Mean loss: 0.00221, Exploration rate: 0.050\n",
            "Timestep: 177583, Episodes: 25170\n",
            "Mean reward: -16.40, Mean length: 17.10\n",
            "Mean loss: 0.00148, Exploration rate: 0.050\n",
            "Timestep: 177659, Episodes: 25180\n",
            "Mean reward: -6.70, Mean length: 7.60\n",
            "Mean loss: 0.00210, Exploration rate: 0.050\n",
            "Timestep: 177685, Episodes: 25190\n",
            "Mean reward: -1.60, Mean length: 2.60\n",
            "Mean loss: 0.00256, Exploration rate: 0.050\n",
            "Timestep: 177804, Episodes: 25200\n",
            "Mean reward: -11.10, Mean length: 11.90\n",
            "Mean loss: 0.00260, Exploration rate: 0.050\n",
            "Timestep: 178070, Episodes: 25210\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.00179, Exploration rate: 0.050\n",
            "Timestep: 178252, Episodes: 25220\n",
            "Mean reward: -17.50, Mean length: 18.20\n",
            "Mean loss: 0.00197, Exploration rate: 0.050\n",
            "Timestep: 178375, Episodes: 25230\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.00234, Exploration rate: 0.050\n",
            "Timestep: 178402, Episodes: 25240\n",
            "Mean reward: -1.70, Mean length: 2.70\n",
            "Mean loss: 0.00244, Exploration rate: 0.050\n",
            "Timestep: 178480, Episodes: 25250\n",
            "Mean reward: -6.90, Mean length: 7.80\n",
            "Mean loss: 0.00223, Exploration rate: 0.050\n",
            "Timestep: 178597, Episodes: 25260\n",
            "Mean reward: -10.90, Mean length: 11.70\n",
            "Mean loss: 0.00233, Exploration rate: 0.050\n",
            "Timestep: 178812, Episodes: 25270\n",
            "Mean reward: -20.90, Mean length: 21.50\n",
            "Mean loss: 0.00248, Exploration rate: 0.050\n",
            "Timestep: 178884, Episodes: 25280\n",
            "Mean reward: -6.30, Mean length: 7.20\n",
            "Mean loss: 0.00240, Exploration rate: 0.050\n",
            "Timestep: 178955, Episodes: 25290\n",
            "Mean reward: -6.20, Mean length: 7.10\n",
            "Mean loss: 0.00276, Exploration rate: 0.050\n",
            "Timestep: 178995, Episodes: 25300\n",
            "Mean reward: -3.00, Mean length: 4.00\n",
            "Mean loss: 0.00295, Exploration rate: 0.050\n",
            "Timestep: 179213, Episodes: 25310\n",
            "Mean reward: -21.20, Mean length: 21.80\n",
            "Mean loss: 0.00387, Exploration rate: 0.050\n",
            "Timestep: 179477, Episodes: 25320\n",
            "Mean reward: -25.90, Mean length: 26.40\n",
            "Mean loss: 0.00427, Exploration rate: 0.050\n",
            "Timestep: 179600, Episodes: 25330\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.00205, Exploration rate: 0.050\n",
            "Timestep: 179818, Episodes: 25340\n",
            "Mean reward: -21.20, Mean length: 21.80\n",
            "Mean loss: 0.00343, Exploration rate: 0.050\n",
            "Timestep: 179939, Episodes: 25350\n",
            "Mean reward: -11.30, Mean length: 12.10\n",
            "Mean loss: 0.00220, Exploration rate: 0.050\n",
            "Timestep: 180106, Episodes: 25360\n",
            "Mean reward: -16.00, Mean length: 16.70\n",
            "Mean loss: 0.00258, Exploration rate: 0.050\n",
            "Timestep: 180465, Episodes: 25370\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.00322, Exploration rate: 0.050\n",
            "Timestep: 180680, Episodes: 25380\n",
            "Mean reward: -20.90, Mean length: 21.50\n",
            "Mean loss: 0.00320, Exploration rate: 0.050\n",
            "Timestep: 180894, Episodes: 25390\n",
            "Mean reward: -20.80, Mean length: 21.40\n",
            "Mean loss: 0.00257, Exploration rate: 0.050\n",
            "Timestep: 181161, Episodes: 25400\n",
            "Mean reward: -26.20, Mean length: 26.70\n",
            "Mean loss: 0.00404, Exploration rate: 0.050\n",
            "Timestep: 181528, Episodes: 25410\n",
            "Mean reward: -36.40, Mean length: 36.70\n",
            "Mean loss: 0.00387, Exploration rate: 0.050\n",
            "Timestep: 181746, Episodes: 25420\n",
            "Mean reward: -21.20, Mean length: 21.80\n",
            "Mean loss: 0.00344, Exploration rate: 0.050\n",
            "Timestep: 182006, Episodes: 25430\n",
            "Mean reward: -25.50, Mean length: 26.00\n",
            "Mean loss: 0.00463, Exploration rate: 0.050\n",
            "Timestep: 182366, Episodes: 25440\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.00409, Exploration rate: 0.050\n",
            "Timestep: 182580, Episodes: 25450\n",
            "Mean reward: -20.80, Mean length: 21.40\n",
            "Mean loss: 0.00442, Exploration rate: 0.050\n",
            "Timestep: 182983, Episodes: 25460\n",
            "Mean reward: -40.10, Mean length: 40.30\n",
            "Mean loss: 0.00463, Exploration rate: 0.050\n",
            "Timestep: 183390, Episodes: 25470\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.00413, Exploration rate: 0.050\n",
            "Timestep: 183795, Episodes: 25480\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.00406, Exploration rate: 0.050\n",
            "Timestep: 184152, Episodes: 25490\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.00508, Exploration rate: 0.050\n",
            "Timestep: 184511, Episodes: 25500\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.00567, Exploration rate: 0.050\n",
            "Timestep: 184867, Episodes: 25510\n",
            "Mean reward: -35.30, Mean length: 35.60\n",
            "Mean loss: 0.00480, Exploration rate: 0.050\n",
            "Timestep: 185319, Episodes: 25520\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.00663, Exploration rate: 0.050\n",
            "Timestep: 185725, Episodes: 25530\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.00548, Exploration rate: 0.050\n",
            "Timestep: 186128, Episodes: 25540\n",
            "Mean reward: -40.10, Mean length: 40.30\n",
            "Mean loss: 0.00601, Exploration rate: 0.050\n",
            "Timestep: 186581, Episodes: 25550\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.00686, Exploration rate: 0.050\n",
            "Timestep: 186988, Episodes: 25560\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.00622, Exploration rate: 0.050\n",
            "Timestep: 187394, Episodes: 25570\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.00674, Exploration rate: 0.050\n",
            "Timestep: 187751, Episodes: 25580\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.00821, Exploration rate: 0.050\n",
            "Timestep: 188157, Episodes: 25590\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.00893, Exploration rate: 0.050\n",
            "Timestep: 188609, Episodes: 25600\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.00841, Exploration rate: 0.050\n",
            "Timestep: 189109, Episodes: 25610\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01264, Exploration rate: 0.050\n",
            "Timestep: 189466, Episodes: 25620\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.01031, Exploration rate: 0.050\n",
            "Timestep: 189966, Episodes: 25630\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01004, Exploration rate: 0.050\n",
            "Timestep: 190322, Episodes: 25640\n",
            "Mean reward: -35.30, Mean length: 35.60\n",
            "Mean loss: 0.01427, Exploration rate: 0.050\n",
            "Timestep: 190682, Episodes: 25650\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.01155, Exploration rate: 0.050\n",
            "Timestep: 191182, Episodes: 25660\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01361, Exploration rate: 0.050\n",
            "Timestep: 191589, Episodes: 25670\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.01355, Exploration rate: 0.050\n",
            "Timestep: 192089, Episodes: 25680\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01505, Exploration rate: 0.050\n",
            "Timestep: 192543, Episodes: 25690\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.01590, Exploration rate: 0.050\n",
            "Timestep: 192996, Episodes: 25700\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.01213, Exploration rate: 0.050\n",
            "Timestep: 193496, Episodes: 25710\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01689, Exploration rate: 0.050\n",
            "Timestep: 193904, Episodes: 25720\n",
            "Mean reward: -40.60, Mean length: 40.80\n",
            "Mean loss: 0.01499, Exploration rate: 0.050\n",
            "Timestep: 194404, Episodes: 25730\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01571, Exploration rate: 0.050\n",
            "Timestep: 194904, Episodes: 25740\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01950, Exploration rate: 0.050\n",
            "Timestep: 195358, Episodes: 25750\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.02420, Exploration rate: 0.050\n",
            "Timestep: 195809, Episodes: 25760\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.02614, Exploration rate: 0.050\n",
            "Timestep: 196309, Episodes: 25770\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02699, Exploration rate: 0.050\n",
            "Timestep: 196809, Episodes: 25780\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02091, Exploration rate: 0.050\n",
            "Timestep: 197274, Episodes: 25790\n",
            "Mean reward: -46.40, Mean length: 46.50\n",
            "Mean loss: 0.02872, Exploration rate: 0.050\n",
            "Timestep: 197737, Episodes: 25800\n",
            "Mean reward: -46.20, Mean length: 46.30\n",
            "Mean loss: 0.02761, Exploration rate: 0.050\n",
            "Timestep: 198202, Episodes: 25810\n",
            "Mean reward: -46.40, Mean length: 46.50\n",
            "Mean loss: 0.03227, Exploration rate: 0.050\n",
            "Timestep: 198604, Episodes: 25820\n",
            "Mean reward: -40.00, Mean length: 40.20\n",
            "Mean loss: 0.02802, Exploration rate: 0.050\n",
            "Timestep: 199055, Episodes: 25830\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.03247, Exploration rate: 0.050\n",
            "Timestep: 199459, Episodes: 25840\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.04010, Exploration rate: 0.050\n",
            "Timestep: 199918, Episodes: 25850\n",
            "Mean reward: -45.80, Mean length: 45.90\n",
            "Mean loss: 0.04092, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_200000.pth\n",
            "Evaluation: Mean reward: -40.80, mean episode length: 41.00\n",
            "Timestep: 200288, Episodes: 25860\n",
            "Mean reward: -36.70, Mean length: 37.00\n",
            "Mean loss: 0.03284, Exploration rate: 0.050\n",
            "Timestep: 200601, Episodes: 25870\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.03382, Exploration rate: 0.050\n",
            "Timestep: 201101, Episodes: 25880\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04414, Exploration rate: 0.050\n",
            "Timestep: 201601, Episodes: 25890\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04332, Exploration rate: 0.050\n",
            "Timestep: 202101, Episodes: 25900\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05021, Exploration rate: 0.050\n",
            "Timestep: 202557, Episodes: 25910\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.04604, Exploration rate: 0.050\n",
            "Timestep: 202961, Episodes: 25920\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.04539, Exploration rate: 0.050\n",
            "Timestep: 203418, Episodes: 25930\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.05481, Exploration rate: 0.050\n",
            "Timestep: 203777, Episodes: 25940\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.05499, Exploration rate: 0.050\n",
            "Timestep: 204246, Episodes: 25950\n",
            "Mean reward: -46.80, Mean length: 46.90\n",
            "Mean loss: 0.05869, Exploration rate: 0.050\n",
            "Timestep: 204706, Episodes: 25960\n",
            "Mean reward: -45.90, Mean length: 46.00\n",
            "Mean loss: 0.05481, Exploration rate: 0.050\n",
            "Timestep: 205159, Episodes: 25970\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.05855, Exploration rate: 0.050\n",
            "Timestep: 205533, Episodes: 25980\n",
            "Mean reward: -37.10, Mean length: 37.40\n",
            "Mean loss: 0.06135, Exploration rate: 0.050\n",
            "Timestep: 205940, Episodes: 25990\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.05250, Exploration rate: 0.050\n",
            "Timestep: 206348, Episodes: 26000\n",
            "Mean reward: -40.60, Mean length: 40.80\n",
            "Mean loss: 0.05803, Exploration rate: 0.050\n",
            "Timestep: 206799, Episodes: 26010\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.05798, Exploration rate: 0.050\n",
            "Timestep: 207204, Episodes: 26020\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.06827, Exploration rate: 0.050\n",
            "Timestep: 207690, Episodes: 26030\n",
            "Mean reward: -48.40, Mean length: 48.60\n",
            "Mean loss: 0.06592, Exploration rate: 0.050\n",
            "Timestep: 208152, Episodes: 26040\n",
            "Mean reward: -46.10, Mean length: 46.20\n",
            "Mean loss: 0.07255, Exploration rate: 0.050\n",
            "Timestep: 208652, Episodes: 26050\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06392, Exploration rate: 0.050\n",
            "Timestep: 209152, Episodes: 26060\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08357, Exploration rate: 0.050\n",
            "Timestep: 209652, Episodes: 26070\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07364, Exploration rate: 0.050\n",
            "Timestep: 210152, Episodes: 26080\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09248, Exploration rate: 0.050\n",
            "Timestep: 210543, Episodes: 26090\n",
            "Mean reward: -38.80, Mean length: 39.10\n",
            "Mean loss: 0.08668, Exploration rate: 0.050\n",
            "Timestep: 210791, Episodes: 26100\n",
            "Mean reward: -24.20, Mean length: 24.80\n",
            "Mean loss: 0.07763, Exploration rate: 0.050\n",
            "Timestep: 211291, Episodes: 26110\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08219, Exploration rate: 0.050\n",
            "Timestep: 211791, Episodes: 26120\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08072, Exploration rate: 0.050\n",
            "Timestep: 212196, Episodes: 26130\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.09422, Exploration rate: 0.050\n",
            "Timestep: 212696, Episodes: 26140\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08088, Exploration rate: 0.050\n",
            "Timestep: 213099, Episodes: 26150\n",
            "Mean reward: -40.10, Mean length: 40.30\n",
            "Mean loss: 0.08719, Exploration rate: 0.050\n",
            "Timestep: 213599, Episodes: 26160\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08190, Exploration rate: 0.050\n",
            "Timestep: 214055, Episodes: 26170\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.08358, Exploration rate: 0.050\n",
            "Timestep: 214506, Episodes: 26180\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.07852, Exploration rate: 0.050\n",
            "Timestep: 215006, Episodes: 26190\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07201, Exploration rate: 0.050\n",
            "Timestep: 215412, Episodes: 26200\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.07864, Exploration rate: 0.050\n",
            "Timestep: 215866, Episodes: 26210\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.06723, Exploration rate: 0.050\n",
            "Timestep: 216358, Episodes: 26220\n",
            "Mean reward: -49.10, Mean length: 49.20\n",
            "Mean loss: 0.07828, Exploration rate: 0.050\n",
            "Timestep: 216858, Episodes: 26230\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08010, Exploration rate: 0.050\n",
            "Timestep: 217358, Episodes: 26240\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07466, Exploration rate: 0.050\n",
            "Timestep: 217858, Episodes: 26250\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07435, Exploration rate: 0.050\n",
            "Timestep: 218322, Episodes: 26260\n",
            "Mean reward: -46.30, Mean length: 46.40\n",
            "Mean loss: 0.08071, Exploration rate: 0.050\n",
            "Timestep: 218822, Episodes: 26270\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07651, Exploration rate: 0.050\n",
            "Timestep: 219241, Episodes: 26280\n",
            "Mean reward: -41.70, Mean length: 41.90\n",
            "Mean loss: 0.08131, Exploration rate: 0.050\n",
            "Timestep: 219694, Episodes: 26290\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.06610, Exploration rate: 0.050\n",
            "Timestep: 220101, Episodes: 26300\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.07519, Exploration rate: 0.050\n",
            "Timestep: 220586, Episodes: 26310\n",
            "Mean reward: -48.40, Mean length: 48.50\n",
            "Mean loss: 0.07426, Exploration rate: 0.050\n",
            "Timestep: 221066, Episodes: 26320\n",
            "Mean reward: -47.90, Mean length: 48.00\n",
            "Mean loss: 0.07598, Exploration rate: 0.050\n",
            "Timestep: 221566, Episodes: 26330\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07033, Exploration rate: 0.050\n",
            "Timestep: 222066, Episodes: 26340\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07009, Exploration rate: 0.050\n",
            "Timestep: 222566, Episodes: 26350\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08744, Exploration rate: 0.050\n",
            "Timestep: 223015, Episodes: 26360\n",
            "Mean reward: -44.70, Mean length: 44.90\n",
            "Mean loss: 0.07682, Exploration rate: 0.050\n",
            "Timestep: 223466, Episodes: 26370\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.07385, Exploration rate: 0.050\n",
            "Timestep: 223966, Episodes: 26380\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06375, Exploration rate: 0.050\n",
            "Timestep: 224396, Episodes: 26390\n",
            "Mean reward: -42.80, Mean length: 43.00\n",
            "Mean loss: 0.08328, Exploration rate: 0.050\n",
            "Timestep: 224753, Episodes: 26400\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.06405, Exploration rate: 0.050\n",
            "Timestep: 225209, Episodes: 26410\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.06490, Exploration rate: 0.050\n",
            "Timestep: 225671, Episodes: 26420\n",
            "Mean reward: -46.10, Mean length: 46.20\n",
            "Mean loss: 0.06633, Exploration rate: 0.050\n",
            "Timestep: 226075, Episodes: 26430\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.07623, Exploration rate: 0.050\n",
            "Timestep: 226531, Episodes: 26440\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.08031, Exploration rate: 0.050\n",
            "Timestep: 226984, Episodes: 26450\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.07255, Exploration rate: 0.050\n",
            "Timestep: 227444, Episodes: 26460\n",
            "Mean reward: -45.90, Mean length: 46.00\n",
            "Mean loss: 0.07265, Exploration rate: 0.050\n",
            "Timestep: 227944, Episodes: 26470\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06566, Exploration rate: 0.050\n",
            "Timestep: 228368, Episodes: 26480\n",
            "Mean reward: -42.20, Mean length: 42.40\n",
            "Mean loss: 0.07188, Exploration rate: 0.050\n",
            "Timestep: 228822, Episodes: 26490\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.06190, Exploration rate: 0.050\n",
            "Timestep: 229143, Episodes: 26500\n",
            "Mean reward: -31.60, Mean length: 32.10\n",
            "Mean loss: 0.06983, Exploration rate: 0.050\n",
            "Timestep: 229555, Episodes: 26510\n",
            "Mean reward: -41.00, Mean length: 41.20\n",
            "Mean loss: 0.06450, Exploration rate: 0.050\n",
            "Timestep: 229914, Episodes: 26520\n",
            "Mean reward: -35.50, Mean length: 35.90\n",
            "Mean loss: 0.06832, Exploration rate: 0.050\n",
            "Timestep: 230414, Episodes: 26530\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06909, Exploration rate: 0.050\n",
            "Timestep: 230845, Episodes: 26540\n",
            "Mean reward: -42.90, Mean length: 43.10\n",
            "Mean loss: 0.05617, Exploration rate: 0.050\n",
            "Timestep: 231345, Episodes: 26550\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06790, Exploration rate: 0.050\n",
            "Timestep: 231845, Episodes: 26560\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06238, Exploration rate: 0.050\n",
            "Timestep: 232308, Episodes: 26570\n",
            "Mean reward: -46.20, Mean length: 46.30\n",
            "Mean loss: 0.07142, Exploration rate: 0.050\n",
            "Timestep: 232714, Episodes: 26580\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.06912, Exploration rate: 0.050\n",
            "Timestep: 233065, Episodes: 26590\n",
            "Mean reward: -34.70, Mean length: 35.10\n",
            "Mean loss: 0.06217, Exploration rate: 0.050\n",
            "Timestep: 233523, Episodes: 26600\n",
            "Mean reward: -45.70, Mean length: 45.80\n",
            "Mean loss: 0.05933, Exploration rate: 0.050\n",
            "Timestep: 233927, Episodes: 26610\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.06705, Exploration rate: 0.050\n",
            "Timestep: 234338, Episodes: 26620\n",
            "Mean reward: -40.90, Mean length: 41.10\n",
            "Mean loss: 0.05790, Exploration rate: 0.050\n",
            "Timestep: 234743, Episodes: 26630\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.05509, Exploration rate: 0.050\n",
            "Timestep: 235200, Episodes: 26640\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.06627, Exploration rate: 0.050\n",
            "Timestep: 235514, Episodes: 26650\n",
            "Mean reward: -31.00, Mean length: 31.40\n",
            "Mean loss: 0.06422, Exploration rate: 0.050\n",
            "Timestep: 235924, Episodes: 26660\n",
            "Mean reward: -40.80, Mean length: 41.00\n",
            "Mean loss: 0.05737, Exploration rate: 0.050\n",
            "Timestep: 236424, Episodes: 26670\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05190, Exploration rate: 0.050\n",
            "Timestep: 236745, Episodes: 26680\n",
            "Mean reward: -31.70, Mean length: 32.10\n",
            "Mean loss: 0.04644, Exploration rate: 0.050\n",
            "Timestep: 237207, Episodes: 26690\n",
            "Mean reward: -46.10, Mean length: 46.20\n",
            "Mean loss: 0.06749, Exploration rate: 0.050\n",
            "Timestep: 237609, Episodes: 26700\n",
            "Mean reward: -40.00, Mean length: 40.20\n",
            "Mean loss: 0.06359, Exploration rate: 0.050\n",
            "Timestep: 237858, Episodes: 26710\n",
            "Mean reward: -24.30, Mean length: 24.90\n",
            "Mean loss: 0.05758, Exploration rate: 0.050\n",
            "Timestep: 238309, Episodes: 26720\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.06149, Exploration rate: 0.050\n",
            "Timestep: 238663, Episodes: 26730\n",
            "Mean reward: -35.10, Mean length: 35.40\n",
            "Mean loss: 0.05806, Exploration rate: 0.050\n",
            "Timestep: 238857, Episodes: 26740\n",
            "Mean reward: -18.70, Mean length: 19.40\n",
            "Mean loss: 0.06100, Exploration rate: 0.050\n",
            "Timestep: 239290, Episodes: 26750\n",
            "Mean reward: -43.10, Mean length: 43.30\n",
            "Mean loss: 0.06239, Exploration rate: 0.050\n",
            "Timestep: 239699, Episodes: 26760\n",
            "Mean reward: -40.70, Mean length: 40.90\n",
            "Mean loss: 0.06323, Exploration rate: 0.050\n",
            "Timestep: 240107, Episodes: 26770\n",
            "Mean reward: -40.60, Mean length: 40.80\n",
            "Mean loss: 0.06786, Exploration rate: 0.050\n",
            "Timestep: 240561, Episodes: 26780\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.06016, Exploration rate: 0.050\n",
            "Timestep: 240885, Episodes: 26790\n",
            "Mean reward: -32.00, Mean length: 32.40\n",
            "Mean loss: 0.06391, Exploration rate: 0.050\n",
            "Timestep: 241385, Episodes: 26800\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05274, Exploration rate: 0.050\n",
            "Timestep: 241885, Episodes: 26810\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05019, Exploration rate: 0.050\n",
            "Timestep: 242286, Episodes: 26820\n",
            "Mean reward: -39.80, Mean length: 40.10\n",
            "Mean loss: 0.04695, Exploration rate: 0.050\n",
            "Timestep: 242595, Episodes: 26830\n",
            "Mean reward: -30.40, Mean length: 30.90\n",
            "Mean loss: 0.05057, Exploration rate: 0.050\n",
            "Timestep: 242967, Episodes: 26840\n",
            "Mean reward: -36.90, Mean length: 37.20\n",
            "Mean loss: 0.05030, Exploration rate: 0.050\n",
            "Timestep: 243372, Episodes: 26850\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.05278, Exploration rate: 0.050\n",
            "Timestep: 243854, Episodes: 26860\n",
            "Mean reward: -48.10, Mean length: 48.20\n",
            "Mean loss: 0.05957, Exploration rate: 0.050\n",
            "Timestep: 244236, Episodes: 26870\n",
            "Mean reward: -37.90, Mean length: 38.20\n",
            "Mean loss: 0.05815, Exploration rate: 0.050\n",
            "Timestep: 244511, Episodes: 26880\n",
            "Mean reward: -27.00, Mean length: 27.50\n",
            "Mean loss: 0.05046, Exploration rate: 0.050\n",
            "Timestep: 244876, Episodes: 26890\n",
            "Mean reward: -36.20, Mean length: 36.50\n",
            "Mean loss: 0.05248, Exploration rate: 0.050\n",
            "Timestep: 245285, Episodes: 26900\n",
            "Mean reward: -40.70, Mean length: 40.90\n",
            "Mean loss: 0.05856, Exploration rate: 0.050\n",
            "Timestep: 245706, Episodes: 26910\n",
            "Mean reward: -41.90, Mean length: 42.10\n",
            "Mean loss: 0.05999, Exploration rate: 0.050\n",
            "Timestep: 246158, Episodes: 26920\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.04963, Exploration rate: 0.050\n",
            "Timestep: 246658, Episodes: 26930\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04606, Exploration rate: 0.050\n",
            "Timestep: 247081, Episodes: 26940\n",
            "Mean reward: -42.10, Mean length: 42.30\n",
            "Mean loss: 0.05153, Exploration rate: 0.050\n",
            "Timestep: 247500, Episodes: 26950\n",
            "Mean reward: -41.70, Mean length: 41.90\n",
            "Mean loss: 0.04347, Exploration rate: 0.050\n",
            "Timestep: 247872, Episodes: 26960\n",
            "Mean reward: -36.90, Mean length: 37.20\n",
            "Mean loss: 0.04965, Exploration rate: 0.050\n",
            "Timestep: 248137, Episodes: 26970\n",
            "Mean reward: -25.90, Mean length: 26.50\n",
            "Mean loss: 0.05307, Exploration rate: 0.050\n",
            "Timestep: 248512, Episodes: 26980\n",
            "Mean reward: -37.20, Mean length: 37.50\n",
            "Mean loss: 0.04424, Exploration rate: 0.050\n",
            "Timestep: 249012, Episodes: 26990\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04013, Exploration rate: 0.050\n",
            "Timestep: 249446, Episodes: 27000\n",
            "Mean reward: -43.20, Mean length: 43.40\n",
            "Mean loss: 0.04370, Exploration rate: 0.050\n",
            "Timestep: 249766, Episodes: 27010\n",
            "Mean reward: -31.60, Mean length: 32.00\n",
            "Mean loss: 0.05576, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_250000.pth\n",
            "Evaluation: Mean reward: -36.80, mean episode length: 37.10\n",
            "Timestep: 250128, Episodes: 27020\n",
            "Mean reward: -35.90, Mean length: 36.20\n",
            "Mean loss: 0.04302, Exploration rate: 0.050\n",
            "Timestep: 250464, Episodes: 27030\n",
            "Mean reward: -33.20, Mean length: 33.60\n",
            "Mean loss: 0.04167, Exploration rate: 0.050\n",
            "Timestep: 250809, Episodes: 27040\n",
            "Mean reward: -34.10, Mean length: 34.50\n",
            "Mean loss: 0.04396, Exploration rate: 0.050\n",
            "Timestep: 251172, Episodes: 27050\n",
            "Mean reward: -36.00, Mean length: 36.30\n",
            "Mean loss: 0.04960, Exploration rate: 0.050\n",
            "Timestep: 251578, Episodes: 27060\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.04797, Exploration rate: 0.050\n",
            "Timestep: 252017, Episodes: 27070\n",
            "Mean reward: -43.70, Mean length: 43.90\n",
            "Mean loss: 0.04870, Exploration rate: 0.050\n",
            "Timestep: 252426, Episodes: 27080\n",
            "Mean reward: -40.70, Mean length: 40.90\n",
            "Mean loss: 0.05498, Exploration rate: 0.050\n",
            "Timestep: 252617, Episodes: 27090\n",
            "Mean reward: -18.40, Mean length: 19.10\n",
            "Mean loss: 0.05111, Exploration rate: 0.050\n",
            "Timestep: 253029, Episodes: 27100\n",
            "Mean reward: -41.00, Mean length: 41.20\n",
            "Mean loss: 0.04597, Exploration rate: 0.050\n",
            "Timestep: 253483, Episodes: 27110\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.05019, Exploration rate: 0.050\n",
            "Timestep: 253752, Episodes: 27120\n",
            "Mean reward: -26.40, Mean length: 26.90\n",
            "Mean loss: 0.03944, Exploration rate: 0.050\n",
            "Timestep: 254017, Episodes: 27130\n",
            "Mean reward: -26.00, Mean length: 26.50\n",
            "Mean loss: 0.05439, Exploration rate: 0.050\n",
            "Timestep: 254336, Episodes: 27140\n",
            "Mean reward: -31.50, Mean length: 31.90\n",
            "Mean loss: 0.04956, Exploration rate: 0.050\n",
            "Timestep: 254604, Episodes: 27150\n",
            "Mean reward: -26.30, Mean length: 26.80\n",
            "Mean loss: 0.04660, Exploration rate: 0.050\n",
            "Timestep: 254934, Episodes: 27160\n",
            "Mean reward: -32.60, Mean length: 33.00\n",
            "Mean loss: 0.04585, Exploration rate: 0.050\n",
            "Timestep: 255249, Episodes: 27170\n",
            "Mean reward: -31.00, Mean length: 31.50\n",
            "Mean loss: 0.04161, Exploration rate: 0.050\n",
            "Timestep: 255612, Episodes: 27180\n",
            "Mean reward: -36.00, Mean length: 36.30\n",
            "Mean loss: 0.04791, Exploration rate: 0.050\n",
            "Timestep: 255947, Episodes: 27190\n",
            "Mean reward: -33.10, Mean length: 33.50\n",
            "Mean loss: 0.03899, Exploration rate: 0.050\n",
            "Timestep: 256306, Episodes: 27200\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.04405, Exploration rate: 0.050\n",
            "Timestep: 256758, Episodes: 27210\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.05103, Exploration rate: 0.050\n",
            "Timestep: 257062, Episodes: 27220\n",
            "Mean reward: -29.90, Mean length: 30.40\n",
            "Mean loss: 0.04946, Exploration rate: 0.050\n",
            "Timestep: 257426, Episodes: 27230\n",
            "Mean reward: -36.00, Mean length: 36.40\n",
            "Mean loss: 0.04231, Exploration rate: 0.050\n",
            "Timestep: 257611, Episodes: 27240\n",
            "Mean reward: -17.80, Mean length: 18.50\n",
            "Mean loss: 0.04783, Exploration rate: 0.050\n",
            "Timestep: 257924, Episodes: 27250\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.03913, Exploration rate: 0.050\n",
            "Timestep: 258243, Episodes: 27260\n",
            "Mean reward: -31.50, Mean length: 31.90\n",
            "Mean loss: 0.04153, Exploration rate: 0.050\n",
            "Timestep: 258536, Episodes: 27270\n",
            "Mean reward: -28.80, Mean length: 29.30\n",
            "Mean loss: 0.03041, Exploration rate: 0.050\n",
            "Timestep: 258851, Episodes: 27280\n",
            "Mean reward: -31.10, Mean length: 31.50\n",
            "Mean loss: 0.05116, Exploration rate: 0.050\n",
            "Timestep: 259164, Episodes: 27290\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.04347, Exploration rate: 0.050\n",
            "Timestep: 259482, Episodes: 27300\n",
            "Mean reward: -31.40, Mean length: 31.80\n",
            "Mean loss: 0.04192, Exploration rate: 0.050\n",
            "Timestep: 259889, Episodes: 27310\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.05204, Exploration rate: 0.050\n",
            "Timestep: 260143, Episodes: 27320\n",
            "Mean reward: -24.80, Mean length: 25.40\n",
            "Mean loss: 0.03369, Exploration rate: 0.050\n",
            "Timestep: 260480, Episodes: 27330\n",
            "Mean reward: -33.30, Mean length: 33.70\n",
            "Mean loss: 0.03715, Exploration rate: 0.050\n",
            "Timestep: 260800, Episodes: 27340\n",
            "Mean reward: -31.60, Mean length: 32.00\n",
            "Mean loss: 0.04045, Exploration rate: 0.050\n",
            "Timestep: 261132, Episodes: 27350\n",
            "Mean reward: -32.80, Mean length: 33.20\n",
            "Mean loss: 0.03893, Exploration rate: 0.050\n",
            "Timestep: 261360, Episodes: 27360\n",
            "Mean reward: -22.20, Mean length: 22.80\n",
            "Mean loss: 0.03957, Exploration rate: 0.050\n",
            "Timestep: 261650, Episodes: 27370\n",
            "Mean reward: -28.50, Mean length: 29.00\n",
            "Mean loss: 0.04727, Exploration rate: 0.050\n",
            "Timestep: 261969, Episodes: 27380\n",
            "Mean reward: -31.50, Mean length: 31.90\n",
            "Mean loss: 0.03891, Exploration rate: 0.050\n",
            "Timestep: 262286, Episodes: 27390\n",
            "Mean reward: -31.30, Mean length: 31.70\n",
            "Mean loss: 0.04495, Exploration rate: 0.050\n",
            "Timestep: 262557, Episodes: 27400\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.03291, Exploration rate: 0.050\n",
            "Timestep: 263015, Episodes: 27410\n",
            "Mean reward: -45.70, Mean length: 45.80\n",
            "Mean loss: 0.03498, Exploration rate: 0.050\n",
            "Timestep: 263386, Episodes: 27420\n",
            "Mean reward: -36.80, Mean length: 37.10\n",
            "Mean loss: 0.03862, Exploration rate: 0.050\n",
            "Timestep: 263748, Episodes: 27430\n",
            "Mean reward: -35.90, Mean length: 36.20\n",
            "Mean loss: 0.03758, Exploration rate: 0.050\n",
            "Timestep: 264027, Episodes: 27440\n",
            "Mean reward: -27.40, Mean length: 27.90\n",
            "Mean loss: 0.02927, Exploration rate: 0.050\n",
            "Timestep: 264244, Episodes: 27450\n",
            "Mean reward: -21.10, Mean length: 21.70\n",
            "Mean loss: 0.03655, Exploration rate: 0.050\n",
            "Timestep: 264426, Episodes: 27460\n",
            "Mean reward: -17.50, Mean length: 18.20\n",
            "Mean loss: 0.03390, Exploration rate: 0.050\n",
            "Timestep: 264784, Episodes: 27470\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.03511, Exploration rate: 0.050\n",
            "Timestep: 264928, Episodes: 27480\n",
            "Mean reward: -13.60, Mean length: 14.40\n",
            "Mean loss: 0.04414, Exploration rate: 0.050\n",
            "Timestep: 265155, Episodes: 27490\n",
            "Mean reward: -22.10, Mean length: 22.70\n",
            "Mean loss: 0.03988, Exploration rate: 0.050\n",
            "Timestep: 265514, Episodes: 27500\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.03449, Exploration rate: 0.050\n",
            "Timestep: 265967, Episodes: 27510\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.03959, Exploration rate: 0.050\n",
            "Timestep: 266290, Episodes: 27520\n",
            "Mean reward: -31.90, Mean length: 32.30\n",
            "Mean loss: 0.03837, Exploration rate: 0.050\n",
            "Timestep: 266541, Episodes: 27530\n",
            "Mean reward: -24.50, Mean length: 25.10\n",
            "Mean loss: 0.03405, Exploration rate: 0.050\n",
            "Timestep: 266779, Episodes: 27540\n",
            "Mean reward: -23.20, Mean length: 23.80\n",
            "Mean loss: 0.02841, Exploration rate: 0.050\n",
            "Timestep: 267050, Episodes: 27550\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.02488, Exploration rate: 0.050\n",
            "Timestep: 267340, Episodes: 27560\n",
            "Mean reward: -28.50, Mean length: 29.00\n",
            "Mean loss: 0.04221, Exploration rate: 0.050\n",
            "Timestep: 267611, Episodes: 27570\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.03763, Exploration rate: 0.050\n",
            "Timestep: 267842, Episodes: 27580\n",
            "Mean reward: -22.50, Mean length: 23.10\n",
            "Mean loss: 0.03418, Exploration rate: 0.050\n",
            "Timestep: 268074, Episodes: 27590\n",
            "Mean reward: -22.60, Mean length: 23.20\n",
            "Mean loss: 0.04640, Exploration rate: 0.050\n",
            "Timestep: 268342, Episodes: 27600\n",
            "Mean reward: -26.30, Mean length: 26.80\n",
            "Mean loss: 0.03927, Exploration rate: 0.050\n",
            "Timestep: 268699, Episodes: 27610\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.03650, Exploration rate: 0.050\n",
            "Timestep: 268991, Episodes: 27620\n",
            "Mean reward: -28.70, Mean length: 29.20\n",
            "Mean loss: 0.03742, Exploration rate: 0.050\n",
            "Timestep: 269219, Episodes: 27630\n",
            "Mean reward: -22.10, Mean length: 22.80\n",
            "Mean loss: 0.03461, Exploration rate: 0.050\n",
            "Timestep: 269412, Episodes: 27640\n",
            "Mean reward: -18.60, Mean length: 19.30\n",
            "Mean loss: 0.03074, Exploration rate: 0.050\n",
            "Timestep: 269725, Episodes: 27650\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.03708, Exploration rate: 0.050\n",
            "Timestep: 270113, Episodes: 27660\n",
            "Mean reward: -38.50, Mean length: 38.80\n",
            "Mean loss: 0.02925, Exploration rate: 0.050\n",
            "Timestep: 270390, Episodes: 27670\n",
            "Mean reward: -27.20, Mean length: 27.70\n",
            "Mean loss: 0.03629, Exploration rate: 0.050\n",
            "Timestep: 270656, Episodes: 27680\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.03556, Exploration rate: 0.050\n",
            "Timestep: 270790, Episodes: 27690\n",
            "Mean reward: -12.60, Mean length: 13.40\n",
            "Mean loss: 0.03339, Exploration rate: 0.050\n",
            "Timestep: 271055, Episodes: 27700\n",
            "Mean reward: -26.00, Mean length: 26.50\n",
            "Mean loss: 0.03675, Exploration rate: 0.050\n",
            "Timestep: 271276, Episodes: 27710\n",
            "Mean reward: -21.50, Mean length: 22.10\n",
            "Mean loss: 0.02921, Exploration rate: 0.050\n",
            "Timestep: 271634, Episodes: 27720\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.03688, Exploration rate: 0.050\n",
            "Timestep: 271949, Episodes: 27730\n",
            "Mean reward: -31.10, Mean length: 31.50\n",
            "Mean loss: 0.03052, Exploration rate: 0.050\n",
            "Timestep: 272262, Episodes: 27740\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.03507, Exploration rate: 0.050\n",
            "Timestep: 272562, Episodes: 27750\n",
            "Mean reward: -29.50, Mean length: 30.00\n",
            "Mean loss: 0.03691, Exploration rate: 0.050\n",
            "Timestep: 272829, Episodes: 27760\n",
            "Mean reward: -26.20, Mean length: 26.70\n",
            "Mean loss: 0.03236, Exploration rate: 0.050\n",
            "Timestep: 273092, Episodes: 27770\n",
            "Mean reward: -25.80, Mean length: 26.30\n",
            "Mean loss: 0.03773, Exploration rate: 0.050\n",
            "Timestep: 273314, Episodes: 27780\n",
            "Mean reward: -21.60, Mean length: 22.20\n",
            "Mean loss: 0.03626, Exploration rate: 0.050\n",
            "Timestep: 273600, Episodes: 27790\n",
            "Mean reward: -28.10, Mean length: 28.60\n",
            "Mean loss: 0.02495, Exploration rate: 0.050\n",
            "Timestep: 273867, Episodes: 27800\n",
            "Mean reward: -26.20, Mean length: 26.70\n",
            "Mean loss: 0.03532, Exploration rate: 0.050\n",
            "Timestep: 273995, Episodes: 27810\n",
            "Mean reward: -12.00, Mean length: 12.80\n",
            "Mean loss: 0.02788, Exploration rate: 0.050\n",
            "Timestep: 274212, Episodes: 27820\n",
            "Mean reward: -21.10, Mean length: 21.70\n",
            "Mean loss: 0.03123, Exploration rate: 0.050\n",
            "Timestep: 274577, Episodes: 27830\n",
            "Mean reward: -36.20, Mean length: 36.50\n",
            "Mean loss: 0.03284, Exploration rate: 0.050\n",
            "Timestep: 274838, Episodes: 27840\n",
            "Mean reward: -25.60, Mean length: 26.10\n",
            "Mean loss: 0.02959, Exploration rate: 0.050\n",
            "Timestep: 275197, Episodes: 27850\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.02443, Exploration rate: 0.050\n",
            "Timestep: 275519, Episodes: 27860\n",
            "Mean reward: -31.80, Mean length: 32.20\n",
            "Mean loss: 0.02948, Exploration rate: 0.050\n",
            "Timestep: 275839, Episodes: 27870\n",
            "Mean reward: -31.60, Mean length: 32.00\n",
            "Mean loss: 0.03442, Exploration rate: 0.050\n",
            "Timestep: 276026, Episodes: 27880\n",
            "Mean reward: -18.00, Mean length: 18.70\n",
            "Mean loss: 0.02968, Exploration rate: 0.050\n",
            "Timestep: 276342, Episodes: 27890\n",
            "Mean reward: -31.20, Mean length: 31.60\n",
            "Mean loss: 0.02878, Exploration rate: 0.050\n",
            "Timestep: 276564, Episodes: 27900\n",
            "Mean reward: -21.60, Mean length: 22.20\n",
            "Mean loss: 0.02422, Exploration rate: 0.050\n",
            "Timestep: 276870, Episodes: 27910\n",
            "Mean reward: -30.20, Mean length: 30.60\n",
            "Mean loss: 0.03308, Exploration rate: 0.050\n",
            "Timestep: 277160, Episodes: 27920\n",
            "Mean reward: -28.50, Mean length: 29.00\n",
            "Mean loss: 0.02393, Exploration rate: 0.050\n",
            "Timestep: 277438, Episodes: 27930\n",
            "Mean reward: -27.30, Mean length: 27.80\n",
            "Mean loss: 0.03195, Exploration rate: 0.050\n",
            "Timestep: 277747, Episodes: 27940\n",
            "Mean reward: -30.50, Mean length: 30.90\n",
            "Mean loss: 0.02976, Exploration rate: 0.050\n",
            "Timestep: 278021, Episodes: 27950\n",
            "Mean reward: -26.90, Mean length: 27.40\n",
            "Mean loss: 0.02671, Exploration rate: 0.050\n",
            "Timestep: 278341, Episodes: 27960\n",
            "Mean reward: -31.60, Mean length: 32.00\n",
            "Mean loss: 0.02195, Exploration rate: 0.050\n",
            "Timestep: 278511, Episodes: 27970\n",
            "Mean reward: -16.30, Mean length: 17.00\n",
            "Mean loss: 0.02342, Exploration rate: 0.050\n",
            "Timestep: 278777, Episodes: 27980\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.02907, Exploration rate: 0.050\n",
            "Timestep: 278955, Episodes: 27990\n",
            "Mean reward: -17.10, Mean length: 17.80\n",
            "Mean loss: 0.03059, Exploration rate: 0.050\n",
            "Timestep: 279356, Episodes: 28000\n",
            "Mean reward: -39.80, Mean length: 40.10\n",
            "Mean loss: 0.02937, Exploration rate: 0.050\n",
            "Timestep: 279609, Episodes: 28010\n",
            "Mean reward: -24.70, Mean length: 25.30\n",
            "Mean loss: 0.03247, Exploration rate: 0.050\n",
            "Timestep: 279838, Episodes: 28020\n",
            "Mean reward: -22.30, Mean length: 22.90\n",
            "Mean loss: 0.02103, Exploration rate: 0.050\n",
            "Timestep: 280019, Episodes: 28030\n",
            "Mean reward: -17.40, Mean length: 18.10\n",
            "Mean loss: 0.01814, Exploration rate: 0.050\n",
            "Timestep: 280289, Episodes: 28040\n",
            "Mean reward: -26.50, Mean length: 27.00\n",
            "Mean loss: 0.02753, Exploration rate: 0.050\n",
            "Timestep: 280377, Episodes: 28050\n",
            "Mean reward: -7.90, Mean length: 8.80\n",
            "Mean loss: 0.02836, Exploration rate: 0.050\n",
            "Timestep: 280640, Episodes: 28060\n",
            "Mean reward: -25.80, Mean length: 26.30\n",
            "Mean loss: 0.02324, Exploration rate: 0.050\n",
            "Timestep: 280925, Episodes: 28070\n",
            "Mean reward: -28.00, Mean length: 28.50\n",
            "Mean loss: 0.02771, Exploration rate: 0.050\n",
            "Timestep: 281112, Episodes: 28080\n",
            "Mean reward: -18.00, Mean length: 18.70\n",
            "Mean loss: 0.02823, Exploration rate: 0.050\n",
            "Timestep: 281294, Episodes: 28090\n",
            "Mean reward: -17.50, Mean length: 18.20\n",
            "Mean loss: 0.03265, Exploration rate: 0.050\n",
            "Timestep: 281420, Episodes: 28100\n",
            "Mean reward: -11.80, Mean length: 12.60\n",
            "Mean loss: 0.02067, Exploration rate: 0.050\n",
            "Timestep: 281701, Episodes: 28110\n",
            "Mean reward: -27.60, Mean length: 28.10\n",
            "Mean loss: 0.02665, Exploration rate: 0.050\n",
            "Timestep: 281931, Episodes: 28120\n",
            "Mean reward: -22.40, Mean length: 23.00\n",
            "Mean loss: 0.02790, Exploration rate: 0.050\n",
            "Timestep: 282199, Episodes: 28130\n",
            "Mean reward: -26.30, Mean length: 26.80\n",
            "Mean loss: 0.02773, Exploration rate: 0.050\n",
            "Timestep: 282372, Episodes: 28140\n",
            "Mean reward: -16.60, Mean length: 17.30\n",
            "Mean loss: 0.02198, Exploration rate: 0.050\n",
            "Timestep: 282547, Episodes: 28150\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.02768, Exploration rate: 0.050\n",
            "Timestep: 282722, Episodes: 28160\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.02731, Exploration rate: 0.050\n",
            "Timestep: 283090, Episodes: 28170\n",
            "Mean reward: -36.50, Mean length: 36.80\n",
            "Mean loss: 0.02733, Exploration rate: 0.050\n",
            "Timestep: 283361, Episodes: 28180\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.02605, Exploration rate: 0.050\n",
            "Timestep: 283642, Episodes: 28190\n",
            "Mean reward: -27.60, Mean length: 28.10\n",
            "Mean loss: 0.02716, Exploration rate: 0.050\n",
            "Timestep: 283905, Episodes: 28200\n",
            "Mean reward: -25.80, Mean length: 26.30\n",
            "Mean loss: 0.02018, Exploration rate: 0.050\n",
            "Timestep: 284078, Episodes: 28210\n",
            "Mean reward: -16.60, Mean length: 17.30\n",
            "Mean loss: 0.01847, Exploration rate: 0.050\n",
            "Timestep: 284399, Episodes: 28220\n",
            "Mean reward: -31.70, Mean length: 32.10\n",
            "Mean loss: 0.03073, Exploration rate: 0.050\n",
            "Timestep: 284671, Episodes: 28230\n",
            "Mean reward: -26.70, Mean length: 27.20\n",
            "Mean loss: 0.02228, Exploration rate: 0.050\n",
            "Timestep: 284979, Episodes: 28240\n",
            "Mean reward: -30.40, Mean length: 30.80\n",
            "Mean loss: 0.02359, Exploration rate: 0.050\n",
            "Timestep: 285246, Episodes: 28250\n",
            "Mean reward: -26.20, Mean length: 26.70\n",
            "Mean loss: 0.02824, Exploration rate: 0.050\n",
            "Timestep: 285378, Episodes: 28260\n",
            "Mean reward: -12.40, Mean length: 13.20\n",
            "Mean loss: 0.02291, Exploration rate: 0.050\n",
            "Timestep: 285606, Episodes: 28270\n",
            "Mean reward: -22.20, Mean length: 22.80\n",
            "Mean loss: 0.02227, Exploration rate: 0.050\n",
            "Timestep: 285919, Episodes: 28280\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.02858, Exploration rate: 0.050\n",
            "Timestep: 286188, Episodes: 28290\n",
            "Mean reward: -26.30, Mean length: 26.90\n",
            "Mean loss: 0.02464, Exploration rate: 0.050\n",
            "Timestep: 286369, Episodes: 28300\n",
            "Mean reward: -17.40, Mean length: 18.10\n",
            "Mean loss: 0.02837, Exploration rate: 0.050\n",
            "Timestep: 286631, Episodes: 28310\n",
            "Mean reward: -25.70, Mean length: 26.20\n",
            "Mean loss: 0.02383, Exploration rate: 0.050\n",
            "Timestep: 286760, Episodes: 28320\n",
            "Mean reward: -12.10, Mean length: 12.90\n",
            "Mean loss: 0.02699, Exploration rate: 0.050\n",
            "Timestep: 286891, Episodes: 28330\n",
            "Mean reward: -12.30, Mean length: 13.10\n",
            "Mean loss: 0.02694, Exploration rate: 0.050\n",
            "Timestep: 287126, Episodes: 28340\n",
            "Mean reward: -22.90, Mean length: 23.50\n",
            "Mean loss: 0.02697, Exploration rate: 0.050\n",
            "Timestep: 287534, Episodes: 28350\n",
            "Mean reward: -40.60, Mean length: 40.80\n",
            "Mean loss: 0.02955, Exploration rate: 0.050\n",
            "Timestep: 287893, Episodes: 28360\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.02631, Exploration rate: 0.050\n",
            "Timestep: 287980, Episodes: 28370\n",
            "Mean reward: -7.80, Mean length: 8.70\n",
            "Mean loss: 0.02153, Exploration rate: 0.050\n",
            "Timestep: 288356, Episodes: 28380\n",
            "Mean reward: -37.30, Mean length: 37.60\n",
            "Mean loss: 0.01867, Exploration rate: 0.050\n",
            "Timestep: 288531, Episodes: 28390\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.02606, Exploration rate: 0.050\n",
            "Timestep: 288891, Episodes: 28400\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.01925, Exploration rate: 0.050\n",
            "Timestep: 289163, Episodes: 28410\n",
            "Mean reward: -26.70, Mean length: 27.20\n",
            "Mean loss: 0.02088, Exploration rate: 0.050\n",
            "Timestep: 289483, Episodes: 28420\n",
            "Mean reward: -31.60, Mean length: 32.00\n",
            "Mean loss: 0.02550, Exploration rate: 0.050\n",
            "Timestep: 289795, Episodes: 28430\n",
            "Mean reward: -30.80, Mean length: 31.20\n",
            "Mean loss: 0.02251, Exploration rate: 0.050\n",
            "Timestep: 290014, Episodes: 28440\n",
            "Mean reward: -21.30, Mean length: 21.90\n",
            "Mean loss: 0.02579, Exploration rate: 0.050\n",
            "Timestep: 290298, Episodes: 28450\n",
            "Mean reward: -27.90, Mean length: 28.40\n",
            "Mean loss: 0.02146, Exploration rate: 0.050\n",
            "Timestep: 290530, Episodes: 28460\n",
            "Mean reward: -22.60, Mean length: 23.20\n",
            "Mean loss: 0.02246, Exploration rate: 0.050\n",
            "Timestep: 290700, Episodes: 28470\n",
            "Mean reward: -16.30, Mean length: 17.00\n",
            "Mean loss: 0.02582, Exploration rate: 0.050\n",
            "Timestep: 291027, Episodes: 28480\n",
            "Mean reward: -32.30, Mean length: 32.70\n",
            "Mean loss: 0.02396, Exploration rate: 0.050\n",
            "Timestep: 291276, Episodes: 28490\n",
            "Mean reward: -24.30, Mean length: 24.90\n",
            "Mean loss: 0.02039, Exploration rate: 0.050\n",
            "Timestep: 291538, Episodes: 28500\n",
            "Mean reward: -25.70, Mean length: 26.20\n",
            "Mean loss: 0.02252, Exploration rate: 0.050\n",
            "Timestep: 291852, Episodes: 28510\n",
            "Mean reward: -31.00, Mean length: 31.40\n",
            "Mean loss: 0.02509, Exploration rate: 0.050\n",
            "Timestep: 292074, Episodes: 28520\n",
            "Mean reward: -21.50, Mean length: 22.20\n",
            "Mean loss: 0.02297, Exploration rate: 0.050\n",
            "Timestep: 292236, Episodes: 28530\n",
            "Mean reward: -15.40, Mean length: 16.20\n",
            "Mean loss: 0.01989, Exploration rate: 0.050\n",
            "Timestep: 292412, Episodes: 28540\n",
            "Mean reward: -16.90, Mean length: 17.60\n",
            "Mean loss: 0.02371, Exploration rate: 0.050\n",
            "Timestep: 292582, Episodes: 28550\n",
            "Mean reward: -16.30, Mean length: 17.00\n",
            "Mean loss: 0.02347, Exploration rate: 0.050\n",
            "Timestep: 292753, Episodes: 28560\n",
            "Mean reward: -16.40, Mean length: 17.10\n",
            "Mean loss: 0.02564, Exploration rate: 0.050\n",
            "Timestep: 293019, Episodes: 28570\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.02066, Exploration rate: 0.050\n",
            "Timestep: 293261, Episodes: 28580\n",
            "Mean reward: -23.60, Mean length: 24.20\n",
            "Mean loss: 0.01510, Exploration rate: 0.050\n",
            "Timestep: 293400, Episodes: 28590\n",
            "Mean reward: -13.10, Mean length: 13.90\n",
            "Mean loss: 0.02835, Exploration rate: 0.050\n",
            "Timestep: 293575, Episodes: 28600\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.02165, Exploration rate: 0.050\n",
            "Timestep: 293698, Episodes: 28610\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.02491, Exploration rate: 0.050\n",
            "Timestep: 293917, Episodes: 28620\n",
            "Mean reward: -21.30, Mean length: 21.90\n",
            "Mean loss: 0.02350, Exploration rate: 0.050\n",
            "Timestep: 294133, Episodes: 28630\n",
            "Mean reward: -21.00, Mean length: 21.60\n",
            "Mean loss: 0.02616, Exploration rate: 0.050\n",
            "Timestep: 294303, Episodes: 28640\n",
            "Mean reward: -16.30, Mean length: 17.00\n",
            "Mean loss: 0.02182, Exploration rate: 0.050\n",
            "Timestep: 294565, Episodes: 28650\n",
            "Mean reward: -25.70, Mean length: 26.20\n",
            "Mean loss: 0.01931, Exploration rate: 0.050\n",
            "Timestep: 294688, Episodes: 28660\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.02518, Exploration rate: 0.050\n",
            "Timestep: 294832, Episodes: 28670\n",
            "Mean reward: -13.60, Mean length: 14.40\n",
            "Mean loss: 0.02484, Exploration rate: 0.050\n",
            "Timestep: 294966, Episodes: 28680\n",
            "Mean reward: -12.60, Mean length: 13.40\n",
            "Mean loss: 0.02697, Exploration rate: 0.050\n",
            "Timestep: 295326, Episodes: 28690\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.02874, Exploration rate: 0.050\n",
            "Timestep: 295737, Episodes: 28700\n",
            "Mean reward: -40.90, Mean length: 41.10\n",
            "Mean loss: 0.02491, Exploration rate: 0.050\n",
            "Timestep: 296001, Episodes: 28710\n",
            "Mean reward: -25.90, Mean length: 26.40\n",
            "Mean loss: 0.02492, Exploration rate: 0.050\n",
            "Timestep: 296361, Episodes: 28720\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.02410, Exploration rate: 0.050\n",
            "Timestep: 296533, Episodes: 28730\n",
            "Mean reward: -16.50, Mean length: 17.20\n",
            "Mean loss: 0.02043, Exploration rate: 0.050\n",
            "Timestep: 296709, Episodes: 28740\n",
            "Mean reward: -16.90, Mean length: 17.60\n",
            "Mean loss: 0.02973, Exploration rate: 0.050\n",
            "Timestep: 296927, Episodes: 28750\n",
            "Mean reward: -21.20, Mean length: 21.80\n",
            "Mean loss: 0.01908, Exploration rate: 0.050\n",
            "Timestep: 297155, Episodes: 28760\n",
            "Mean reward: -22.20, Mean length: 22.80\n",
            "Mean loss: 0.02435, Exploration rate: 0.050\n",
            "Timestep: 297424, Episodes: 28770\n",
            "Mean reward: -26.40, Mean length: 26.90\n",
            "Mean loss: 0.01613, Exploration rate: 0.050\n",
            "Timestep: 297646, Episodes: 28780\n",
            "Mean reward: -21.60, Mean length: 22.20\n",
            "Mean loss: 0.01876, Exploration rate: 0.050\n",
            "Timestep: 297961, Episodes: 28790\n",
            "Mean reward: -31.10, Mean length: 31.50\n",
            "Mean loss: 0.01731, Exploration rate: 0.050\n",
            "Timestep: 298226, Episodes: 28800\n",
            "Mean reward: -26.00, Mean length: 26.50\n",
            "Mean loss: 0.01805, Exploration rate: 0.050\n",
            "Timestep: 298514, Episodes: 28810\n",
            "Mean reward: -28.30, Mean length: 28.80\n",
            "Mean loss: 0.02521, Exploration rate: 0.050\n",
            "Timestep: 298778, Episodes: 28820\n",
            "Mean reward: -25.90, Mean length: 26.40\n",
            "Mean loss: 0.02781, Exploration rate: 0.050\n",
            "Timestep: 299005, Episodes: 28830\n",
            "Mean reward: -22.10, Mean length: 22.70\n",
            "Mean loss: 0.01568, Exploration rate: 0.050\n",
            "Timestep: 299364, Episodes: 28840\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.02310, Exploration rate: 0.050\n",
            "Timestep: 299572, Episodes: 28850\n",
            "Mean reward: -20.10, Mean length: 20.80\n",
            "Mean loss: 0.01797, Exploration rate: 0.050\n",
            "Timestep: 299700, Episodes: 28860\n",
            "Mean reward: -12.00, Mean length: 12.80\n",
            "Mean loss: 0.02132, Exploration rate: 0.050\n",
            "Timestep: 299934, Episodes: 28870\n",
            "Mean reward: -22.80, Mean length: 23.40\n",
            "Mean loss: 0.02239, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_300000.pth\n",
            "Evaluation: Mean reward: -25.90, mean episode length: 26.40\n",
            "Timestep: 300338, Episodes: 28880\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.02711, Exploration rate: 0.050\n",
            "Timestep: 300461, Episodes: 28890\n",
            "Mean reward: -11.50, Mean length: 12.30\n",
            "Mean loss: 0.02011, Exploration rate: 0.050\n",
            "Timestep: 300727, Episodes: 28900\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.01761, Exploration rate: 0.050\n",
            "Timestep: 300911, Episodes: 28910\n",
            "Mean reward: -17.70, Mean length: 18.40\n",
            "Mean loss: 0.02061, Exploration rate: 0.050\n",
            "Timestep: 301077, Episodes: 28920\n",
            "Mean reward: -15.80, Mean length: 16.60\n",
            "Mean loss: 0.02066, Exploration rate: 0.050\n",
            "Timestep: 301151, Episodes: 28930\n",
            "Mean reward: -6.50, Mean length: 7.40\n",
            "Mean loss: 0.02243, Exploration rate: 0.050\n",
            "Timestep: 301278, Episodes: 28940\n",
            "Mean reward: -11.90, Mean length: 12.70\n",
            "Mean loss: 0.02060, Exploration rate: 0.050\n",
            "Timestep: 301498, Episodes: 28950\n",
            "Mean reward: -21.40, Mean length: 22.00\n",
            "Mean loss: 0.01908, Exploration rate: 0.050\n",
            "Timestep: 301761, Episodes: 28960\n",
            "Mean reward: -25.80, Mean length: 26.30\n",
            "Mean loss: 0.02722, Exploration rate: 0.050\n",
            "Timestep: 302030, Episodes: 28970\n",
            "Mean reward: -26.40, Mean length: 26.90\n",
            "Mean loss: 0.01528, Exploration rate: 0.050\n",
            "Timestep: 302307, Episodes: 28980\n",
            "Mean reward: -27.20, Mean length: 27.70\n",
            "Mean loss: 0.02234, Exploration rate: 0.050\n",
            "Timestep: 302577, Episodes: 28990\n",
            "Mean reward: -26.50, Mean length: 27.00\n",
            "Mean loss: 0.01571, Exploration rate: 0.050\n",
            "Timestep: 302854, Episodes: 29000\n",
            "Mean reward: -27.20, Mean length: 27.70\n",
            "Mean loss: 0.01711, Exploration rate: 0.050\n",
            "Timestep: 303163, Episodes: 29010\n",
            "Mean reward: -30.50, Mean length: 30.90\n",
            "Mean loss: 0.02043, Exploration rate: 0.050\n",
            "Timestep: 303311, Episodes: 29020\n",
            "Mean reward: -13.90, Mean length: 14.80\n",
            "Mean loss: 0.01698, Exploration rate: 0.050\n",
            "Timestep: 303579, Episodes: 29030\n",
            "Mean reward: -26.30, Mean length: 26.80\n",
            "Mean loss: 0.01690, Exploration rate: 0.050\n",
            "Timestep: 303811, Episodes: 29040\n",
            "Mean reward: -22.60, Mean length: 23.20\n",
            "Mean loss: 0.01804, Exploration rate: 0.050\n",
            "Timestep: 304172, Episodes: 29050\n",
            "Mean reward: -35.80, Mean length: 36.10\n",
            "Mean loss: 0.02352, Exploration rate: 0.050\n",
            "Timestep: 304477, Episodes: 29060\n",
            "Mean reward: -30.00, Mean length: 30.50\n",
            "Mean loss: 0.01566, Exploration rate: 0.050\n",
            "Timestep: 304701, Episodes: 29070\n",
            "Mean reward: -21.80, Mean length: 22.40\n",
            "Mean loss: 0.01393, Exploration rate: 0.050\n",
            "Timestep: 305026, Episodes: 29080\n",
            "Mean reward: -32.10, Mean length: 32.50\n",
            "Mean loss: 0.01632, Exploration rate: 0.050\n",
            "Timestep: 305423, Episodes: 29090\n",
            "Mean reward: -39.40, Mean length: 39.70\n",
            "Mean loss: 0.02333, Exploration rate: 0.050\n",
            "Timestep: 305733, Episodes: 29100\n",
            "Mean reward: -30.60, Mean length: 31.00\n",
            "Mean loss: 0.02615, Exploration rate: 0.050\n",
            "Timestep: 306074, Episodes: 29110\n",
            "Mean reward: -33.70, Mean length: 34.10\n",
            "Mean loss: 0.01371, Exploration rate: 0.050\n",
            "Timestep: 306221, Episodes: 29120\n",
            "Mean reward: -13.90, Mean length: 14.70\n",
            "Mean loss: 0.02261, Exploration rate: 0.050\n",
            "Timestep: 306492, Episodes: 29130\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.02207, Exploration rate: 0.050\n",
            "Timestep: 306667, Episodes: 29140\n",
            "Mean reward: -16.80, Mean length: 17.50\n",
            "Mean loss: 0.01971, Exploration rate: 0.050\n",
            "Timestep: 306929, Episodes: 29150\n",
            "Mean reward: -25.60, Mean length: 26.20\n",
            "Mean loss: 0.01613, Exploration rate: 0.050\n",
            "Timestep: 307239, Episodes: 29160\n",
            "Mean reward: -30.60, Mean length: 31.00\n",
            "Mean loss: 0.01837, Exploration rate: 0.050\n",
            "Timestep: 307644, Episodes: 29170\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.02495, Exploration rate: 0.050\n",
            "Timestep: 308002, Episodes: 29180\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.01753, Exploration rate: 0.050\n",
            "Timestep: 308225, Episodes: 29190\n",
            "Mean reward: -21.70, Mean length: 22.30\n",
            "Mean loss: 0.01215, Exploration rate: 0.050\n",
            "Timestep: 308648, Episodes: 29200\n",
            "Mean reward: -42.10, Mean length: 42.30\n",
            "Mean loss: 0.02004, Exploration rate: 0.050\n",
            "Timestep: 308924, Episodes: 29210\n",
            "Mean reward: -27.10, Mean length: 27.60\n",
            "Mean loss: 0.01667, Exploration rate: 0.050\n",
            "Timestep: 309190, Episodes: 29220\n",
            "Mean reward: -26.00, Mean length: 26.60\n",
            "Mean loss: 0.01462, Exploration rate: 0.050\n",
            "Timestep: 309336, Episodes: 29230\n",
            "Mean reward: -13.80, Mean length: 14.60\n",
            "Mean loss: 0.01548, Exploration rate: 0.050\n",
            "Timestep: 309742, Episodes: 29240\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.01958, Exploration rate: 0.050\n",
            "Timestep: 310100, Episodes: 29250\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.02389, Exploration rate: 0.050\n",
            "Timestep: 310413, Episodes: 29260\n",
            "Mean reward: -30.90, Mean length: 31.30\n",
            "Mean loss: 0.01977, Exploration rate: 0.050\n",
            "Timestep: 310628, Episodes: 29270\n",
            "Mean reward: -20.90, Mean length: 21.50\n",
            "Mean loss: 0.02303, Exploration rate: 0.050\n",
            "Timestep: 310894, Episodes: 29280\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.02459, Exploration rate: 0.050\n",
            "Timestep: 311154, Episodes: 29290\n",
            "Mean reward: -25.50, Mean length: 26.00\n",
            "Mean loss: 0.01584, Exploration rate: 0.050\n",
            "Timestep: 311512, Episodes: 29300\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.02095, Exploration rate: 0.050\n",
            "Timestep: 311809, Episodes: 29310\n",
            "Mean reward: -29.20, Mean length: 29.70\n",
            "Mean loss: 0.01520, Exploration rate: 0.050\n",
            "Timestep: 311881, Episodes: 29320\n",
            "Mean reward: -6.30, Mean length: 7.20\n",
            "Mean loss: 0.01183, Exploration rate: 0.050\n",
            "Timestep: 312147, Episodes: 29330\n",
            "Mean reward: -26.10, Mean length: 26.60\n",
            "Mean loss: 0.01575, Exploration rate: 0.050\n",
            "Timestep: 312359, Episodes: 29340\n",
            "Mean reward: -20.60, Mean length: 21.20\n",
            "Mean loss: 0.01910, Exploration rate: 0.050\n",
            "Timestep: 312628, Episodes: 29350\n",
            "Mean reward: -26.40, Mean length: 26.90\n",
            "Mean loss: 0.01915, Exploration rate: 0.050\n",
            "Timestep: 313038, Episodes: 29360\n",
            "Mean reward: -40.80, Mean length: 41.00\n",
            "Mean loss: 0.02212, Exploration rate: 0.050\n",
            "Timestep: 313292, Episodes: 29370\n",
            "Mean reward: -24.80, Mean length: 25.40\n",
            "Mean loss: 0.01303, Exploration rate: 0.050\n",
            "Timestep: 313603, Episodes: 29380\n",
            "Mean reward: -30.70, Mean length: 31.10\n",
            "Mean loss: 0.01496, Exploration rate: 0.050\n",
            "Timestep: 313752, Episodes: 29390\n",
            "Mean reward: -14.10, Mean length: 14.90\n",
            "Mean loss: 0.01384, Exploration rate: 0.050\n",
            "Timestep: 314020, Episodes: 29400\n",
            "Mean reward: -26.20, Mean length: 26.80\n",
            "Mean loss: 0.02420, Exploration rate: 0.050\n",
            "Timestep: 314332, Episodes: 29410\n",
            "Mean reward: -30.80, Mean length: 31.20\n",
            "Mean loss: 0.01717, Exploration rate: 0.050\n",
            "Timestep: 314553, Episodes: 29420\n",
            "Mean reward: -21.50, Mean length: 22.10\n",
            "Mean loss: 0.01865, Exploration rate: 0.050\n",
            "Timestep: 314825, Episodes: 29430\n",
            "Mean reward: -26.70, Mean length: 27.20\n",
            "Mean loss: 0.01609, Exploration rate: 0.050\n",
            "Timestep: 315096, Episodes: 29440\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.01776, Exploration rate: 0.050\n",
            "Timestep: 315320, Episodes: 29450\n",
            "Mean reward: -21.80, Mean length: 22.40\n",
            "Mean loss: 0.01510, Exploration rate: 0.050\n",
            "Timestep: 315634, Episodes: 29460\n",
            "Mean reward: -31.00, Mean length: 31.40\n",
            "Mean loss: 0.01613, Exploration rate: 0.050\n",
            "Timestep: 315756, Episodes: 29470\n",
            "Mean reward: -11.40, Mean length: 12.20\n",
            "Mean loss: 0.01806, Exploration rate: 0.050\n",
            "Timestep: 316072, Episodes: 29480\n",
            "Mean reward: -31.20, Mean length: 31.60\n",
            "Mean loss: 0.01486, Exploration rate: 0.050\n",
            "Timestep: 316301, Episodes: 29490\n",
            "Mean reward: -22.30, Mean length: 22.90\n",
            "Mean loss: 0.01292, Exploration rate: 0.050\n",
            "Timestep: 316617, Episodes: 29500\n",
            "Mean reward: -31.20, Mean length: 31.60\n",
            "Mean loss: 0.01916, Exploration rate: 0.050\n",
            "Timestep: 316888, Episodes: 29510\n",
            "Mean reward: -26.60, Mean length: 27.10\n",
            "Mean loss: 0.01520, Exploration rate: 0.050\n",
            "Timestep: 317175, Episodes: 29520\n",
            "Mean reward: -28.20, Mean length: 28.70\n",
            "Mean loss: 0.01811, Exploration rate: 0.050\n",
            "Timestep: 317436, Episodes: 29530\n",
            "Mean reward: -25.60, Mean length: 26.10\n",
            "Mean loss: 0.01861, Exploration rate: 0.050\n",
            "Timestep: 317669, Episodes: 29540\n",
            "Mean reward: -22.70, Mean length: 23.30\n",
            "Mean loss: 0.01971, Exploration rate: 0.050\n",
            "Timestep: 317986, Episodes: 29550\n",
            "Mean reward: -31.30, Mean length: 31.70\n",
            "Mean loss: 0.01302, Exploration rate: 0.050\n",
            "Timestep: 318297, Episodes: 29560\n",
            "Mean reward: -30.70, Mean length: 31.10\n",
            "Mean loss: 0.01095, Exploration rate: 0.050\n",
            "Timestep: 318422, Episodes: 29570\n",
            "Mean reward: -11.70, Mean length: 12.50\n",
            "Mean loss: 0.01506, Exploration rate: 0.050\n",
            "Timestep: 318637, Episodes: 29580\n",
            "Mean reward: -20.90, Mean length: 21.50\n",
            "Mean loss: 0.01327, Exploration rate: 0.050\n",
            "Timestep: 318803, Episodes: 29590\n",
            "Mean reward: -15.80, Mean length: 16.60\n",
            "Mean loss: 0.01604, Exploration rate: 0.050\n",
            "Timestep: 319073, Episodes: 29600\n",
            "Mean reward: -26.50, Mean length: 27.00\n",
            "Mean loss: 0.02169, Exploration rate: 0.050\n",
            "Timestep: 319431, Episodes: 29610\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.02142, Exploration rate: 0.050\n",
            "Timestep: 319618, Episodes: 29620\n",
            "Mean reward: -18.00, Mean length: 18.70\n",
            "Mean loss: 0.01515, Exploration rate: 0.050\n",
            "Timestep: 319975, Episodes: 29630\n",
            "Mean reward: -35.40, Mean length: 35.70\n",
            "Mean loss: 0.01898, Exploration rate: 0.050\n",
            "Timestep: 320290, Episodes: 29640\n",
            "Mean reward: -31.10, Mean length: 31.50\n",
            "Mean loss: 0.01287, Exploration rate: 0.050\n",
            "Timestep: 320470, Episodes: 29650\n",
            "Mean reward: -17.30, Mean length: 18.00\n",
            "Mean loss: 0.01756, Exploration rate: 0.050\n",
            "Timestep: 320692, Episodes: 29660\n",
            "Mean reward: -21.60, Mean length: 22.20\n",
            "Mean loss: 0.01198, Exploration rate: 0.050\n",
            "Timestep: 321006, Episodes: 29670\n",
            "Mean reward: -31.00, Mean length: 31.40\n",
            "Mean loss: 0.01960, Exploration rate: 0.050\n",
            "Timestep: 321379, Episodes: 29680\n",
            "Mean reward: -37.00, Mean length: 37.30\n",
            "Mean loss: 0.01517, Exploration rate: 0.050\n",
            "Timestep: 321694, Episodes: 29690\n",
            "Mean reward: -31.10, Mean length: 31.50\n",
            "Mean loss: 0.01673, Exploration rate: 0.050\n",
            "Timestep: 321956, Episodes: 29700\n",
            "Mean reward: -25.70, Mean length: 26.20\n",
            "Mean loss: 0.01709, Exploration rate: 0.050\n",
            "Timestep: 322189, Episodes: 29710\n",
            "Mean reward: -22.70, Mean length: 23.30\n",
            "Mean loss: 0.01617, Exploration rate: 0.050\n",
            "Timestep: 322559, Episodes: 29720\n",
            "Mean reward: -36.70, Mean length: 37.00\n",
            "Mean loss: 0.01152, Exploration rate: 0.050\n",
            "Timestep: 322875, Episodes: 29730\n",
            "Mean reward: -31.20, Mean length: 31.60\n",
            "Mean loss: 0.01426, Exploration rate: 0.050\n",
            "Timestep: 323140, Episodes: 29740\n",
            "Mean reward: -26.00, Mean length: 26.50\n",
            "Mean loss: 0.01418, Exploration rate: 0.050\n",
            "Timestep: 323408, Episodes: 29750\n",
            "Mean reward: -26.30, Mean length: 26.80\n",
            "Mean loss: 0.01327, Exploration rate: 0.050\n",
            "Timestep: 323680, Episodes: 29760\n",
            "Mean reward: -26.70, Mean length: 27.20\n",
            "Mean loss: 0.01518, Exploration rate: 0.050\n",
            "Timestep: 323944, Episodes: 29770\n",
            "Mean reward: -25.90, Mean length: 26.40\n",
            "Mean loss: 0.01496, Exploration rate: 0.050\n",
            "Timestep: 324223, Episodes: 29780\n",
            "Mean reward: -27.40, Mean length: 27.90\n",
            "Mean loss: 0.01931, Exploration rate: 0.050\n",
            "Timestep: 324438, Episodes: 29790\n",
            "Mean reward: -20.90, Mean length: 21.50\n",
            "Mean loss: 0.01666, Exploration rate: 0.050\n",
            "Timestep: 324846, Episodes: 29800\n",
            "Mean reward: -40.60, Mean length: 40.80\n",
            "Mean loss: 0.01663, Exploration rate: 0.050\n",
            "Timestep: 325250, Episodes: 29810\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.01623, Exploration rate: 0.050\n",
            "Timestep: 325655, Episodes: 29820\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.01958, Exploration rate: 0.050\n",
            "Timestep: 325874, Episodes: 29830\n",
            "Mean reward: -21.30, Mean length: 21.90\n",
            "Mean loss: 0.01713, Exploration rate: 0.050\n",
            "Timestep: 326186, Episodes: 29840\n",
            "Mean reward: -30.80, Mean length: 31.20\n",
            "Mean loss: 0.01679, Exploration rate: 0.050\n",
            "Timestep: 326545, Episodes: 29850\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.01806, Exploration rate: 0.050\n",
            "Timestep: 326998, Episodes: 29860\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.01512, Exploration rate: 0.050\n",
            "Timestep: 327400, Episodes: 29870\n",
            "Mean reward: -40.00, Mean length: 40.20\n",
            "Mean loss: 0.01453, Exploration rate: 0.050\n",
            "Timestep: 327674, Episodes: 29880\n",
            "Mean reward: -26.90, Mean length: 27.40\n",
            "Mean loss: 0.01310, Exploration rate: 0.050\n",
            "Timestep: 328125, Episodes: 29890\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.02054, Exploration rate: 0.050\n",
            "Timestep: 328441, Episodes: 29900\n",
            "Mean reward: -31.20, Mean length: 31.60\n",
            "Mean loss: 0.01322, Exploration rate: 0.050\n",
            "Timestep: 328789, Episodes: 29910\n",
            "Mean reward: -34.40, Mean length: 34.80\n",
            "Mean loss: 0.01714, Exploration rate: 0.050\n",
            "Timestep: 329150, Episodes: 29920\n",
            "Mean reward: -35.80, Mean length: 36.10\n",
            "Mean loss: 0.01563, Exploration rate: 0.050\n",
            "Timestep: 329508, Episodes: 29930\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.01488, Exploration rate: 0.050\n",
            "Timestep: 329820, Episodes: 29940\n",
            "Mean reward: -30.80, Mean length: 31.20\n",
            "Mean loss: 0.01751, Exploration rate: 0.050\n",
            "Timestep: 330225, Episodes: 29950\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.01813, Exploration rate: 0.050\n",
            "Timestep: 330627, Episodes: 29960\n",
            "Mean reward: -40.00, Mean length: 40.20\n",
            "Mean loss: 0.01906, Exploration rate: 0.050\n",
            "Timestep: 330909, Episodes: 29970\n",
            "Mean reward: -27.70, Mean length: 28.20\n",
            "Mean loss: 0.02629, Exploration rate: 0.050\n",
            "Timestep: 331314, Episodes: 29980\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.02248, Exploration rate: 0.050\n",
            "Timestep: 331641, Episodes: 29990\n",
            "Mean reward: -32.30, Mean length: 32.70\n",
            "Mean loss: 0.02509, Exploration rate: 0.050\n",
            "Timestep: 332001, Episodes: 30000\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.01936, Exploration rate: 0.050\n",
            "Timestep: 332376, Episodes: 30010\n",
            "Mean reward: -37.20, Mean length: 37.50\n",
            "Mean loss: 0.02534, Exploration rate: 0.050\n",
            "Timestep: 332792, Episodes: 30020\n",
            "Mean reward: -41.40, Mean length: 41.60\n",
            "Mean loss: 0.02499, Exploration rate: 0.050\n",
            "Timestep: 333172, Episodes: 30030\n",
            "Mean reward: -37.70, Mean length: 38.00\n",
            "Mean loss: 0.02572, Exploration rate: 0.050\n",
            "Timestep: 333457, Episodes: 30040\n",
            "Mean reward: -28.00, Mean length: 28.50\n",
            "Mean loss: 0.02441, Exploration rate: 0.050\n",
            "Timestep: 333915, Episodes: 30050\n",
            "Mean reward: -45.70, Mean length: 45.80\n",
            "Mean loss: 0.02545, Exploration rate: 0.050\n",
            "Timestep: 334236, Episodes: 30060\n",
            "Mean reward: -31.70, Mean length: 32.10\n",
            "Mean loss: 0.02118, Exploration rate: 0.050\n",
            "Timestep: 334594, Episodes: 30070\n",
            "Mean reward: -35.50, Mean length: 35.80\n",
            "Mean loss: 0.02062, Exploration rate: 0.050\n",
            "Timestep: 334999, Episodes: 30080\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.02514, Exploration rate: 0.050\n",
            "Timestep: 335499, Episodes: 30090\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02520, Exploration rate: 0.050\n",
            "Timestep: 335999, Episodes: 30100\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.01966, Exploration rate: 0.050\n",
            "Timestep: 336402, Episodes: 30110\n",
            "Mean reward: -40.10, Mean length: 40.30\n",
            "Mean loss: 0.02978, Exploration rate: 0.050\n",
            "Timestep: 336853, Episodes: 30120\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.02820, Exploration rate: 0.050\n",
            "Timestep: 337305, Episodes: 30130\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.02552, Exploration rate: 0.050\n",
            "Timestep: 337805, Episodes: 30140\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03223, Exploration rate: 0.050\n",
            "Timestep: 338305, Episodes: 30150\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02768, Exploration rate: 0.050\n",
            "Timestep: 338805, Episodes: 30160\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03093, Exploration rate: 0.050\n",
            "Timestep: 339256, Episodes: 30170\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.03031, Exploration rate: 0.050\n",
            "Timestep: 339662, Episodes: 30180\n",
            "Mean reward: -40.40, Mean length: 40.60\n",
            "Mean loss: 0.03014, Exploration rate: 0.050\n",
            "Timestep: 340162, Episodes: 30190\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02887, Exploration rate: 0.050\n",
            "Timestep: 340662, Episodes: 30200\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02648, Exploration rate: 0.050\n",
            "Timestep: 341162, Episodes: 30210\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02937, Exploration rate: 0.050\n",
            "Timestep: 341662, Episodes: 30220\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02796, Exploration rate: 0.050\n",
            "Timestep: 342162, Episodes: 30230\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03115, Exploration rate: 0.050\n",
            "Timestep: 342615, Episodes: 30240\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.02560, Exploration rate: 0.050\n",
            "Timestep: 343115, Episodes: 30250\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03236, Exploration rate: 0.050\n",
            "Timestep: 343615, Episodes: 30260\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02723, Exploration rate: 0.050\n",
            "Timestep: 344115, Episodes: 30270\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03140, Exploration rate: 0.050\n",
            "Timestep: 344615, Episodes: 30280\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03416, Exploration rate: 0.050\n",
            "Timestep: 345115, Episodes: 30290\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02869, Exploration rate: 0.050\n",
            "Timestep: 345615, Episodes: 30300\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03015, Exploration rate: 0.050\n",
            "Timestep: 346068, Episodes: 30310\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.02430, Exploration rate: 0.050\n",
            "Timestep: 346568, Episodes: 30320\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02744, Exploration rate: 0.050\n",
            "Timestep: 347068, Episodes: 30330\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03559, Exploration rate: 0.050\n",
            "Timestep: 347568, Episodes: 30340\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02561, Exploration rate: 0.050\n",
            "Timestep: 348068, Episodes: 30350\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03107, Exploration rate: 0.050\n",
            "Timestep: 348470, Episodes: 30360\n",
            "Mean reward: -40.00, Mean length: 40.20\n",
            "Mean loss: 0.02974, Exploration rate: 0.050\n",
            "Timestep: 348970, Episodes: 30370\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03088, Exploration rate: 0.050\n",
            "Timestep: 349423, Episodes: 30380\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.02821, Exploration rate: 0.050\n",
            "Timestep: 349875, Episodes: 30390\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.02779, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_350000.pth\n",
            "Evaluation: Mean reward: -50.00, mean episode length: 50.00\n",
            "Timestep: 350375, Episodes: 30400\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.02790, Exploration rate: 0.050\n",
            "Timestep: 350826, Episodes: 30410\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.02578, Exploration rate: 0.050\n",
            "Timestep: 351326, Episodes: 30420\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03562, Exploration rate: 0.050\n",
            "Timestep: 351826, Episodes: 30430\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03082, Exploration rate: 0.050\n",
            "Timestep: 352326, Episodes: 30440\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03373, Exploration rate: 0.050\n",
            "Timestep: 352826, Episodes: 30450\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03130, Exploration rate: 0.050\n",
            "Timestep: 353277, Episodes: 30460\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.04002, Exploration rate: 0.050\n",
            "Timestep: 353728, Episodes: 30470\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.02973, Exploration rate: 0.050\n",
            "Timestep: 354228, Episodes: 30480\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03888, Exploration rate: 0.050\n",
            "Timestep: 354680, Episodes: 30490\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.03953, Exploration rate: 0.050\n",
            "Timestep: 355131, Episodes: 30500\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.03840, Exploration rate: 0.050\n",
            "Timestep: 355582, Episodes: 30510\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.04916, Exploration rate: 0.050\n",
            "Timestep: 356082, Episodes: 30520\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05100, Exploration rate: 0.050\n",
            "Timestep: 356582, Episodes: 30530\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03531, Exploration rate: 0.050\n",
            "Timestep: 357082, Episodes: 30540\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04904, Exploration rate: 0.050\n",
            "Timestep: 357535, Episodes: 30550\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.03451, Exploration rate: 0.050\n",
            "Timestep: 357989, Episodes: 30560\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.04639, Exploration rate: 0.050\n",
            "Timestep: 358489, Episodes: 30570\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04503, Exploration rate: 0.050\n",
            "Timestep: 358989, Episodes: 30580\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04159, Exploration rate: 0.050\n",
            "Timestep: 359489, Episodes: 30590\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06112, Exploration rate: 0.050\n",
            "Timestep: 359989, Episodes: 30600\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05600, Exploration rate: 0.050\n",
            "Timestep: 360489, Episodes: 30610\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05093, Exploration rate: 0.050\n",
            "Timestep: 360942, Episodes: 30620\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.05803, Exploration rate: 0.050\n",
            "Timestep: 361442, Episodes: 30630\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05336, Exploration rate: 0.050\n",
            "Timestep: 361942, Episodes: 30640\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06087, Exploration rate: 0.050\n",
            "Timestep: 362396, Episodes: 30650\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.06210, Exploration rate: 0.050\n",
            "Timestep: 362847, Episodes: 30660\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.06430, Exploration rate: 0.050\n",
            "Timestep: 363347, Episodes: 30670\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07026, Exploration rate: 0.050\n",
            "Timestep: 363847, Episodes: 30680\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07862, Exploration rate: 0.050\n",
            "Timestep: 364300, Episodes: 30690\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.07888, Exploration rate: 0.050\n",
            "Timestep: 364800, Episodes: 30700\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08066, Exploration rate: 0.050\n",
            "Timestep: 365251, Episodes: 30710\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.06846, Exploration rate: 0.050\n",
            "Timestep: 365751, Episodes: 30720\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06949, Exploration rate: 0.050\n",
            "Timestep: 366251, Episodes: 30730\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07829, Exploration rate: 0.050\n",
            "Timestep: 366751, Episodes: 30740\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07416, Exploration rate: 0.050\n",
            "Timestep: 367251, Episodes: 30750\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08678, Exploration rate: 0.050\n",
            "Timestep: 367751, Episodes: 30760\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07701, Exploration rate: 0.050\n",
            "Timestep: 368251, Episodes: 30770\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08416, Exploration rate: 0.050\n",
            "Timestep: 368751, Episodes: 30780\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07387, Exploration rate: 0.050\n",
            "Timestep: 369251, Episodes: 30790\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10324, Exploration rate: 0.050\n",
            "Timestep: 369702, Episodes: 30800\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.08869, Exploration rate: 0.050\n",
            "Timestep: 370202, Episodes: 30810\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07611, Exploration rate: 0.050\n",
            "Timestep: 370702, Episodes: 30820\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07693, Exploration rate: 0.050\n",
            "Timestep: 371202, Episodes: 30830\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08940, Exploration rate: 0.050\n",
            "Timestep: 371702, Episodes: 30840\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08352, Exploration rate: 0.050\n",
            "Timestep: 372202, Episodes: 30850\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10633, Exploration rate: 0.050\n",
            "Timestep: 372702, Episodes: 30860\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08530, Exploration rate: 0.050\n",
            "Timestep: 373202, Episodes: 30870\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07718, Exploration rate: 0.050\n",
            "Timestep: 373702, Episodes: 30880\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08633, Exploration rate: 0.050\n",
            "Timestep: 374202, Episodes: 30890\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10902, Exploration rate: 0.050\n",
            "Timestep: 374702, Episodes: 30900\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09019, Exploration rate: 0.050\n",
            "Timestep: 375202, Episodes: 30910\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10811, Exploration rate: 0.050\n",
            "Timestep: 375702, Episodes: 30920\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11036, Exploration rate: 0.050\n",
            "Timestep: 376202, Episodes: 30930\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11021, Exploration rate: 0.050\n",
            "Timestep: 376658, Episodes: 30940\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.10274, Exploration rate: 0.050\n",
            "Timestep: 377158, Episodes: 30950\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11721, Exploration rate: 0.050\n",
            "Timestep: 377658, Episodes: 30960\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10722, Exploration rate: 0.050\n",
            "Timestep: 378158, Episodes: 30970\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11477, Exploration rate: 0.050\n",
            "Timestep: 378611, Episodes: 30980\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.10197, Exploration rate: 0.050\n",
            "Timestep: 379083, Episodes: 30990\n",
            "Mean reward: -47.10, Mean length: 47.20\n",
            "Mean loss: 0.11632, Exploration rate: 0.050\n",
            "Timestep: 379583, Episodes: 31000\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09260, Exploration rate: 0.050\n",
            "Timestep: 380083, Episodes: 31010\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11883, Exploration rate: 0.050\n",
            "Timestep: 380583, Episodes: 31020\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10661, Exploration rate: 0.050\n",
            "Timestep: 381083, Episodes: 31030\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10709, Exploration rate: 0.050\n",
            "Timestep: 381583, Episodes: 31040\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11332, Exploration rate: 0.050\n",
            "Timestep: 382074, Episodes: 31050\n",
            "Mean reward: -49.00, Mean length: 49.10\n",
            "Mean loss: 0.11821, Exploration rate: 0.050\n",
            "Timestep: 382574, Episodes: 31060\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10223, Exploration rate: 0.050\n",
            "Timestep: 383074, Episodes: 31070\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09872, Exploration rate: 0.050\n",
            "Timestep: 383528, Episodes: 31080\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.13060, Exploration rate: 0.050\n",
            "Timestep: 383984, Episodes: 31090\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.11171, Exploration rate: 0.050\n",
            "Timestep: 384484, Episodes: 31100\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08468, Exploration rate: 0.050\n",
            "Timestep: 384935, Episodes: 31110\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.10353, Exploration rate: 0.050\n",
            "Timestep: 385394, Episodes: 31120\n",
            "Mean reward: -45.80, Mean length: 45.90\n",
            "Mean loss: 0.11647, Exploration rate: 0.050\n",
            "Timestep: 385854, Episodes: 31130\n",
            "Mean reward: -45.90, Mean length: 46.00\n",
            "Mean loss: 0.11254, Exploration rate: 0.050\n",
            "Timestep: 386311, Episodes: 31140\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.11378, Exploration rate: 0.050\n",
            "Timestep: 386811, Episodes: 31150\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10987, Exploration rate: 0.050\n",
            "Timestep: 387279, Episodes: 31160\n",
            "Mean reward: -46.70, Mean length: 46.80\n",
            "Mean loss: 0.11384, Exploration rate: 0.050\n",
            "Timestep: 387779, Episodes: 31170\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11693, Exploration rate: 0.050\n",
            "Timestep: 388279, Episodes: 31180\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11265, Exploration rate: 0.050\n",
            "Timestep: 388779, Episodes: 31190\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12210, Exploration rate: 0.050\n",
            "Timestep: 389279, Episodes: 31200\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11062, Exploration rate: 0.050\n",
            "Timestep: 389779, Episodes: 31210\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10389, Exploration rate: 0.050\n",
            "Timestep: 390139, Episodes: 31220\n",
            "Mean reward: -35.70, Mean length: 36.00\n",
            "Mean loss: 0.11073, Exploration rate: 0.050\n",
            "Timestep: 390639, Episodes: 31230\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11626, Exploration rate: 0.050\n",
            "Timestep: 391139, Episodes: 31240\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10916, Exploration rate: 0.050\n",
            "Timestep: 391597, Episodes: 31250\n",
            "Mean reward: -45.70, Mean length: 45.80\n",
            "Mean loss: 0.12275, Exploration rate: 0.050\n",
            "Timestep: 392097, Episodes: 31260\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10429, Exploration rate: 0.050\n",
            "Timestep: 392597, Episodes: 31270\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11999, Exploration rate: 0.050\n",
            "Timestep: 393097, Episodes: 31280\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11529, Exploration rate: 0.050\n",
            "Timestep: 393597, Episodes: 31290\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12646, Exploration rate: 0.050\n",
            "Timestep: 394097, Episodes: 31300\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12715, Exploration rate: 0.050\n",
            "Timestep: 394567, Episodes: 31310\n",
            "Mean reward: -46.90, Mean length: 47.00\n",
            "Mean loss: 0.11276, Exploration rate: 0.050\n",
            "Timestep: 395067, Episodes: 31320\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12788, Exploration rate: 0.050\n",
            "Timestep: 395543, Episodes: 31330\n",
            "Mean reward: -47.50, Mean length: 47.60\n",
            "Mean loss: 0.11706, Exploration rate: 0.050\n",
            "Timestep: 395952, Episodes: 31340\n",
            "Mean reward: -40.70, Mean length: 40.90\n",
            "Mean loss: 0.10963, Exploration rate: 0.050\n",
            "Timestep: 396376, Episodes: 31350\n",
            "Mean reward: -42.20, Mean length: 42.40\n",
            "Mean loss: 0.11630, Exploration rate: 0.050\n",
            "Timestep: 396830, Episodes: 31360\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.12864, Exploration rate: 0.050\n",
            "Timestep: 397330, Episodes: 31370\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10245, Exploration rate: 0.050\n",
            "Timestep: 397830, Episodes: 31380\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11183, Exploration rate: 0.050\n",
            "Timestep: 398330, Episodes: 31390\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10897, Exploration rate: 0.050\n",
            "Timestep: 398741, Episodes: 31400\n",
            "Mean reward: -40.90, Mean length: 41.10\n",
            "Mean loss: 0.13172, Exploration rate: 0.050\n",
            "Timestep: 399241, Episodes: 31410\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12349, Exploration rate: 0.050\n",
            "Timestep: 399741, Episodes: 31420\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12565, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_400000.pth\n",
            "Evaluation: Mean reward: -35.60, mean episode length: 36.00\n",
            "Timestep: 400241, Episodes: 31430\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11670, Exploration rate: 0.050\n",
            "Timestep: 400741, Episodes: 31440\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.13762, Exploration rate: 0.050\n",
            "Timestep: 401223, Episodes: 31450\n",
            "Mean reward: -48.10, Mean length: 48.20\n",
            "Mean loss: 0.13849, Exploration rate: 0.050\n",
            "Timestep: 401723, Episodes: 31460\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11465, Exploration rate: 0.050\n",
            "Timestep: 402128, Episodes: 31470\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.12324, Exploration rate: 0.050\n",
            "Timestep: 402628, Episodes: 31480\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11876, Exploration rate: 0.050\n",
            "Timestep: 403128, Episodes: 31490\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11145, Exploration rate: 0.050\n",
            "Timestep: 403628, Episodes: 31500\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.13168, Exploration rate: 0.050\n",
            "Timestep: 404128, Episodes: 31510\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12114, Exploration rate: 0.050\n",
            "Timestep: 404579, Episodes: 31520\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.10626, Exploration rate: 0.050\n",
            "Timestep: 405036, Episodes: 31530\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.12995, Exploration rate: 0.050\n",
            "Timestep: 405536, Episodes: 31540\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.13066, Exploration rate: 0.050\n",
            "Timestep: 406036, Episodes: 31550\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12905, Exploration rate: 0.050\n",
            "Timestep: 406536, Episodes: 31560\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12430, Exploration rate: 0.050\n",
            "Timestep: 407014, Episodes: 31570\n",
            "Mean reward: -47.70, Mean length: 47.80\n",
            "Mean loss: 0.10552, Exploration rate: 0.050\n",
            "Timestep: 407483, Episodes: 31580\n",
            "Mean reward: -46.80, Mean length: 46.90\n",
            "Mean loss: 0.12917, Exploration rate: 0.050\n",
            "Timestep: 407939, Episodes: 31590\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.11133, Exploration rate: 0.050\n",
            "Timestep: 408406, Episodes: 31600\n",
            "Mean reward: -46.60, Mean length: 46.70\n",
            "Mean loss: 0.13998, Exploration rate: 0.050\n",
            "Timestep: 408778, Episodes: 31610\n",
            "Mean reward: -36.90, Mean length: 37.20\n",
            "Mean loss: 0.11696, Exploration rate: 0.050\n",
            "Timestep: 409243, Episodes: 31620\n",
            "Mean reward: -46.40, Mean length: 46.50\n",
            "Mean loss: 0.13470, Exploration rate: 0.050\n",
            "Timestep: 409743, Episodes: 31630\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11259, Exploration rate: 0.050\n",
            "Timestep: 410214, Episodes: 31640\n",
            "Mean reward: -47.00, Mean length: 47.10\n",
            "Mean loss: 0.10706, Exploration rate: 0.050\n",
            "Timestep: 410714, Episodes: 31650\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12492, Exploration rate: 0.050\n",
            "Timestep: 411214, Episodes: 31660\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.13357, Exploration rate: 0.050\n",
            "Timestep: 411714, Episodes: 31670\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12983, Exploration rate: 0.050\n",
            "Timestep: 412169, Episodes: 31680\n",
            "Mean reward: -45.40, Mean length: 45.50\n",
            "Mean loss: 0.12392, Exploration rate: 0.050\n",
            "Timestep: 412669, Episodes: 31690\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12832, Exploration rate: 0.050\n",
            "Timestep: 413169, Episodes: 31700\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12377, Exploration rate: 0.050\n",
            "Timestep: 413669, Episodes: 31710\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12423, Exploration rate: 0.050\n",
            "Timestep: 414169, Episodes: 31720\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11743, Exploration rate: 0.050\n",
            "Timestep: 414669, Episodes: 31730\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11054, Exploration rate: 0.050\n",
            "Timestep: 415123, Episodes: 31740\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.12082, Exploration rate: 0.050\n",
            "Timestep: 415623, Episodes: 31750\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12303, Exploration rate: 0.050\n",
            "Timestep: 416076, Episodes: 31760\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.11228, Exploration rate: 0.050\n",
            "Timestep: 416576, Episodes: 31770\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11018, Exploration rate: 0.050\n",
            "Timestep: 417034, Episodes: 31780\n",
            "Mean reward: -45.70, Mean length: 45.80\n",
            "Mean loss: 0.13752, Exploration rate: 0.050\n",
            "Timestep: 417534, Episodes: 31790\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.12351, Exploration rate: 0.050\n",
            "Timestep: 418034, Episodes: 31800\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11216, Exploration rate: 0.050\n",
            "Timestep: 418534, Episodes: 31810\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10067, Exploration rate: 0.050\n",
            "Timestep: 418987, Episodes: 31820\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.11848, Exploration rate: 0.050\n",
            "Timestep: 419487, Episodes: 31830\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11486, Exploration rate: 0.050\n",
            "Timestep: 419892, Episodes: 31840\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.11765, Exploration rate: 0.050\n",
            "Timestep: 420343, Episodes: 31850\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.10832, Exploration rate: 0.050\n",
            "Timestep: 420753, Episodes: 31860\n",
            "Mean reward: -40.70, Mean length: 41.00\n",
            "Mean loss: 0.12442, Exploration rate: 0.050\n",
            "Timestep: 421253, Episodes: 31870\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11229, Exploration rate: 0.050\n",
            "Timestep: 421738, Episodes: 31880\n",
            "Mean reward: -48.40, Mean length: 48.50\n",
            "Mean loss: 0.12507, Exploration rate: 0.050\n",
            "Timestep: 422238, Episodes: 31890\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10783, Exploration rate: 0.050\n",
            "Timestep: 422738, Episodes: 31900\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11114, Exploration rate: 0.050\n",
            "Timestep: 423238, Episodes: 31910\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10947, Exploration rate: 0.050\n",
            "Timestep: 423738, Episodes: 31920\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08882, Exploration rate: 0.050\n",
            "Timestep: 424238, Episodes: 31930\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10985, Exploration rate: 0.050\n",
            "Timestep: 424738, Episodes: 31940\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10907, Exploration rate: 0.050\n",
            "Timestep: 425238, Episodes: 31950\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11516, Exploration rate: 0.050\n",
            "Timestep: 425738, Episodes: 31960\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11842, Exploration rate: 0.050\n",
            "Timestep: 426238, Episodes: 31970\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10722, Exploration rate: 0.050\n",
            "Timestep: 426641, Episodes: 31980\n",
            "Mean reward: -40.00, Mean length: 40.30\n",
            "Mean loss: 0.11428, Exploration rate: 0.050\n",
            "Timestep: 427141, Episodes: 31990\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09347, Exploration rate: 0.050\n",
            "Timestep: 427641, Episodes: 32000\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10943, Exploration rate: 0.050\n",
            "Timestep: 428094, Episodes: 32010\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.11483, Exploration rate: 0.050\n",
            "Timestep: 428533, Episodes: 32020\n",
            "Mean reward: -43.70, Mean length: 43.90\n",
            "Mean loss: 0.09049, Exploration rate: 0.050\n",
            "Timestep: 428943, Episodes: 32030\n",
            "Mean reward: -40.70, Mean length: 41.00\n",
            "Mean loss: 0.11226, Exploration rate: 0.050\n",
            "Timestep: 429424, Episodes: 32040\n",
            "Mean reward: -48.00, Mean length: 48.10\n",
            "Mean loss: 0.08905, Exploration rate: 0.050\n",
            "Timestep: 429845, Episodes: 32050\n",
            "Mean reward: -41.90, Mean length: 42.10\n",
            "Mean loss: 0.09961, Exploration rate: 0.050\n",
            "Timestep: 430345, Episodes: 32060\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09817, Exploration rate: 0.050\n",
            "Timestep: 430817, Episodes: 32070\n",
            "Mean reward: -47.10, Mean length: 47.20\n",
            "Mean loss: 0.09878, Exploration rate: 0.050\n",
            "Timestep: 431317, Episodes: 32080\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09664, Exploration rate: 0.050\n",
            "Timestep: 431783, Episodes: 32090\n",
            "Mean reward: -46.50, Mean length: 46.60\n",
            "Mean loss: 0.09171, Exploration rate: 0.050\n",
            "Timestep: 432283, Episodes: 32100\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10631, Exploration rate: 0.050\n",
            "Timestep: 432757, Episodes: 32110\n",
            "Mean reward: -47.30, Mean length: 47.40\n",
            "Mean loss: 0.11005, Exploration rate: 0.050\n",
            "Timestep: 433247, Episodes: 32120\n",
            "Mean reward: -48.90, Mean length: 49.00\n",
            "Mean loss: 0.09993, Exploration rate: 0.050\n",
            "Timestep: 433747, Episodes: 32130\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09595, Exploration rate: 0.050\n",
            "Timestep: 434247, Episodes: 32140\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09701, Exploration rate: 0.050\n",
            "Timestep: 434698, Episodes: 32150\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.11026, Exploration rate: 0.050\n",
            "Timestep: 435119, Episodes: 32160\n",
            "Mean reward: -41.90, Mean length: 42.10\n",
            "Mean loss: 0.11655, Exploration rate: 0.050\n",
            "Timestep: 435619, Episodes: 32170\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10734, Exploration rate: 0.050\n",
            "Timestep: 436119, Episodes: 32180\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.11229, Exploration rate: 0.050\n",
            "Timestep: 436575, Episodes: 32190\n",
            "Mean reward: -45.50, Mean length: 45.60\n",
            "Mean loss: 0.11610, Exploration rate: 0.050\n",
            "Timestep: 437075, Episodes: 32200\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09276, Exploration rate: 0.050\n",
            "Timestep: 437509, Episodes: 32210\n",
            "Mean reward: -43.20, Mean length: 43.40\n",
            "Mean loss: 0.09680, Exploration rate: 0.050\n",
            "Timestep: 438001, Episodes: 32220\n",
            "Mean reward: -49.10, Mean length: 49.20\n",
            "Mean loss: 0.09869, Exploration rate: 0.050\n",
            "Timestep: 438454, Episodes: 32230\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.09061, Exploration rate: 0.050\n",
            "Timestep: 438861, Episodes: 32240\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.11255, Exploration rate: 0.050\n",
            "Timestep: 439361, Episodes: 32250\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09980, Exploration rate: 0.050\n",
            "Timestep: 439861, Episodes: 32260\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09959, Exploration rate: 0.050\n",
            "Timestep: 440355, Episodes: 32270\n",
            "Mean reward: -49.30, Mean length: 49.40\n",
            "Mean loss: 0.08608, Exploration rate: 0.050\n",
            "Timestep: 440776, Episodes: 32280\n",
            "Mean reward: -41.90, Mean length: 42.10\n",
            "Mean loss: 0.07533, Exploration rate: 0.050\n",
            "Timestep: 441251, Episodes: 32290\n",
            "Mean reward: -47.40, Mean length: 47.50\n",
            "Mean loss: 0.11617, Exploration rate: 0.050\n",
            "Timestep: 441751, Episodes: 32300\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10163, Exploration rate: 0.050\n",
            "Timestep: 442203, Episodes: 32310\n",
            "Mean reward: -45.00, Mean length: 45.20\n",
            "Mean loss: 0.07022, Exploration rate: 0.050\n",
            "Timestep: 442703, Episodes: 32320\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09653, Exploration rate: 0.050\n",
            "Timestep: 443203, Episodes: 32330\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10111, Exploration rate: 0.050\n",
            "Timestep: 443658, Episodes: 32340\n",
            "Mean reward: -45.40, Mean length: 45.50\n",
            "Mean loss: 0.08135, Exploration rate: 0.050\n",
            "Timestep: 444158, Episodes: 32350\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09206, Exploration rate: 0.050\n",
            "Timestep: 444658, Episodes: 32360\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10786, Exploration rate: 0.050\n",
            "Timestep: 445115, Episodes: 32370\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.08440, Exploration rate: 0.050\n",
            "Timestep: 445615, Episodes: 32380\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08572, Exploration rate: 0.050\n",
            "Timestep: 446115, Episodes: 32390\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07679, Exploration rate: 0.050\n",
            "Timestep: 446531, Episodes: 32400\n",
            "Mean reward: -41.40, Mean length: 41.60\n",
            "Mean loss: 0.08803, Exploration rate: 0.050\n",
            "Timestep: 447031, Episodes: 32410\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07870, Exploration rate: 0.050\n",
            "Timestep: 447484, Episodes: 32420\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.09314, Exploration rate: 0.050\n",
            "Timestep: 447984, Episodes: 32430\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08426, Exploration rate: 0.050\n",
            "Timestep: 448393, Episodes: 32440\n",
            "Mean reward: -40.70, Mean length: 40.90\n",
            "Mean loss: 0.09554, Exploration rate: 0.050\n",
            "Timestep: 448850, Episodes: 32450\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.10555, Exploration rate: 0.050\n",
            "Timestep: 449327, Episodes: 32460\n",
            "Mean reward: -47.60, Mean length: 47.70\n",
            "Mean loss: 0.09233, Exploration rate: 0.050\n",
            "Timestep: 449815, Episodes: 32470\n",
            "Mean reward: -48.70, Mean length: 48.80\n",
            "Mean loss: 0.08866, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_450000.pth\n",
            "Evaluation: Mean reward: -39.40, mean episode length: 39.70\n",
            "Timestep: 450312, Episodes: 32480\n",
            "Mean reward: -49.60, Mean length: 49.70\n",
            "Mean loss: 0.09130, Exploration rate: 0.050\n",
            "Timestep: 450812, Episodes: 32490\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07906, Exploration rate: 0.050\n",
            "Timestep: 451312, Episodes: 32500\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.10104, Exploration rate: 0.050\n",
            "Timestep: 451812, Episodes: 32510\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09387, Exploration rate: 0.050\n",
            "Timestep: 452312, Episodes: 32520\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07795, Exploration rate: 0.050\n",
            "Timestep: 452722, Episodes: 32530\n",
            "Mean reward: -40.80, Mean length: 41.00\n",
            "Mean loss: 0.10329, Exploration rate: 0.050\n",
            "Timestep: 453127, Episodes: 32540\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.09639, Exploration rate: 0.050\n",
            "Timestep: 453578, Episodes: 32550\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.09016, Exploration rate: 0.050\n",
            "Timestep: 454029, Episodes: 32560\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.08758, Exploration rate: 0.050\n",
            "Timestep: 454492, Episodes: 32570\n",
            "Mean reward: -46.20, Mean length: 46.30\n",
            "Mean loss: 0.08714, Exploration rate: 0.050\n",
            "Timestep: 454992, Episodes: 32580\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09286, Exploration rate: 0.050\n",
            "Timestep: 455443, Episodes: 32590\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.08358, Exploration rate: 0.050\n",
            "Timestep: 455883, Episodes: 32600\n",
            "Mean reward: -43.80, Mean length: 44.00\n",
            "Mean loss: 0.08078, Exploration rate: 0.050\n",
            "Timestep: 456383, Episodes: 32610\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09918, Exploration rate: 0.050\n",
            "Timestep: 456883, Episodes: 32620\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07300, Exploration rate: 0.050\n",
            "Timestep: 457383, Episodes: 32630\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08334, Exploration rate: 0.050\n",
            "Timestep: 457859, Episodes: 32640\n",
            "Mean reward: -47.50, Mean length: 47.60\n",
            "Mean loss: 0.08318, Exploration rate: 0.050\n",
            "Timestep: 458276, Episodes: 32650\n",
            "Mean reward: -41.50, Mean length: 41.70\n",
            "Mean loss: 0.09090, Exploration rate: 0.050\n",
            "Timestep: 458759, Episodes: 32660\n",
            "Mean reward: -48.20, Mean length: 48.30\n",
            "Mean loss: 0.08553, Exploration rate: 0.050\n",
            "Timestep: 459259, Episodes: 32670\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08081, Exploration rate: 0.050\n",
            "Timestep: 459711, Episodes: 32680\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.06532, Exploration rate: 0.050\n",
            "Timestep: 460168, Episodes: 32690\n",
            "Mean reward: -45.60, Mean length: 45.70\n",
            "Mean loss: 0.08392, Exploration rate: 0.050\n",
            "Timestep: 460668, Episodes: 32700\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06901, Exploration rate: 0.050\n",
            "Timestep: 461130, Episodes: 32710\n",
            "Mean reward: -46.10, Mean length: 46.20\n",
            "Mean loss: 0.08628, Exploration rate: 0.050\n",
            "Timestep: 461630, Episodes: 32720\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.09428, Exploration rate: 0.050\n",
            "Timestep: 462082, Episodes: 32730\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.08302, Exploration rate: 0.050\n",
            "Timestep: 462476, Episodes: 32740\n",
            "Mean reward: -39.10, Mean length: 39.40\n",
            "Mean loss: 0.06997, Exploration rate: 0.050\n",
            "Timestep: 462976, Episodes: 32750\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08002, Exploration rate: 0.050\n",
            "Timestep: 463476, Episodes: 32760\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08409, Exploration rate: 0.050\n",
            "Timestep: 463976, Episodes: 32770\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06685, Exploration rate: 0.050\n",
            "Timestep: 464427, Episodes: 32780\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.07292, Exploration rate: 0.050\n",
            "Timestep: 464927, Episodes: 32790\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.08657, Exploration rate: 0.050\n",
            "Timestep: 465387, Episodes: 32800\n",
            "Mean reward: -45.90, Mean length: 46.00\n",
            "Mean loss: 0.08140, Exploration rate: 0.050\n",
            "Timestep: 465840, Episodes: 32810\n",
            "Mean reward: -45.10, Mean length: 45.30\n",
            "Mean loss: 0.07218, Exploration rate: 0.050\n",
            "Timestep: 466307, Episodes: 32820\n",
            "Mean reward: -46.60, Mean length: 46.70\n",
            "Mean loss: 0.06014, Exploration rate: 0.050\n",
            "Timestep: 466775, Episodes: 32830\n",
            "Mean reward: -46.70, Mean length: 46.80\n",
            "Mean loss: 0.07167, Exploration rate: 0.050\n",
            "Timestep: 467191, Episodes: 32840\n",
            "Mean reward: -41.40, Mean length: 41.60\n",
            "Mean loss: 0.07698, Exploration rate: 0.050\n",
            "Timestep: 467579, Episodes: 32850\n",
            "Mean reward: -38.50, Mean length: 38.80\n",
            "Mean loss: 0.07652, Exploration rate: 0.050\n",
            "Timestep: 468079, Episodes: 32860\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05706, Exploration rate: 0.050\n",
            "Timestep: 468565, Episodes: 32870\n",
            "Mean reward: -48.50, Mean length: 48.60\n",
            "Mean loss: 0.06741, Exploration rate: 0.050\n",
            "Timestep: 469018, Episodes: 32880\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.07872, Exploration rate: 0.050\n",
            "Timestep: 469422, Episodes: 32890\n",
            "Mean reward: -40.20, Mean length: 40.40\n",
            "Mean loss: 0.07771, Exploration rate: 0.050\n",
            "Timestep: 469922, Episodes: 32900\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06986, Exploration rate: 0.050\n",
            "Timestep: 470281, Episodes: 32910\n",
            "Mean reward: -35.60, Mean length: 35.90\n",
            "Mean loss: 0.05939, Exploration rate: 0.050\n",
            "Timestep: 470781, Episodes: 32920\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05834, Exploration rate: 0.050\n",
            "Timestep: 471214, Episodes: 32930\n",
            "Mean reward: -43.10, Mean length: 43.30\n",
            "Mean loss: 0.06678, Exploration rate: 0.050\n",
            "Timestep: 471636, Episodes: 32940\n",
            "Mean reward: -42.00, Mean length: 42.20\n",
            "Mean loss: 0.04972, Exploration rate: 0.050\n",
            "Timestep: 472136, Episodes: 32950\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06690, Exploration rate: 0.050\n",
            "Timestep: 472543, Episodes: 32960\n",
            "Mean reward: -40.50, Mean length: 40.70\n",
            "Mean loss: 0.06509, Exploration rate: 0.050\n",
            "Timestep: 473043, Episodes: 32970\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06700, Exploration rate: 0.050\n",
            "Timestep: 473543, Episodes: 32980\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.07020, Exploration rate: 0.050\n",
            "Timestep: 473998, Episodes: 32990\n",
            "Mean reward: -45.40, Mean length: 45.50\n",
            "Mean loss: 0.06591, Exploration rate: 0.050\n",
            "Timestep: 474451, Episodes: 33000\n",
            "Mean reward: -45.20, Mean length: 45.30\n",
            "Mean loss: 0.06781, Exploration rate: 0.050\n",
            "Timestep: 474889, Episodes: 33010\n",
            "Mean reward: -43.60, Mean length: 43.80\n",
            "Mean loss: 0.07399, Exploration rate: 0.050\n",
            "Timestep: 475389, Episodes: 33020\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06629, Exploration rate: 0.050\n",
            "Timestep: 475889, Episodes: 33030\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06188, Exploration rate: 0.050\n",
            "Timestep: 476389, Episodes: 33040\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05406, Exploration rate: 0.050\n",
            "Timestep: 476844, Episodes: 33050\n",
            "Mean reward: -45.40, Mean length: 45.50\n",
            "Mean loss: 0.06741, Exploration rate: 0.050\n",
            "Timestep: 477298, Episodes: 33060\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.06287, Exploration rate: 0.050\n",
            "Timestep: 477798, Episodes: 33070\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05895, Exploration rate: 0.050\n",
            "Timestep: 478218, Episodes: 33080\n",
            "Mean reward: -41.80, Mean length: 42.00\n",
            "Mean loss: 0.06203, Exploration rate: 0.050\n",
            "Timestep: 478718, Episodes: 33090\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04869, Exploration rate: 0.050\n",
            "Timestep: 479172, Episodes: 33100\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.05862, Exploration rate: 0.050\n",
            "Timestep: 479611, Episodes: 33110\n",
            "Mean reward: -43.70, Mean length: 43.90\n",
            "Mean loss: 0.06352, Exploration rate: 0.050\n",
            "Timestep: 480062, Episodes: 33120\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.05216, Exploration rate: 0.050\n",
            "Timestep: 480562, Episodes: 33130\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05520, Exploration rate: 0.050\n",
            "Timestep: 481062, Episodes: 33140\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05619, Exploration rate: 0.050\n",
            "Timestep: 481562, Episodes: 33150\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05365, Exploration rate: 0.050\n",
            "Timestep: 482062, Episodes: 33160\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05882, Exploration rate: 0.050\n",
            "Timestep: 482562, Episodes: 33170\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05537, Exploration rate: 0.050\n",
            "Timestep: 483013, Episodes: 33180\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.05779, Exploration rate: 0.050\n",
            "Timestep: 483513, Episodes: 33190\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05252, Exploration rate: 0.050\n",
            "Timestep: 484013, Episodes: 33200\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05965, Exploration rate: 0.050\n",
            "Timestep: 484413, Episodes: 33210\n",
            "Mean reward: -39.70, Mean length: 40.00\n",
            "Mean loss: 0.04427, Exploration rate: 0.050\n",
            "Timestep: 484913, Episodes: 33220\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05685, Exploration rate: 0.050\n",
            "Timestep: 485413, Episodes: 33230\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06239, Exploration rate: 0.050\n",
            "Timestep: 485796, Episodes: 33240\n",
            "Mean reward: -38.00, Mean length: 38.30\n",
            "Mean loss: 0.04990, Exploration rate: 0.050\n",
            "Timestep: 486163, Episodes: 33250\n",
            "Mean reward: -36.40, Mean length: 36.70\n",
            "Mean loss: 0.05777, Exploration rate: 0.050\n",
            "Timestep: 486663, Episodes: 33260\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06965, Exploration rate: 0.050\n",
            "Timestep: 487163, Episodes: 33270\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04709, Exploration rate: 0.050\n",
            "Timestep: 487663, Episodes: 33280\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05931, Exploration rate: 0.050\n",
            "Timestep: 488117, Episodes: 33290\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.05050, Exploration rate: 0.050\n",
            "Timestep: 488617, Episodes: 33300\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05945, Exploration rate: 0.050\n",
            "Timestep: 489117, Episodes: 33310\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04141, Exploration rate: 0.050\n",
            "Timestep: 489617, Episodes: 33320\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05039, Exploration rate: 0.050\n",
            "Timestep: 490117, Episodes: 33330\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.03773, Exploration rate: 0.050\n",
            "Timestep: 490617, Episodes: 33340\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05342, Exploration rate: 0.050\n",
            "Timestep: 491117, Episodes: 33350\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04283, Exploration rate: 0.050\n",
            "Timestep: 491571, Episodes: 33360\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.05620, Exploration rate: 0.050\n",
            "Timestep: 492071, Episodes: 33370\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04950, Exploration rate: 0.050\n",
            "Timestep: 492525, Episodes: 33380\n",
            "Mean reward: -45.30, Mean length: 45.40\n",
            "Mean loss: 0.05086, Exploration rate: 0.050\n",
            "Timestep: 492969, Episodes: 33390\n",
            "Mean reward: -44.20, Mean length: 44.40\n",
            "Mean loss: 0.04682, Exploration rate: 0.050\n",
            "Timestep: 493434, Episodes: 33400\n",
            "Mean reward: -46.40, Mean length: 46.50\n",
            "Mean loss: 0.05106, Exploration rate: 0.050\n",
            "Timestep: 493934, Episodes: 33410\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04192, Exploration rate: 0.050\n",
            "Timestep: 494434, Episodes: 33420\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05099, Exploration rate: 0.050\n",
            "Timestep: 494934, Episodes: 33430\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04050, Exploration rate: 0.050\n",
            "Timestep: 495434, Episodes: 33440\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.06053, Exploration rate: 0.050\n",
            "Timestep: 495934, Episodes: 33450\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04803, Exploration rate: 0.050\n",
            "Timestep: 496434, Episodes: 33460\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05430, Exploration rate: 0.050\n",
            "Timestep: 496934, Episodes: 33470\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.05205, Exploration rate: 0.050\n",
            "Timestep: 497434, Episodes: 33480\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04765, Exploration rate: 0.050\n",
            "Timestep: 497934, Episodes: 33490\n",
            "Mean reward: -50.00, Mean length: 50.00\n",
            "Mean loss: 0.04535, Exploration rate: 0.050\n",
            "Timestep: 498385, Episodes: 33500\n",
            "Mean reward: -45.00, Mean length: 45.10\n",
            "Mean loss: 0.04362, Exploration rate: 0.050\n",
            "Timestep: 498882, Episodes: 33510\n",
            "Mean reward: -49.60, Mean length: 49.70\n",
            "Mean loss: 0.04708, Exploration rate: 0.050\n",
            "Timestep: 499287, Episodes: 33520\n",
            "Mean reward: -40.30, Mean length: 40.50\n",
            "Mean loss: 0.04845, Exploration rate: 0.050\n",
            "Timestep: 499739, Episodes: 33530\n",
            "Mean reward: -45.10, Mean length: 45.20\n",
            "Mean loss: 0.05112, Exploration rate: 0.050\n",
            "Model saved to dqn_models/PandaReach-v3_500000.pth\n",
            "Model saved to dqn_models/PandaReach-v3_final.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import panda_gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Type, Union, Optional, Any, Callable\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                     \"mps\" if torch.backends.mps.is_available() else\n",
        "                     \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, dtype=np.float64)\n",
        "        self.var = np.ones(shape, dtype=np.float64)\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - self.mean\n",
        "        tot_count = self.count + batch_count\n",
        "\n",
        "        new_mean = self.mean + delta * batch_count / tot_count\n",
        "        m_a = self.var * self.count\n",
        "        m_b = batch_var * batch_count\n",
        "        m_2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
        "        new_var = m_2 / tot_count\n",
        "\n",
        "        self.mean = new_mean\n",
        "        self.var = new_var\n",
        "        self.count = tot_count\n",
        "\n",
        "class VecNormalize:\n",
        "    def __init__(self, obs_rms=None, ret_rms=None, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "        self.obs_rms = obs_rms or RunningMeanStd(shape=(1,))\n",
        "        self.ret_rms = ret_rms or RunningMeanStd(shape=())\n",
        "        self.clipob = clipob\n",
        "        self.cliprew = cliprew\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.returns = 0.0\n",
        "\n",
        "    def normalize_obs(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            normalized_obs = {}\n",
        "            for key, value in obs.items():\n",
        "                if key in [\"observation\", \"desired_goal\"]:\n",
        "                    normalized_obs[key] = self._normalize_obs(value)\n",
        "                else:\n",
        "                    normalized_obs[key] = value\n",
        "            return normalized_obs\n",
        "        else:\n",
        "            return self._normalize_obs(obs)\n",
        "\n",
        "    def _normalize_obs(self, obs):\n",
        "        obs_array = np.array(obs)\n",
        "        self.obs_rms.update(obs_array)\n",
        "        return np.clip((obs_array - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon),\n",
        "                       -self.clipob, self.clipob)\n",
        "\n",
        "    def normalize_reward(self, reward):\n",
        "        self.returns = self.returns * self.gamma + reward\n",
        "        self.ret_rms.update(np.array([self.returns]))\n",
        "        return np.clip(reward / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "\n",
        "    def reset_returns(self):\n",
        "        self.returns = 0.0\n",
        "\n",
        "# NN for DQN\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim, goal_dim, action_dim, net_arch=[256, 256], activation_fn=nn.ReLU):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.input_dim = obs_dim + goal_dim\n",
        "        self.layers = nn.ModuleList()\n",
        "        last_dim = self.input_dim\n",
        "\n",
        "        for dim in net_arch:\n",
        "            self.layers.append(nn.Linear(last_dim, dim))\n",
        "            self.layers.append(activation_fn())\n",
        "            last_dim = dim\n",
        "\n",
        "        self.q_layer = nn.Linear(last_dim, action_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.orthogonal_(module.weight, gain=1)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        q_values = self.q_layer(x)\n",
        "        return q_values\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, obs_dim, goal_dim, action_dim):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.observations = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
        "        self.goals = np.zeros((buffer_size, goal_dim), dtype=np.float32)\n",
        "        self.actions = np.zeros(buffer_size, dtype=np.int64)\n",
        "        self.rewards = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.next_observations = np.zeros((buffer_size, obs_dim), dtype=np.float32)\n",
        "        self.next_goals = np.zeros((buffer_size, goal_dim), dtype=np.float32)\n",
        "        self.dones = np.zeros(buffer_size, dtype=np.float32)\n",
        "\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "\n",
        "    def add(self, obs, goal, action, reward, next_obs, next_goal, done):\n",
        "        self.observations[self.pos] = obs\n",
        "        self.goals[self.pos] = goal\n",
        "        self.actions[self.pos] = action\n",
        "        self.rewards[self.pos] = reward\n",
        "        self.next_observations[self.pos] = next_obs\n",
        "        self.next_goals[self.pos] = next_goal\n",
        "        self.dones[self.pos] = done\n",
        "\n",
        "        self.pos += 1\n",
        "        if self.pos == self.buffer_size:\n",
        "            self.full = True\n",
        "            self.pos = 0\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_size = self.buffer_size if self.full else self.pos\n",
        "        batch_indices = np.random.randint(0, max_size, size=batch_size)\n",
        "\n",
        "        obs_batch = self.observations[batch_indices]\n",
        "        goal_batch = self.goals[batch_indices]\n",
        "        action_batch = self.actions[batch_indices]\n",
        "        reward_batch = self.rewards[batch_indices]\n",
        "        next_obs_batch = self.next_observations[batch_indices]\n",
        "        next_goal_batch = self.next_goals[batch_indices]\n",
        "        done_batch = self.dones[batch_indices]\n",
        "\n",
        "        return (\n",
        "            obs_batch,\n",
        "            goal_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_obs_batch,\n",
        "            next_goal_batch,\n",
        "            done_batch\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.buffer_size if self.full else self.pos\n",
        "\n",
        "# DQN Agent\n",
        "class DQN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate=1e-4,\n",
        "        buffer_size=100000,\n",
        "        batch_size=64,\n",
        "        target_update_freq=1000,\n",
        "        gamma=0.99,\n",
        "        tau=1.0,\n",
        "        eps_start=1.0,\n",
        "        eps_end=0.05,\n",
        "        eps_decay=50000,\n",
        "        double_q=True,\n",
        "        exploration_fraction=0.1,\n",
        "        policy_kwargs=None,\n",
        "        device=device\n",
        "    ):\n",
        "        # Env info\n",
        "        self.observation_space = env.observation_space\n",
        "        self.action_space = env.action_space\n",
        "        self.env = env\n",
        "\n",
        "        self.obs_dim = env.observation_space['observation'].shape[0]\n",
        "        self.goal_dim = env.observation_space['desired_goal'].shape[0]\n",
        "\n",
        "        # Discretize continuous action space\n",
        "        self.actions_per_dim = 5\n",
        "        self.action_dim = self.actions_per_dim ** env.action_space.shape[0]\n",
        "\n",
        "        # Discrete action to continuous action\n",
        "        self.action_map = self._create_action_map(env.action_space.shape[0], self.actions_per_dim,\n",
        "                                                 env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.double_q = double_q\n",
        "\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "        self.epsilon = eps_start\n",
        "\n",
        "        policy_kwargs = policy_kwargs or {}\n",
        "        self.q_network = QNetwork(\n",
        "            self.obs_dim,\n",
        "            self.goal_dim,\n",
        "            self.action_dim,\n",
        "            net_arch=policy_kwargs.get(\"net_arch\", [256, 256])\n",
        "        )\n",
        "        self.target_q_network = QNetwork(\n",
        "            self.obs_dim,\n",
        "            self.goal_dim,\n",
        "            self.action_dim,\n",
        "            net_arch=policy_kwargs.get(\"net_arch\", [256, 256])\n",
        "        )\n",
        "\n",
        "        self.q_network.to(device)\n",
        "        self.target_q_network.to(device)\n",
        "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size, self.obs_dim, self.goal_dim, 1)\n",
        "        self.normalizer = VecNormalize(gamma=gamma)\n",
        "\n",
        "        self.timesteps = 0\n",
        "        self.logger = {\n",
        "            \"timesteps\": [],\n",
        "            \"mean_reward\": [],\n",
        "            \"episode_length\": [],\n",
        "            \"loss\": []\n",
        "        }\n",
        "\n",
        "        self.callbacks = []\n",
        "        self.loss_history = deque(maxlen=100)\n",
        "\n",
        "    def _create_action_map(self, action_dims, actions_per_dim, low, high):\n",
        "        dim_values = []\n",
        "        for dim in range(action_dims):\n",
        "            values = np.linspace(low[dim], high[dim], actions_per_dim)\n",
        "            dim_values.append(values)\n",
        "\n",
        "        action_map = []\n",
        "        for idx in range(actions_per_dim ** action_dims):\n",
        "            action = []\n",
        "            temp_idx = idx\n",
        "            for dim in range(action_dims):\n",
        "                action_idx = temp_idx % actions_per_dim\n",
        "                temp_idx = temp_idx // actions_per_dim\n",
        "                action.append(dim_values[dim][action_idx])\n",
        "            action_map.append(np.array(action))\n",
        "\n",
        "        return action_map\n",
        "\n",
        "    def _process_observation(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            observation = np.array(obs[\"observation\"], dtype=np.float32)\n",
        "            goal = np.array(obs[\"desired_goal\"], dtype=np.float32)\n",
        "        else:\n",
        "            observation = obs[:self.obs_dim]\n",
        "            goal = obs[self.obs_dim:]\n",
        "\n",
        "        return observation, goal\n",
        "\n",
        "    def _combined_input(self, obs, goal):\n",
        "        if isinstance(obs, np.ndarray) and isinstance(goal, np.ndarray):\n",
        "            return np.concatenate([obs, goal], axis=-1)\n",
        "        else:\n",
        "            return torch.cat([obs, goal], dim=-1)\n",
        "\n",
        "    def _normalize_observation(self, obs):\n",
        "        if isinstance(obs, dict):\n",
        "            return self.normalizer.normalize_obs(obs)\n",
        "        return obs\n",
        "\n",
        "    def _normalize_reward(self, reward):\n",
        "        return self.normalizer.normalize_reward(reward)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        fraction = min(1.0, self.timesteps / self.eps_decay)\n",
        "        self.epsilon = self.eps_start + fraction * (self.eps_end - self.eps_start)\n",
        "\n",
        "    def predict(self, observation, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            observation = self._normalize_observation(observation)\n",
        "            obs_array, goal_array = self._process_observation(observation)\n",
        "\n",
        "            obs_tensor = torch.FloatTensor(obs_array).unsqueeze(0).to(device)\n",
        "            goal_tensor = torch.FloatTensor(goal_array).unsqueeze(0).to(device)\n",
        "\n",
        "            x = self._combined_input(obs_tensor, goal_tensor)\n",
        "\n",
        "            # Epsilon-greedy action\n",
        "            if deterministic or np.random.random() > self.epsilon:\n",
        "                # Greedy\n",
        "                q_values = self.q_network(x)\n",
        "                action_idx = q_values.argmax(dim=1).item()\n",
        "            else:\n",
        "                # Random\n",
        "                action_idx = np.random.randint(0, self.action_dim)\n",
        "\n",
        "            # Discrete action to continuous using action map\n",
        "            action = self.action_map[action_idx]\n",
        "\n",
        "            return action, action_idx\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return 0.0 \n",
        "\n",
        "        (\n",
        "            obs_batch,\n",
        "            goal_batch,\n",
        "            action_batch,\n",
        "            reward_batch,\n",
        "            next_obs_batch,\n",
        "            next_goal_batch,\n",
        "            done_batch\n",
        "        ) = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        obs_tensor = torch.FloatTensor(obs_batch).to(device)\n",
        "        goal_tensor = torch.FloatTensor(goal_batch).to(device)\n",
        "        action_tensor = torch.LongTensor(action_batch).to(device)\n",
        "        reward_tensor = torch.FloatTensor(reward_batch).to(device)\n",
        "        next_obs_tensor = torch.FloatTensor(next_obs_batch).to(device)\n",
        "        next_goal_tensor = torch.FloatTensor(next_goal_batch).to(device)\n",
        "        done_tensor = torch.FloatTensor(done_batch).to(device)\n",
        "\n",
        "        state_tensor = self._combined_input(obs_tensor, goal_tensor)\n",
        "        next_state_tensor = self._combined_input(next_obs_tensor, next_goal_tensor)\n",
        "\n",
        "        current_q_values = self.q_network(state_tensor).gather(1, action_tensor.unsqueeze(1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.double_q:\n",
        "                # Double DQN\n",
        "                next_q_values_online = self.q_network(next_state_tensor)\n",
        "                next_actions = next_q_values_online.argmax(dim=1, keepdim=True)\n",
        "                next_q_values = self.target_q_network(next_state_tensor).gather(1, next_actions)\n",
        "            else:\n",
        "                # Regular DQN\n",
        "                next_q_values = self.target_q_network(next_state_tensor).max(1, keepdim=True)[0]\n",
        "\n",
        "            # Bellman equation\n",
        "            target_q_values = reward_tensor.unsqueeze(1) + (1 - done_tensor.unsqueeze(1)) * self.gamma * next_q_values\n",
        "\n",
        "        # Loss\n",
        "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.loss_history.append(loss.item())\n",
        "\n",
        "        if self.timesteps % self.target_update_freq == 0:\n",
        "            if self.tau >= 1.0:\n",
        "                # Hard update\n",
        "                self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "            else:\n",
        "                # Soft update\n",
        "                for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
        "                    target_param.data.copy_(\n",
        "                        self.tau * param.data + (1 - self.tau) * target_param.data\n",
        "                    )\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def learn(self, total_timesteps, callback=None, log_interval=100,\n",
        "              train_freq=4, eval_env=None, eval_freq=10000, n_eval_episodes=5):\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        current_episode_reward = 0\n",
        "        current_episode_length = 0\n",
        "\n",
        "        obs, _ = self.env.reset()\n",
        "        all_losses = []\n",
        "\n",
        "        while self.timesteps < total_timesteps:\n",
        "            action, action_idx = self.predict(obs)\n",
        "\n",
        "            next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            current_episode_reward += reward\n",
        "            current_episode_length += 1\n",
        "\n",
        "            obs_array, goal_array = self._process_observation(self._normalize_observation(obs))\n",
        "            next_obs_array, next_goal_array = self._process_observation(self._normalize_observation(next_obs))\n",
        "            norm_reward = self._normalize_reward(reward)\n",
        "\n",
        "            self.replay_buffer.add(\n",
        "                obs_array,\n",
        "                goal_array,\n",
        "                action_idx, \n",
        "                norm_reward,\n",
        "                next_obs_array,\n",
        "                next_goal_array,\n",
        "                float(done)\n",
        "            )\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            if self.timesteps % train_freq == 0:\n",
        "                loss = self.train_step()\n",
        "                if loss is not None: \n",
        "                    all_losses.append(loss)\n",
        "\n",
        "            self.update_epsilon()\n",
        "\n",
        "            if done:\n",
        "                episode_rewards.append(current_episode_reward)\n",
        "                episode_lengths.append(current_episode_length)\n",
        "\n",
        "                if len(episode_rewards) % log_interval == 0:\n",
        "                    mean_reward = np.mean(episode_rewards[-log_interval:])\n",
        "                    mean_length = np.mean(episode_lengths[-log_interval:])\n",
        "                    mean_loss = np.mean([l for l in all_losses[-log_interval*train_freq:] if l is not None]) if all_losses else 0\n",
        "\n",
        "                    print(f\"Timestep: {self.timesteps}, Episodes: {len(episode_rewards)}\")\n",
        "                    print(f\"Mean reward: {mean_reward:.2f}, Mean length: {mean_length:.2f}\")\n",
        "                    print(f\"Mean loss: {mean_loss:.5f}, Exploration rate: {self.epsilon:.3f}\")\n",
        "\n",
        "                    # Store \n",
        "                    self.logger[\"timesteps\"].append(self.timesteps)\n",
        "                    self.logger[\"mean_reward\"].append(mean_reward)\n",
        "                    self.logger[\"episode_length\"].append(mean_length)\n",
        "                    self.logger[\"loss\"].append(mean_loss)\n",
        "\n",
        "                current_episode_reward = 0\n",
        "                current_episode_length = 0\n",
        "\n",
        "                obs, _ = self.env.reset()\n",
        "                self.normalizer.reset_returns()\n",
        "\n",
        "            if eval_env is not None and eval_freq is not None and self.timesteps % eval_freq == 0:\n",
        "                self.evaluate_policy(eval_env, n_eval_episodes)\n",
        "\n",
        "            self.timesteps += 1\n",
        "\n",
        "            if callback is not None:\n",
        "                callback(locals(), globals())\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate_policy(self, env=None, n_eval_episodes=10, deterministic=True):\n",
        "        eval_env = env or self.env\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for _ in range(n_eval_episodes):\n",
        "            obs, _ = eval_env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "\n",
        "            while not done:\n",
        "                action, _ = self.predict(obs, deterministic=deterministic)\n",
        "                obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "\n",
        "        mean_reward = np.mean(episode_rewards)\n",
        "        mean_length = np.mean(episode_lengths)\n",
        "\n",
        "        print(f\"Evaluation: Mean reward: {mean_reward:.2f}, mean episode length: {mean_length:.2f}\")\n",
        "\n",
        "        return mean_reward, mean_length\n",
        "\n",
        "    def save(self, path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save({\n",
        "            \"q_network_state_dict\": self.q_network.state_dict(),\n",
        "            \"target_q_network_state_dict\": self.target_q_network.state_dict(),\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"normalizer_mean\": self.normalizer.obs_rms.mean,\n",
        "            \"normalizer_var\": self.normalizer.obs_rms.var,\n",
        "            \"normalizer_count\": self.normalizer.obs_rms.count,\n",
        "            \"ret_rms_mean\": self.normalizer.ret_rms.mean,\n",
        "            \"ret_rms_var\": self.normalizer.ret_rms.var,\n",
        "            \"ret_rms_count\": self.normalizer.ret_rms.count,\n",
        "            \"action_map\": self.action_map,\n",
        "            \"epsilon\": self.epsilon\n",
        "        }, path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        self.q_network.load_state_dict(checkpoint[\"q_network_state_dict\"])\n",
        "        self.target_q_network.load_state_dict(checkpoint[\"target_q_network_state_dict\"])\n",
        "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        self.normalizer.obs_rms.mean = checkpoint[\"normalizer_mean\"]\n",
        "        self.normalizer.obs_rms.var = checkpoint[\"normalizer_var\"]\n",
        "        self.normalizer.obs_rms.count = checkpoint[\"normalizer_count\"]\n",
        "        self.normalizer.ret_rms.mean = checkpoint[\"ret_rms_mean\"]\n",
        "        self.normalizer.ret_rms.var = checkpoint[\"ret_rms_var\"]\n",
        "        self.normalizer.ret_rms.count = checkpoint[\"ret_rms_count\"]\n",
        "\n",
        "        self.action_map = checkpoint[\"action_map\"]\n",
        "        self.epsilon = checkpoint[\"epsilon\"]\n",
        "\n",
        "        print(f\"Model loaded from {path}\")\n",
        "        return self\n",
        "\n",
        "def train_dqn(env_name=\"PandaReach-v3\", total_timesteps=1000000, log_interval=10, save_interval=10000, eval_interval=50000, n_eval_episodes=10):\n",
        "    print(f\"Training on {env_name}...\")\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    eval_env = gym.make(env_name)\n",
        "\n",
        "    os.makedirs('dqn_models', exist_ok=True)\n",
        "    os.makedirs('dqn_logs', exist_ok=True)\n",
        "\n",
        "    # DQN agent\n",
        "    dqn = DQN(\n",
        "        env=env,\n",
        "        learning_rate=5e-4,\n",
        "        buffer_size=100000,\n",
        "        batch_size=128,\n",
        "        target_update_freq=1000,\n",
        "        gamma=0.99,\n",
        "        tau=1.0,\n",
        "        eps_start=1.0,\n",
        "        eps_end=0.05,\n",
        "        eps_decay=50000,\n",
        "        double_q=True,\n",
        "        policy_kwargs={\"net_arch\": [256, 256]}\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = lambda local_vars, global_vars: (\n",
        "        local_vars['self'].save(f\"dqn_models/{env_name}_{local_vars['self'].timesteps}.pth\")\n",
        "        if local_vars['self'].timesteps % save_interval == 0\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    dqn.learn(\n",
        "        total_timesteps=total_timesteps,\n",
        "        callback=checkpoint_callback,\n",
        "        log_interval=log_interval,\n",
        "        train_freq=4,\n",
        "        eval_env=eval_env,\n",
        "        eval_freq=eval_interval,\n",
        "        n_eval_episodes=n_eval_episodes\n",
        "    )\n",
        "\n",
        "    dqn.save(f\"dqn_models/{env_name}_final.pth\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Mean Reward\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(dqn.logger[\"timesteps\"], dqn.logger[\"mean_reward\"])\n",
        "    plt.title('Mean Reward')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Reward')\n",
        "\n",
        "    # Episode length\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(dqn.logger[\"timesteps\"], dqn.logger[\"episode_length\"])\n",
        "    plt.title('Episode Length')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Length')\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(dqn.logger[\"timesteps\"], dqn.logger[\"loss\"])\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dqn_logs/training_progress.png')\n",
        "    plt.close()\n",
        "\n",
        "    return dqn\n",
        "\n",
        "def evaluate_dqn(env_name=\"PandaReach-v3\", model_path=None, n_eval_episodes=10, render=True):\n",
        "    print(f\"Evaluating on {env_name}...\")\n",
        "\n",
        "    env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
        "    dqn = DQN(env=env)\n",
        "\n",
        "    if model_path:\n",
        "        dqn.load(model_path)\n",
        "        print(f\"Model loaded from {model_path}\")\n",
        "    else:\n",
        "        print(\"No model loaded, using random policy\")\n",
        "\n",
        "    mean_reward, mean_length = dqn.evaluate_policy(n_eval_episodes=n_eval_episodes)\n",
        "\n",
        "    env.close()\n",
        "    return mean_reward, mean_length\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_name = \"PandaReach-v3\"\n",
        "\n",
        "    dqn_agent = train_dqn(\n",
        "        env_name=env_name,\n",
        "        total_timesteps=500000, \n",
        "        log_interval=10,\n",
        "        save_interval=50000,\n",
        "        eval_interval=50000\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2V03e53s34C"
      },
      "outputs": [],
      "source": [
        "#check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP0UTnzCs4lR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
